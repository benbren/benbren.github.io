I"ª<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="highlighter-rouge">YEAR</code> is a four-digit number, <code class="highlighter-rouge">MONTH</code> and <code class="highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Youâ€™ll remember in OLS, we had something called theÂ <strong>normal equations</strong>Â - a nice, succinct, simple formula for calculating out best-fit parameters. Youâ€™ll also remember, then, that we had to invert an n by n matrix, <script type="math/tex">X^TX</script> to get these parameters.. which, if n is large, is computationallyÂ veryÂ expensive. Today weâ€™ll discuss gradient descent, a less computationally expensive way (for large n) to obtain parameters that minimize our squared error.</p>

<p>Itâ€™s important to first note that gradient descent is not limited to least squares. Gradient descent is just an algorithm that can be used to maximize (or minimize) any convex objective function by calculating the gradient at a point, taking a step in that direction, calculation the gradient again, taking a step in that directionâ€¦ so on and so forth, until the gradient is basically norm 0. It just so happens that the least squares function is convex, so gradient descent converges to the minimum.</p>

<p>This algorithm wonâ€™t work for non-convex functions as it may only search out a local minimum, not the global minimum. If youâ€™re standing at the base of two hills, the gradient is just going to take you up the steepest hill, not necessarily the tallest one, right? Okay.. anyways, do we remember what the gradient is?</p>

<p>Letâ€™s say we have</p>

<script type="math/tex; mode=display">J(\beta_0, \beta_1, \dots, \beta_q): \mathbb{R}^{q+1} \rightarrow \mathbb{R}</script>

<p>then, the gradient is defined as</p>

<script type="math/tex; mode=display">\nabla JÂ = \frac{\partial J}{\partial \beta_0} \hat{e_1} + \dots +Â \frac{\partial J}{\partial \beta_q} \hat{e_{q+1}} = \begin{bmatrix}Â \frac{\partial J}{\partial \beta_0} \\ \vdots \\Â \frac{\partial J}{\partial \beta_q} \end{bmatrix}</script>

<p>so, essentially, each component of the gradient tells you how fast your function changes with respect to the standard basis in each direction. We can find theÂ directional derivativeÂ of any unit vectorÂ $latex \vec{v}$ by $\nabla J \cdot \vec{v}$</p>

<script type="math/tex; mode=display">\nabla J \cdot \vec{v} = \| \nabla J\|\|\vec{v}\|\cos(\theta) =\| \nabla J\|\cos(\theta)</script>

<p>since $\vec{v}$ is a unit vector, where $\theta$ is the angle between the two vectors. $\cos(\theta)$ is max when $\theta = 0$ - that is, when $ \vec{v}$ is in the direction of the gradient! So the steepest ascent is in the direction of the gradient, and the steepest descent is in the opposite direction. We should note that the steepest ascent here is limited to unit vectors, so the steepest ascent is really in the direction of $\frac{\nabla J}{| \nabla J|}$, right? But thatâ€™s the same direction as the gradient. And we are going to kinda normalize the gradient in our own way laterâ€¦ you will see.</p>

<figure class="highlight"><pre><code class="language-julia" data-lang="julia"><span class="n">gradient_descent</span> <span class="o">=</span> <span class="k">function</span><span class="x">(</span> <span class="n">X</span> <span class="x">,</span> <span class="n">y</span> <span class="x">,</span> <span class="n">Î²</span> <span class="x">,</span> <span class="n">Î±</span> <span class="x">,</span> <span class="n">threshold</span><span class="x">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="nb">true</span><span class="x">)</span>
    <span class="k">if</span> <span class="n">intercept</span>
        <span class="k">if</span> <span class="n">length</span><span class="x">(</span><span class="n">size</span><span class="x">(</span><span class="n">X</span><span class="x">)[</span><span class="mi">2</span><span class="x">])</span> <span class="o">==</span> <span class="n">length</span><span class="x">(</span><span class="n">Î²</span><span class="x">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">inter</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">size</span><span class="x">(</span><span class="n">X</span><span class="x">)[</span><span class="mi">1</span><span class="x">])</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">hcat</span><span class="x">(</span><span class="n">inter</span><span class="x">,</span><span class="n">X</span><span class="x">)</span>
        <span class="k">end</span>
    <span class="k">end</span>

    <span class="n">n</span> <span class="o">=</span> <span class="n">size</span><span class="x">(</span><span class="n">X</span><span class="x">)[</span><span class="mi">1</span><span class="x">]</span>
    <span class="n">q_plus_one</span> <span class="o">=</span> <span class="n">size</span><span class="x">(</span><span class="n">X</span><span class="x">)[</span><span class="mi">2</span><span class="x">]</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">q_plus_one</span><span class="x">)</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">Î±</span> <span class="o">=</span> <span class="x">(</span><span class="n">Î±</span><span class="o">/</span><span class="n">n</span><span class="x">)</span>

    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="mi">10000</span> <span class="o">&amp;&amp;</span> <span class="n">e</span> <span class="o">&gt;</span> <span class="n">threshold</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">X</span><span class="o">*</span><span class="n">Î²</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span>

        <span class="n">gradient</span> <span class="o">=</span> <span class="n">Î±</span> <span class="o">*</span> <span class="x">(</span><span class="n">X</span><span class="err">'</span> <span class="o">*</span> <span class="n">error</span><span class="x">)</span>
        <span class="n">Î²</span> <span class="o">=</span> <span class="n">Î²</span> <span class="o">-</span> <span class="n">gradient</span>

        <span class="n">e</span> <span class="o">=</span> <span class="n">norm</span><span class="x">(</span><span class="n">gradient</span><span class="x">)</span>

        <span class="n">it</span> <span class="o">=</span> <span class="n">it</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">end</span>

    <span class="k">return</span> <span class="n">Î²</span><span class="x">,</span> <span class="n">e</span>
<span class="k">end</span> </code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyllâ€™s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>

:ET