I"×*<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>Lots of people go through life talking about statistics and not actually knowing what a statistic <em>is</em>, sadly enough. The point of statistics, and statistical inference in general, is to make inferences about a <em>population</em> based off a sample. In general, you have a population parameter (<script type="math/tex">\theta</script>) that needs to be estimated (could be high - dimensional - the most we will talk about is <em>max</em> 2, so no worries) and some data (a random sample, once could sayâ€¦) <script type="math/tex">X_ 1, \dots, X_ n</script> from that population, from which we observe the data <script type="math/tex">X_ 1, \dots, X_ n</script>. The goal of statistical inference is to take that sample and make really good educated guesses about the population and also make good, educated guesses about how good and educated your guess is. Make sense? I didnâ€™t think so.</p>

<p><strong>Inference</strong> is the process of drawing information about a population based off a sample (like I said above, with a fancy word defining it). The points is - probability theory is based on knowing <script type="math/tex">\theta</script>. We never actually know <script type="math/tex">\theta</script>, so to do anything in practice we need inference about <script type="math/tex">\theta</script>. Big time important.</p>

<p>For example, we can make conclusions about the probability we get a certain number of successes in a certain number of trials (i.e Binomial(<script type="math/tex">n,p</script>), or the probability that a certain metric is less than some value (i.e <script type="math/tex">\mathcal{N}(\mu, \sigma^2</script>)) - but that requires us to know the distribution those values follow in the population (i.e to know the population parameters).</p>

<p>This post will be about a key concept in inference - the <em>sufficient statistic</em>. First - and back to the beginning - what is a statistic?!</p>

<p>A statistics takes a random variable <script type="math/tex">\mathbf{X}</script> and maps it via some function <script type="math/tex">T(.)</script>. That is,</p>

<script type="math/tex; mode=display">T(x_1, \dots, x_n) = T(\mathbf{X}): \mathbb{R}^n \rightarrow \mathbb{R}^m</script>

<p>A few things. <script type="math/tex">T(\mathbf{X})</script> is also a random variable, obviously. There is no restriction on <script type="math/tex">m</script>, but I think you probably get the idea that <script type="math/tex">% <![CDATA[
m < n %]]></script>, because it doesnâ€™t really make sense to make our data more complicated. We also donâ€™t want to lose information in this mapping - this is a key point. For instance, if <script type="math/tex">T(\mathbf{X}) = X_1</script>, we have lost so much information contained in our data by using this transformation.</p>

<p>So, in summary, we want to take our data and use some transformation <script type="math/tex">T(\mathbf{X})</script> to make that data simpler and no less informative. That is what makes a good statistic good. This is why we use some statistics, and not others - as we will see. Anyways, this seems easy. Not like thereâ€™s a million choices for <script type="math/tex">T</script> or anythingâ€¦</p>

<p>Anyways. Letâ€™s define what a sufficient statistic is. A statistic <script type="math/tex">T(\mathbf{X})</script> is called a <em>sufficient statistic</em> for the parameter <script type="math/tex">\theta</script> if the distribution of the data <script type="math/tex">\mathbf{X}</script> given <script type="math/tex">T(\mathbf{X})</script> does not depend on <script type="math/tex">\theta</script>. Why? This means that, given that we know the value of <script type="math/tex">T(\mathbf{X})</script>, the information left in the sample does not contain any information about <script type="math/tex">\theta</script>. This should be obvious - if the distribution of the data does not depend on the parameter, how can you get information about the parameter from that distribution ya know? Formally, if</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x})</script>

<p>then <script type="math/tex">T(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>.</p>

<p>Letâ€™s look at an example: assume <script type="math/tex">X_1, \dots, X_n \sim \textrm{Bernoulli}(p)</script> and <script type="math/tex">T(\mathbf{X}) = \sum_{i=1}^n X_i</script>. Is <script type="math/tex">T</script> sufficient for <script type="math/tex">p</script>? Yep. Next.</p>

<p>Just kidding. Letâ€™s see why.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t) = \frac{P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t)}{P(T(\mathbf{X}) = t))}</script>

<script type="math/tex; mode=display">% <![CDATA[
= \begin{cases} 
	\frac{P(\mathbf{X} = \mathbf{x})}{P(T(\mathbf{X}) = t))} & \textrm{if} \; T(\mathbf{X}) = t \\
	0 & \textrm{else} 
 	\end{cases} %]]></script>

<p>(only considering the top case)</p>

<script type="math/tex; mode=display">= \frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{q_{T(\mathbf{X})}(t \vert p)}</script>

<p>Now, notice the distribution of <script type="math/tex">T(\mathbf{X}) \sim \textrm{Binomial}(n,p)</script>. Therefore,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x})}{f_{T(\mathbf{X})}(t)} = \frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{t}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{t}}</script>

<p>which doesnâ€™t contain <script type="math/tex">p</script>. So now - yep, <script type="math/tex">T(\mathbf{X})</script> is sufficient for <script type="math/tex">p</script>!</p>

<p>Now, in typical fashion, lets define like a million more ways to show something is sufficient. The first - shown through example. From above we showed that the original definiton could  boil down to</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q_{T(\mathbf{X})}(t \vert \theta)} = \frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q(T(\mathbf{X}) \vert \theta)}</script>

<p>so, if this ratio is free of <script type="math/tex">\theta</script> for all <script type="math/tex">\mathbf{x}</script> in the support of <script type="math/tex">\mathbf{X}</script>, then <script type="math/tex">(T(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>.</p>

<p>So, the example above boils down to</p>

<script type="math/tex; mode=display">\frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{T(\mathbf{X})}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{T(\mathbf{X})}}</script>

<p>which does not depend on <script type="math/tex">\theta</script> so we come to the same conclusion. This seems redundant. It is, really, but the benefit is we donâ€™t have to consider <script type="math/tex">T(\mathbf{X}) = t</script> and <script type="math/tex">T(\mathbf{X}) \neq t</script>. So, originally, we <em>techincally</em> had to say that 0 did not depend on <script type="math/tex">p</script>, which is obvious. In this second definition, we donâ€™t even need to consider that.</p>

<p>Letâ€™s try another example using this second definition of sufficiency. Letâ€™s check whether <script type="math/tex">T(\mathbf{X}) = \bar{X}</script> is sufficient for <script type="math/tex">\mu</script> in the normal distribution (with known variance <script type="math/tex">\sigma^2</script>).</p>

<script type="math/tex; mode=display">f_X(x \vert \mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x - \mu)^2}{2\sigma^2}\right)</script>

<p>Recall that <script type="math/tex">\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})</script>. Therefore,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert \mu)}{q(T(\mathbf{X}) \vert \mu)} = \frac{(\frac{1}{\sqrt{2\pi\sigma^2}})^n\textrm{exp}\left(- \displaystyle\sum_{i=1}^n\frac{(x_i - \mu)^2}{2\sigma^2}\right)}{\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-n(\bar{x} - \mu)^2}{2\sigma^2}\right)} = \mathcal{K} \frac{\textrm{exp}\left(\frac{- \sum_{i=1}^n(x_i^2 - 2x_i\mu)}{2\sigma^2}\right)}{\textrm{exp}\left( \frac{2\bar{x}n \mu - n\bar{x}^2}{2 \sigma^2}\right)}</script>

<script type="math/tex; mode=display">= \mathcal{K}\frac{\textrm{exp}\left( \frac{-\sum_{i=1}^{n}x_i^2}{2 \sigma^2}\right)}{\textrm{exp}\left(\frac{-n\bar{x}^2}{2 \sigma^2}\right)}</script>

<p>which doesnâ€™t depend on <script type="math/tex">\mu</script>! So, the sample mean is a sufficient statistic for the population average in a normal distribution - hence why we use it all the time!</p>

<p>Now, moving on to even more ways to show sufficiencyâ€¦</p>

<p><strong>Factorization Theorem</strong> : <script type="math/tex">T(\mathbf{X})</script> is sufficient if <em>and only if</em> there exists <script type="math/tex">g(t\vert \theta)</script> and <script type="math/tex">h(\theta)</script> such that <script type="math/tex">\forall \mathbf{x}, \theta</script>,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta) = g(T(\mathbf{X})\vert \theta)h(\mathbf{x})</script>

<p>Since this is an <em>iff</em>, we have to prove it in both directions. First,</p>

<p><strong>Proof</strong>: Sufficiency <script type="math/tex">\implies</script> Factorization</p>

<p>This is the easy case.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x}\vert \theta) = P(\mathbf{X} = \mathbf{x} \vert \theta)</script>

<script type="math/tex; mode=display">= P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t \vert \theta)</script>

<p>and, by the definition of conditional probability,</p>

<script type="math/tex; mode=display">= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t,\theta)</script>

<p>and, by sufficiency (i.e. we assume given <script type="math/tex">T(\mathbf{X})</script>, the distribution of the data is free of <script type="math/tex">\theta</script>).</p>

<script type="math/tex; mode=display">= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t)</script>

<script type="math/tex; mode=display">= g(t \vert \theta)h(\mathbf{x})</script>

<p>and we have proved that direction. Now,</p>

<p><strong>Proof</strong>: Factorization <script type="math/tex">\implies</script> Sufficiency</p>

<p>Let <script type="math/tex">q(y \vert \theta)</script> be the pmf of <script type="math/tex">T(\mathbf{X})</script> and let <script type="math/tex">A_t</script> be the set containing all the possible data that yield a statistics <script type="math/tex">T\mathbf{x} = t</script>. That is,</p>

<script type="math/tex; mode=display">A_t = \{ \mathbf{y}: T(\mathbf{y}) = t\}</script>
:ET