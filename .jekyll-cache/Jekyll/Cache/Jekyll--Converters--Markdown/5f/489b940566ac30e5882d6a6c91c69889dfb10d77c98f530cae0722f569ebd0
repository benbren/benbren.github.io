I"Ö<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>When most people think about statistics, they think about flipping a coin or gambling or having flashbacks to dropping out of high school because they hated the class so muchâ€¦ moral of the story is, itâ€™s just one thing to think about. What if I told you that that wasnâ€™t necessarily true? I am about to tell you that. Statistics is the science of modeling randomness. But what is random, and what is not? That is a fundamental question in statistics and splits the subject into two separate approaches - Bayesian and Frequentist.</p>

<p>What does it mean to say that I am 50% certain it will rain tomorrow? That is not so much a statement of probability, but rather of uncertainty. It will rain or it will not rain, but that outcome is something you are uncertain of. Consider another example: flip a coin and cover it up immediately once it lands, so you donâ€™t know what the result is. Whatâ€™re the chances that the coin is heads? From a traditional, frequentist perspective, there is no answer to this question. It already happened. There is already a result - youâ€™re just ignorant. But thatâ€™s not really how we think, is it? We donâ€™t know what the result is - we are uncertain of it. Letâ€™s consider a medical example. Say youâ€™re getting screened for the flu, and the result comes out positive. Do you actually have the flu? In the frequentist approach, that doesnâ€™t make sense to ask. We do have sensitivity and specificity. Let <script type="math/tex">\theta = 1</script> if you have the flu and <script type="math/tex">\theta = 0</script> if you donâ€™t. Then the sensitivity is the probability the test is positive given that you have the flue, or <script type="math/tex">P(T = 1 \vert \theta = 1)</script> and the specificity is <script type="math/tex">P(T = 0 \vert \theta = 0)</script>,Â the probability the test is negative when you donâ€™t have the flu. If specificity is not 1, you might get tested positive but not have the disease. It makes sense then to ask â€˜â€œDo I actually have the flu?â€, but does it make statistical sense to ask â€˜Do I have the flu?â€™? That is, what is <script type="math/tex">P(\theta =1 \vert T = 1)</script>? A frequentist says â€˜Bad question. <script type="math/tex">\theta</script> is fixedâ€™. A Bayesian will come in and say â€˜Hold on, <script type="math/tex">\theta</script> is not actually fixed. We can estimate thisâ€™</p>

<p>This is the whole premise of Bayesian statistics - to use probability theory to not <em>only</em> quantify probability but also to quantify uncertainty. Much better!! Solving problems that were disregarded earlier - we like that!</p>

<p>Okay, moving onâ€¦ now that Iâ€™ve converted you to the Bayesian approach to statisticsâ€¦</p>

<p>:)</p>

<p>Letâ€™s talk about the setup. In a frequentist approach, we have a random variable <script type="math/tex">\mathbf{X}</script>, observed data from that random variable, a model, and a parameter <script type="math/tex">\theta \in \Omega</script> that is unknownÂ but <em>fixed</em>. In the Bayesian framework, we donâ€™t consider that italicized part. Itâ€™s a subtle change that makes a huge difference. Now, we have</p>

<script type="math/tex; mode=display">\theta \sim \pi(\theta)</script>

<p>where <script type="math/tex">\pi</script> is called aÂ <em>priorÂ distribution</em>. Now, we need to make an adjustment to your original dataâ€™sÂ distribution. Now, we have a conditional distribution!</p>

<script type="math/tex; mode=display">\mathbf{X} \vert \theta \sim f(\mathbf{x}\vert \theta)</script>

<p>and, by Bayes rule, the joint distribution of <script type="math/tex">\mathbf{X}</script> and <script type="math/tex">\theta</script> is</p>

<script type="math/tex; mode=display">f(\mathbf{x},\theta) = \pi(\theta)f(\mathbf{x}|\theta)</script>

<p>and, thus, the marginal distribution ofÂ Â <script type="math/tex">\mathbf{X}</script> is</p>

<script type="math/tex; mode=display">m(\mathbf{x}) = \displaystyle\int_{\theta \in \Omega} {f(\mathbf{x},\theta)d\theta}</script>

<script type="math/tex; mode=display">\pi(\theta \vert \mathbf{x}) = \frac{\pi(\theta)f(\mathbf{x} \vert \theta)}{m(\mathbf{x}) }</script>

<p>We now have all the tools we need to do some basic Bayesian analysis!</p>

<p>and, so, by Bayes rule again, the posterior distribution of $latex \theta$ is</p>

<p>Let us look at <script type="math/tex">X_1, \dots, X_n \sim Bernoulli(p)</script> where <script type="math/tex">p \sim Beta(\alpha, \beta)</script>. Then we could calculateÂ <script type="math/tex">\pi(\theta \vert \mathbf{x}) = \frac{\pi(p)f(\mathbf{x}\vert \theta)}{m(\mathbf{x}) }</script>, but that would take forever. Does <script type="math/tex">m(\mathbf{x})</script> matter? No. Why? Think about it. We want the posterior distribtion of <script type="math/tex">p</script>, right? SoÂ <script type="math/tex">m(\mathbf{x})</script> has <em>literally</em> (millenialsâ€¦) nothing to do withÂ <script type="math/tex">p</script>, therefore it just acts as a normalizing constant in the equation to make sure thatÂ <script type="math/tex">\pi(p \vert \mathbf{x})</script> is a true pdf. Therefore, we can just look at the numerator of the expression and recognize the kernel of the distribution to understand what family the posterior distribution comes from. Soo, in math,</p>

<script type="math/tex; mode=display">\pi(p \vert \mathbf{x}) \propto \pi(p)f(\mathbf{x} \vert p)</script>

<p>We know</p>

<script type="math/tex; mode=display">f(\mathbf{x}\vert p) = \displaystyle \prod_{i=1}^{n} f(\mathbf{x_i}\vert p) =Â \displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i})</script>

<p>and</p>

<script type="math/tex; mode=display">\pi(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}</script>

<p>so</p>

<script type="math/tex; mode=display">\pi(p \vert \mathbf{x}) \propto\displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i}) \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}</script>

<script type="math/tex; mode=display">= p^{\sum_{i=1}^{n}x_i}(1-p)^{n - \sum_{i=1}^{n}x_i}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}</script>
:ET