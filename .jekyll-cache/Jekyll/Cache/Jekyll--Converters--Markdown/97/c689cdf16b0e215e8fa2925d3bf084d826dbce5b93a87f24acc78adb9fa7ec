I"à<p>Youâ€™ll remember in OLS, we had something called theÂ <strong>normal equations</strong>Â - a nice, succinct, simple formula for calculating out best-fit parameters. Youâ€™ll also remember, then, that we had to invert an n by n matrix, $X^TX$ to get these parameters.. which, if n is large, is computationallyÂ veryÂ expensive. Today weâ€™ll discuss gradient descent, a less computationally expensive way (for large n) to obtain parameters that minimize our squared error.</p>
:ET