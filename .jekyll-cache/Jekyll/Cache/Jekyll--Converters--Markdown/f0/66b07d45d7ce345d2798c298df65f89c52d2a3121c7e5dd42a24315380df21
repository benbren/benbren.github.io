I"¾F<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>Lots of people go through life talking about statistics and not actually knowing what a statistic <em>is</em>, sadly enough. The point of statistics, and statistical inference in general, is to make inferences about a <em>population</em> based off a sample. In general, you have a population parameter (<script type="math/tex">\theta</script>) that needs to be estimated (could be high - dimensional - the most we will talk about is <em>max</em> 2, so no worries) and some data (a random sample, once could sayâ€¦) <script type="math/tex">X_ 1, \dots, X_ n</script> from that population, from which we observe the data <script type="math/tex">x_ 1, \dots, x_ n</script>. The goal of statistical inference is to take that sample and make really good educated guesses about the population and also make good, educated guesses about how good and educated your guess is. Make sense? I didnâ€™t think so.</p>

<p><strong>Inference</strong> is the process of drawing information about a population based off a sample (like I said above, with a fancy word defining it). The points is - probability theory is based on knowing <script type="math/tex">\theta</script>. We never actually know <script type="math/tex">\theta</script>, so to do anything in practice we need inference about <script type="math/tex">\theta</script>. Big time important.</p>

<p>For example, we can make conclusions about the probability we get a certain number of successes in a certain number of trials (i.e Binomial(<script type="math/tex">n,p</script>), or the probability that a certain metric is less than some value (i.e <script type="math/tex">\mathcal{N}(\mu, \sigma^2</script>)) - but that requires us to know the distribution those values follow in the population (i.e to know the population parameters).</p>

<p>This post will be about a key concept in inference - the <em>sufficient statistic</em>. First - and back to the beginning - what is a statistic?!</p>

<p>A statistics takes a random variable <script type="math/tex">\mathbf{X}</script> and maps it via some function <script type="math/tex">T(.)</script>. That is,</p>

<script type="math/tex; mode=display">T(x_1, \dots, x_n) = T(\mathbf{X}): \mathbb{R}^n \rightarrow \mathbb{R}^m</script>

<p>A few things. <script type="math/tex">T(\mathbf{X})</script> is also a random variable, obviously. There is no restriction on <script type="math/tex">m</script>, but I think you probably get the idea that <script type="math/tex">% <![CDATA[
m < n %]]></script>, because it doesnâ€™t really make sense to make our data more complicated. We also donâ€™t want to lose information in this mapping - this is a key point. For instance, if <script type="math/tex">T(\mathbf{X}) = X_1</script>, we have lost so much information contained in our data by using this transformation.</p>

<p>So, in summary, we want to take our data and use some transformation <script type="math/tex">T(\mathbf{X})</script> to make that data simpler and no less informative. That is what makes a good statistic good. This is why we use some statistics, and not others - as we will see. Anyways, this seems easy. Not like thereâ€™s a million choices for <script type="math/tex">T</script> or anythingâ€¦</p>

<p>Anyways. Letâ€™s define what a sufficient statistic is. A statistic <script type="math/tex">T(\mathbf{X})</script> is called a <em>sufficient statistic</em> for the parameter <script type="math/tex">\theta</script> if the distribution of the data <script type="math/tex">\mathbf{X}</script> given <script type="math/tex">T(\mathbf{X})</script> does not depend on <script type="math/tex">\theta</script>. Why? This means that, given that we know the value of <script type="math/tex">T(\mathbf{X})</script>, the information left in the sample does not contain any information about <script type="math/tex">\theta</script>. This should be obvious - if the distribution of the data does not depend on the parameter, how can you get information about the parameter from that distribution ya know? Formally, if</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x})</script>

<p>then <script type="math/tex">T(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>.</p>

<p>Letâ€™s look at an example: assume <script type="math/tex">X_1, \dots, X_n \sim \textrm{Bernoulli}(p)</script> and <script type="math/tex">T(\mathbf{X}) = \sum_{i=1}^n X_i</script>. Is <script type="math/tex">T</script> sufficient for <script type="math/tex">p</script>? Yep. Next.</p>

<p>Just kidding. Letâ€™s see why.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t) = \frac{P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t)}{P(T(\mathbf{X}) = t))}</script>

<script type="math/tex; mode=display">% <![CDATA[
= \begin{cases} 
	\frac{P(\mathbf{X} = \mathbf{x})}{P(T(\mathbf{X}) = t))} & \textrm{if} \; T(\mathbf{X}) = t \\
	0 & \textrm{else} 
 	\end{cases} %]]></script>

<p>(only considering the top case)</p>

<script type="math/tex; mode=display">= \frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{q_{T(\mathbf{X})}(t \vert p)}</script>

<p>Now, notice the distribution of <script type="math/tex">T(\mathbf{X}) \sim \textrm{Binomial}(n,p)</script>. Therefore,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x})}{f_{T(\mathbf{X})}(t)} = \frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{t}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{t}}</script>

<p>which doesnâ€™t contain <script type="math/tex">p</script>. So now - yep, <script type="math/tex">T(\mathbf{X})</script> is sufficient for <script type="math/tex">p</script>!</p>

<p>Now, in typical fashion, lets define like a million more ways to show something is sufficient. The first - shown through example. From above we showed that the original definiton could  boil down to</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q_{T(\mathbf{X})}(t \vert \theta)} = \frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q(T(\mathbf{x}) \vert \theta)}</script>

<p>so, if this ratio is free of <script type="math/tex">\theta</script> for all <script type="math/tex">\mathbf{x}</script> in the support of <script type="math/tex">\mathbf{X}</script>, then <script type="math/tex">(T(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>.</p>

<p>So, the example above boils down to</p>

<script type="math/tex; mode=display">\frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{T(\mathbf{x})}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{T(\mathbf{x})}}</script>

<p>which does not depend on <script type="math/tex">\theta</script> so we come to the same conclusion. This seems redundant. It is, really, but the benefit is we donâ€™t have to consider <script type="math/tex">T(\mathbf{X}) = t</script> and <script type="math/tex">T(\mathbf{X}) \neq t</script>. So, originally, we <em>techincally</em> had to say that 0 did not depend on <script type="math/tex">p</script>, which is obvious. In this second definition, we donâ€™t even need to consider that.</p>

<p>Letâ€™s try another example using this second definition of sufficiency. Letâ€™s check whether <script type="math/tex">T(\mathbf{X}) = \bar{X}</script> is sufficient for <script type="math/tex">\mu</script> in the normal distribution (with known variance <script type="math/tex">\sigma^2</script>).</p>

<script type="math/tex; mode=display">f_X(x \vert \mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x - \mu)^2}{2\sigma^2}\right)</script>

<p>Recall that <script type="math/tex">\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})</script>. Therefore,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert \mu)}{q(T(\mathbf{X}) \vert \mu)} = \frac{(\frac{1}{\sqrt{2\pi\sigma^2}})^n\textrm{exp}\left(- \displaystyle\sum_{i=1}^n\frac{(x_i - \mu)^2}{2\sigma^2}\right)}{\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-n(\bar{x} - \mu)^2}{2\sigma^2}\right)} = \mathcal{K} \frac{\textrm{exp}\left(\frac{- \sum_{i=1}^n(x_i^2 - 2x_i\mu)}{2\sigma^2}\right)}{\textrm{exp}\left( \frac{2\bar{x}n \mu - n\bar{x}^2}{2 \sigma^2}\right)}</script>

<script type="math/tex; mode=display">= \mathcal{K}\frac{\textrm{exp}\left( \frac{-\sum_{i=1}^{n}x_i^2}{2 \sigma^2}\right)}{\textrm{exp}\left(\frac{-n\bar{x}^2}{2 \sigma^2}\right)}</script>

<p>which doesnâ€™t depend on <script type="math/tex">\mu</script>! So, the sample mean is a sufficient statistic for the population average in a normal distribution - hence why we use it all the time!</p>

<p>Now, moving on to even more ways to show sufficiencyâ€¦</p>

<p><strong>Factorization Theorem</strong> : <script type="math/tex">T(\mathbf{X})</script> is sufficient if <em>and only if</em> there exists <script type="math/tex">g(t\vert \theta)</script> and <script type="math/tex">h(\theta)</script> such that <script type="math/tex">\forall \mathbf{x}, \theta</script>,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta) = g(T(\mathbf{x})\vert \theta)h(\mathbf{x})</script>

<p>Since this is an <em>iff</em>, we have to prove it in both directions. First,</p>

<p><strong>Proof</strong>: Sufficiency <script type="math/tex">\implies</script> Factorization</p>

<p>This is the easy case.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x}\vert \theta) = P(\mathbf{X} = \mathbf{x} \vert \theta)</script>

<script type="math/tex; mode=display">= P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t \vert \theta)</script>

<p>and, by the definition of conditional probability,</p>

<script type="math/tex; mode=display">= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t,\theta)</script>

<p>and, by sufficiency (i.e. we assume given <script type="math/tex">T(\mathbf{X})</script>, the distribution of the data is free of <script type="math/tex">\theta</script>).</p>

<script type="math/tex; mode=display">= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t)</script>

<script type="math/tex; mode=display">= g(t \vert \theta)h(\mathbf{x})</script>

<p>and we have proved that direction. Now,</p>

<p><strong>Proof</strong>: Factorization <script type="math/tex">\implies</script> Sufficiency</p>

<p>Let <script type="math/tex">q(y \vert \theta)</script> be the pmf of <script type="math/tex">T(\mathbf{X})</script> and let <script type="math/tex">A_t</script> be the set containing all the possible data that yield a statistics <script type="math/tex">T(\mathbf{x}) = t</script>. That is,</p>

<script type="math/tex; mode=display">A_t = \{ \mathbf{y}: T(\mathbf{y}) = t\}</script>

<p>Then,</p>

<script type="math/tex; mode=display">q(t \vert \theta) = P(T(\mathbf{X}) = t \vert \theta) = \displaystyle\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)</script>

<p>So, since we are assuming factorization,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x}\vert\theta)}{q(t \vert \theta)} = \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)}</script>

<script type="math/tex; mode=display">= \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} g(T(\mathbf{y}) \vert \theta) h(\mathbf{y})}</script>

<p>and, since for <script type="math/tex">y \in A_t</script>, <script type="math/tex">T(\mathbf{y}) = t = T(\mathbf{x})</script></p>

<script type="math/tex; mode=display">= \frac{h(\mathbf{x})}{\sum_{y \in A_T}h(\mathbf{y})}</script>

<p>and this does not ever depend on <script type="math/tex">\theta</script>, so <script type="math/tex">T(\mathbf{X})</script> is sufficient and we have completed the proof!</p>

<p>So, this makes things wicked easyâ€¦. letâ€™s go back to the two examples we have done above. First, <script type="math/tex">\mathbf{X} \sim \textrm{Bernoulli}(p)</script>.</p>

<script type="math/tex; mode=display">f(\mathbf{x} \vert p) = \displaystyle\prod p^{x_i}(1-p)^{1- x_i} = p^{\sum x_i}(1-p)^{n - \sum x_i} = p^t(1-p)^{n-t}</script>

<p>and then <script type="math/tex">g(t \vert p) = p^t(1-p)^{n-p}</script> and <script type="math/tex">h(\mathbf{y}) = 1</script>, then we can see that <script type="math/tex">\sum_{i= 1}^n X_i</script> is sufficient for <script type="math/tex">p</script>.</p>

<p>Now, let us consider the second, normal with known variance, example.</p>

<script type="math/tex; mode=display">f_X(\mathbf{x} \vert \mu) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)</script>

<script type="math/tex; mode=display">= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right) \times \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)</script>

<p>so <script type="math/tex">h(\mathbf{x}) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right)</script> and <script type="math/tex">g(t \vert \mu) = \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)</script> when <script type="math/tex">t = \bar{x}</script>, so the sample mean is sufficient for <script type="math/tex">\mu</script>. Way easier - we like that.</p>

<p>Letâ€™s try a new example. Consider <script type="math/tex">\mathbf{X} \sim \textrm{Uniform}(0,\theta)</script>. What is a sufficient statistic for <script type="math/tex">\theta</script>? To assess this, letâ€™s use the factorization theorem. Remember,</p>

<script type="math/tex; mode=display">% <![CDATA[
f(x \vert \theta) = \begin{cases} 
\frac{1}{\theta} & \textrm{if} \; x \in [0,\theta] \\
0 & \textrm{else}
\end{cases} %]]></script>

<p>which is identical to</p>

<script type="math/tex; mode=display">\frac{1}{\theta}I(0 \leq x) I (x \leq \theta)</script>

<p>so, therefore,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta) = \displaystyle\prod_{i=1}^n \frac{1}{\theta}I(0 \leq x_i) I (x_i \leq \theta) = \theta^{-n}I(x_{(n)} \leq \theta) I(0 \leq x_{(1)})</script>

<p>so we can conclude that the max, <script type="math/tex">X_{(n)}</script>, is a sufficient statistic for <script type="math/tex">\theta</script>.</p>

<p>Now, letâ€™s consider a two-dimensional (<em>whoa</em>) sufficient statistic! When I learned this, the professor made this seem very straight forward - but I know things get real iffy real quick when the dimension of things increase - so lets take it slow. What we are looking for now is <script type="math/tex">T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))</script> such that the factorization theorem still applies. Letâ€™s consider a normal distribution with unknown variance. Now, what we need is to find a factorization</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = g(T_1(\mathbf{x}), T_2(\mathbf{X}) \vert \mu, \sigma^2)h(\mathbf{x})</script>

<p>This differs from the one dimensional case where we kind of just tossed the parts of the exponent that did not depend on <script type="math/tex">\mu</script> into the constant and called it a day. Now, we can do that but only for the parts of the exponent that donâ€™t depend on <script type="math/tex">\mu</script> and <script type="math/tex">\sigma^2</script> which is none of them so trick-statement you canâ€™t toss anything into the constant from the exponent. Turns out that constant infront isnâ€™t even a constant anymore since it involves <script type="math/tex">\sigma^2</script>. Bummer. All this means, though, is more algebra - which, although it is tedious, is not difficult if you pay attention (which <em>I know</em> is difficult). So, letâ€™s get to it.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)</script>

<script type="math/tex; mode=display">= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-\sum_{i=1}^n((x_i - \bar{x}) + (\bar{x} - \mu))^2}{2\sigma^2}\right)</script>

<script type="math/tex; mode=display">= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 +2 \sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu)  - n(\bar{x} - \mu)^2\right)\right)</script>

<p>and now note that 
<script type="math/tex">\sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu) = \bar{x}\sum x_i - \mu\sum x_i - n\bar{x} + n\bar{x}\mu = 0</script></p>

<p>so, therefore,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 - n(\bar{x} - \mu)^2\right)\right)</script>

<p>and by using <script type="math/tex">h(\mathbf{x}) = 1</script>, we have the factorization theorem! Therefore,</p>

<script type="math/tex; mode=display">T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (\bar{X}, \sum_{i=1}^n (X_i - \bar{X})^2)</script>

<p>is a sufficient statistic for <script type="math/tex">\mathbf{\phi} = (\mu, \sigma^2)</script></p>

<p>Letâ€™s try another one - <script type="math/tex">\mathbf{X} \sim \textrm{Uniform}(\alpha, \theta)</script>. Extending our knowledge of the one-dimensional case, this should be fairly simple.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta, \alpha) = \displaystyle\prod_{i=1}^n \frac{1}{\theta - \alpha}I(\alpha \leq x_i) I (x_i \leq \theta) = (\theta - \alpha)^{-n}I(x_{(n)} \leq \theta) I(\alpha \leq x_{(1)})</script>

<p>so we can conclude that</p>

<script type="math/tex; mode=display">T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (X_{(1)}, X_{(n)})</script>

<p>is suffcient for <script type="math/tex">\phi = (\alpha,theta)</script></p>
:ET