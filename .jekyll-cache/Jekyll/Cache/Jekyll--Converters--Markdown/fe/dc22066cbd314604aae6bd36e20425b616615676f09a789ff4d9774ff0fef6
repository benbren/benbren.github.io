I"Wÿ<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>Lots of people go through life talking about statistics and not actually knowing what a statistic <em>is</em>, sadly enough. The point of statistics, and statistical inference in general, is to make inferences about a <em>population</em> based off a sample. In general, you have a population parameter (<script type="math/tex">\theta</script>) that needs to be estimated (could be high - dimensional - the most we will talk about is <em>max</em> 2, so no worries) and some data (a random sample, once could say‚Ä¶) <script type="math/tex">X_ 1, \dots, X_ n</script> from that population, from which we observe the data <script type="math/tex">x_ 1, \dots, x_ n</script>. The goal of statistical inference is to take that sample and make really good educated guesses about the population and also make good, educated guesses about how good and educated your guess is. Make sense? I didn‚Äôt think so.</p>

<p><strong>Inference</strong> is the process of drawing information about a population based off a sample (like I said above, with a fancy word defining it). The points is - probability theory is based on knowing <script type="math/tex">\theta</script>. We never actually know <script type="math/tex">\theta</script>, so to do anything in practice we need inference about <script type="math/tex">\theta</script>. Big time important.</p>

<p>For example, we can make conclusions about the probability we get a certain number of successes in a certain number of trials (i.e Binomial(<script type="math/tex">n,p</script>), or the probability that a certain metric is less than some value (i.e <script type="math/tex">\mathcal{N}(\mu, \sigma^2</script>)) - but that requires us to know the distribution those values follow in the population (i.e to know the population parameters).</p>

<p>First - and back to the beginning - what is a statistic?! (FYI - this is less of a post, more of a literal book chapter.. fair warning!)</p>

<p>A statistics takes a random variable <script type="math/tex">\mathbf{X}</script> and maps it via some function <script type="math/tex">T(.)</script>. That is,</p>

<script type="math/tex; mode=display">T(x_1, \dots, x_n) = T(\mathbf{X}): \mathbb{R}^n \rightarrow \mathbb{R}^m</script>

<p>A few things. <script type="math/tex">T(\mathbf{X})</script> is also a random variable, obviously. There is no restriction on <script type="math/tex">m</script>, but I think you probably get the idea that <script type="math/tex">% <![CDATA[
m < n %]]></script>, because it doesn‚Äôt really make sense to make our data more complicated. We also don‚Äôt want to lose information in this mapping - this is a key point. For instance, if <script type="math/tex">T(\mathbf{X}) = X_1</script>, we have lost so much information contained in our data by using this transformation.</p>

<p>So, in summary, we want to take our data and use some transformation <script type="math/tex">T(\mathbf{X})</script> to make that data simpler and no less informative. That is what makes a good statistic good. This is why we use some statistics, and not others - as we will see. Anyways, this seems easy. Not like there‚Äôs a million choices for <script type="math/tex">T</script> or anything‚Ä¶</p>

<h2 id="sufficient-statistic">Sufficient Statistic</h2>

<h4 id="definition">Definition</h4>

<p>Anyways. Let‚Äôs define what a sufficient statistic is. A statistic <script type="math/tex">T(\mathbf{X})</script> is called a <em>sufficient statistic</em> for the parameter <script type="math/tex">\theta</script> if the distribution of <script type="math/tex">\mathbf{X}</script> given <script type="math/tex">T(\mathbf{X})</script> does not depend on <script type="math/tex">\theta</script>. Why? This means that, given that we know the value of <script type="math/tex">T(\mathbf{X})</script>, the information left in the sample does not contain any information about <script type="math/tex">\theta</script>. This should be obvious - if the distribution of the data does not depend on the parameter, how can you get information about the parameter from that distribution ya know? Formally, if</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x})</script>

<p>then <script type="math/tex">T(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>.</p>

<p>Let‚Äôs look at an example: assume <script type="math/tex">X_1, \dots, X_n \sim \textrm{Bernoulli}(p)</script> and <script type="math/tex">T(\mathbf{X}) = \sum_{i=1}^n X_i</script>. Is <script type="math/tex">T</script> sufficient for <script type="math/tex">p</script>? Yep. Next.</p>

<p>Just kidding. Let‚Äôs see why.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t) = \frac{P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t)}{P(T(\mathbf{X}) = t))}</script>

<script type="math/tex; mode=display">% <![CDATA[
= \begin{cases} 
	\frac{P(\mathbf{X} = \mathbf{x})}{P(T(\mathbf{X}) = t))} & \textrm{if} \; T(\mathbf{X}) = t \\
	0 & \textrm{else} 
 	\end{cases} %]]></script>

<p>(only considering the top case)</p>

<script type="math/tex; mode=display">= \frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{q_{T(\mathbf{X})}(t \vert p)}</script>

<p>Now, notice the distribution of <script type="math/tex">T(\mathbf{X}) \sim \textrm{Binomial}(n,p)</script>. Therefore,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x})}{f_{T(\mathbf{X})}(t)} = \frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{t}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{t}}</script>

<p>which doesn‚Äôt contain <script type="math/tex">p</script>. So now - yep, <script type="math/tex">T(\mathbf{X})</script> is sufficient for <script type="math/tex">p</script>!</p>

<h4 id="theorem-1">Theorem 1</h4>

<p>Now, in typical fashion, lets define like a million more ways to show something is sufficient. The first - shown through example. From above we showed that the original definiton could  boil down to</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q_{T(\mathbf{X})}(t \vert \theta)} = \frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q(T(\mathbf{x}) \vert \theta)}</script>

<p>so, if this ratio is free of <script type="math/tex">\theta</script> for all <script type="math/tex">\mathbf{x}</script> in the support of <script type="math/tex">\mathbf{X}</script>, then <script type="math/tex">(T(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>.</p>

<p>So, the example above boils down to</p>

<script type="math/tex; mode=display">\frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{T(\mathbf{x})}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{T(\mathbf{x})}}</script>

<p>which does not depend on <script type="math/tex">\theta</script> so we come to the same conclusion. This seems redundant. It is, really, but the benefit is we don‚Äôt have to consider <script type="math/tex">T(\mathbf{X}) = t</script> and <script type="math/tex">T(\mathbf{X}) \neq t</script>. So, originally, we <em>techincally</em> had to say that 0 did not depend on <script type="math/tex">p</script>, which is obvious. In this second definition, we don‚Äôt even need to consider that.</p>

<p>Let‚Äôs try another example using this second definition of sufficiency. Let‚Äôs check whether <script type="math/tex">T(\mathbf{X}) = \bar{X}</script> is sufficient for <script type="math/tex">\mu</script> in the normal distribution (with known variance <script type="math/tex">\sigma^2</script>).</p>

<script type="math/tex; mode=display">f_X(x \vert \mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x - \mu)^2}{2\sigma^2}\right)</script>

<p>Recall that <script type="math/tex">\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})</script>. Therefore,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert \mu)}{q(T(\mathbf{X}) \vert \mu)} = \frac{(\frac{1}{\sqrt{2\pi\sigma^2}})^n\textrm{exp}\left(- \displaystyle\sum_{i=1}^n\frac{(x_i - \mu)^2}{2\sigma^2}\right)}{\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-n(\bar{x} - \mu)^2}{2\sigma^2}\right)} = \mathcal{K} \frac{\textrm{exp}\left(\frac{- \sum_{i=1}^n(x_i^2 - 2x_i\mu)}{2\sigma^2}\right)}{\textrm{exp}\left( \frac{2\bar{x}n \mu - n\bar{x}^2}{2 \sigma^2}\right)}</script>

<script type="math/tex; mode=display">= \mathcal{K}\frac{\textrm{exp}\left( \frac{-\sum_{i=1}^{n}x_i^2}{2 \sigma^2}\right)}{\textrm{exp}\left(\frac{-n\bar{x}^2}{2 \sigma^2}\right)}</script>

<p>which doesn‚Äôt depend on <script type="math/tex">\mu</script>! So, the sample mean is a sufficient statistic for the population average in a normal distribution - hence why we use it all the time!</p>

<p>Now, moving on to even more ways to show sufficiency‚Ä¶</p>

<h4 id="factorization-theorem">Factorization Theorem</h4>

<p><strong>Factorization Theorem</strong> : <script type="math/tex">T(\mathbf{X})</script> is sufficient if <em>and only if</em> there exists <script type="math/tex">g(t\vert \theta)</script> and <script type="math/tex">h(\theta)</script> such that <script type="math/tex">\forall \mathbf{x}, \theta</script>,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta) = g(T(\mathbf{x})\vert \theta)h(\mathbf{x})</script>

<p>Since this is an <em>iff</em>, we have to prove it in both directions. First,</p>

<p><strong>Proof</strong>: Sufficiency <script type="math/tex">\implies</script> Factorization</p>

<p>This is the easy case.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x}\vert \theta) = P(\mathbf{X} = \mathbf{x} \vert \theta)</script>

<script type="math/tex; mode=display">= P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t \vert \theta)</script>

<p>and, by the definition of conditional probability,</p>

<script type="math/tex; mode=display">= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t,\theta)</script>

<p>and, by sufficiency (i.e. we assume given <script type="math/tex">T(\mathbf{X})</script>, the distribution of the data is free of <script type="math/tex">\theta</script>).</p>

<script type="math/tex; mode=display">= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t)</script>

<script type="math/tex; mode=display">= g(t \vert \theta)h(\mathbf{x})</script>

<p>and we have proved that direction. Now,</p>

<p><strong>Proof</strong>: Factorization <script type="math/tex">\implies</script> Sufficiency</p>

<p>Let <script type="math/tex">q(y \vert \theta)</script> be the pmf of <script type="math/tex">T(\mathbf{X})</script> and let <script type="math/tex">A_t</script> be the set containing all the possible data that yield a statistics <script type="math/tex">T(\mathbf{x}) = t</script>. That is,</p>

<script type="math/tex; mode=display">A_t = \{ \mathbf{y}: T(\mathbf{y}) = t\}</script>

<p>Then,</p>

<script type="math/tex; mode=display">q(t \vert \theta) = P(T(\mathbf{X}) = t \vert \theta) = \displaystyle\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)</script>

<p>So, since we are assuming factorization,</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x}\vert\theta)}{q(t \vert \theta)} = \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)}</script>

<script type="math/tex; mode=display">= \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} g(T(\mathbf{y}) \vert \theta) h(\mathbf{y})}</script>

<p>and, since for <script type="math/tex">y \in A_t</script>, <script type="math/tex">T(\mathbf{y}) = t = T(\mathbf{x})</script></p>

<script type="math/tex; mode=display">= \frac{h(\mathbf{x})}{\sum_{y \in A_T}h(\mathbf{y})}</script>

<p>and this does not ever depend on <script type="math/tex">\theta</script>, so <script type="math/tex">T(\mathbf{X})</script> is sufficient and we have completed the proof!</p>

<p>So, this makes things wicked easy‚Ä¶. let‚Äôs go back to the two examples we have done above. First, <script type="math/tex">\mathbf{X} \sim \textrm{Bernoulli}(p)</script>.</p>

<script type="math/tex; mode=display">f(\mathbf{x} \vert p) = \displaystyle\prod p^{x_i}(1-p)^{1- x_i} = p^{\sum x_i}(1-p)^{n - \sum x_i} = p^t(1-p)^{n-t}</script>

<p>and then <script type="math/tex">g(t \vert p) = p^t(1-p)^{n-p}</script> and <script type="math/tex">h(\mathbf{y}) = 1</script>, then we can see that <script type="math/tex">\sum_{i= 1}^n X_i</script> is sufficient for <script type="math/tex">p</script>.</p>

<p>Now, let us consider the second, normal with known variance, example.</p>

<script type="math/tex; mode=display">f_X(\mathbf{x} \vert \mu) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)</script>

<script type="math/tex; mode=display">= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right) \times \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)</script>

<p>so <script type="math/tex">h(\mathbf{x}) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right)</script> and <script type="math/tex">g(t \vert \mu) = \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)</script> when <script type="math/tex">t = \bar{x}</script>, so the sample mean is sufficient for <script type="math/tex">\mu</script>. Way easier - we like that.</p>

<p>Let‚Äôs try a new example. Consider <script type="math/tex">\mathbf{X} \sim \textrm{Uniform}(0,\theta)</script>. What is a sufficient statistic for <script type="math/tex">\theta</script>? To assess this, let‚Äôs use the factorization theorem. Remember,</p>

<script type="math/tex; mode=display">% <![CDATA[
f(x \vert \theta) = \begin{cases} 
\frac{1}{\theta} & \textrm{if} \; x \in [0,\theta] \\
0 & \textrm{else}
\end{cases} %]]></script>

<p>which is identical to</p>

<script type="math/tex; mode=display">\frac{1}{\theta}I(0 \leq x) I (x \leq \theta)</script>

<p>so, therefore,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta) = \displaystyle\prod_{i=1}^n \frac{1}{\theta}I(0 \leq x_i) I (x_i \leq \theta) = \theta^{-n}I(x_{(n)} \leq \theta) I(0 \leq x_{(1)})</script>

<p>so we can conclude that the max, <script type="math/tex">X_{(n)}</script>, is a sufficient statistic for <script type="math/tex">\theta</script>.</p>

<p>Now, let‚Äôs consider a two-dimensional (<em>whoa</em>) sufficient statistic! When I learned this, the professor made this seem very straight forward - but I know things get real iffy real quick when the dimension of things increase - so lets take it slow. What we are looking for now is <script type="math/tex">T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))</script> such that the factorization theorem still applies. Let‚Äôs consider a normal distribution with unknown variance. Now, what we need is to find a factorization</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = g(T_1(\mathbf{x}), T_2(\mathbf{X}) \vert \mu, \sigma^2)h(\mathbf{x})</script>

<p>This differs from the one dimensional case where we kind of just tossed the parts of the exponent that did not depend on <script type="math/tex">\mu</script> into the constant and called it a day. Now, we can do that but only for the parts of the exponent that don‚Äôt depend on <script type="math/tex">\mu</script> and <script type="math/tex">\sigma^2</script> which is none of them so trick-statement you can‚Äôt toss anything into the constant from the exponent. Turns out that constant infront isn‚Äôt even a constant anymore since it involves <script type="math/tex">\sigma^2</script>. Bummer. All this means, though, is more algebra - which, although it is tedious, is not difficult if you pay attention (which <em>I know</em> is difficult). So, let‚Äôs get to it.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)</script>

<script type="math/tex; mode=display">= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-\sum_{i=1}^n((x_i - \bar{x}) + (\bar{x} - \mu))^2}{2\sigma^2}\right)</script>

<script type="math/tex; mode=display">= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 +2 \sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu)  - n(\bar{x} - \mu)^2\right)\right)</script>

<p>and now note that 
<script type="math/tex">\sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu) = \bar{x}\sum x_i - \mu\sum x_i - n\bar{x} + n\bar{x}\mu = 0</script></p>

<p>so, therefore,</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 - n(\bar{x} - \mu)^2\right)\right)</script>

<p>and by using <script type="math/tex">h(\mathbf{x}) = 1</script>, we have the factorization theorem! Therefore,</p>

<script type="math/tex; mode=display">T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (\bar{X}, \sum_{i=1}^n (X_i - \bar{X})^2)</script>

<p>is a sufficient statistic for <script type="math/tex">\mathbf{\phi} = (\mu, \sigma^2)</script></p>

<p>Let‚Äôs try another one - <script type="math/tex">\mathbf{X} \sim \textrm{Uniform}(\alpha, \theta)</script>. Extending our knowledge of the one-dimensional case, this should be fairly simple.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}(\mathbf{x} \vert \theta, \alpha) = \displaystyle\prod_{i=1}^n \frac{1}{\theta - \alpha}I(\alpha \leq x_i) I (x_i \leq \theta) = (\theta - \alpha)^{-n}I(x_{(n)} \leq \theta) I(\alpha \leq x_{(1)})</script>

<p>so we can conclude that</p>

<script type="math/tex; mode=display">T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (X_{(1)}, X_{(n)})</script>

<p>is suffcient for <script type="math/tex">\phi = (\alpha,\theta)</script></p>

<h2 id="minimal-sufficient-statistic">Minimal Sufficient Statistic</h2>

<p>As you have (hopefully) noticed, sufficient statistics are not even close to unique. For instance, the sample itself is always sufficient. The order statistics are also always sufficient as we can set <script type="math/tex">h(\mathbf{x})</script> to be 1 and then the joint distribution is the same, just reordered (see the examples section for more detials on this). Also, any one-to-one function of a sufficient statistic is also a sufficient statistic, as that value maps uniquely back to the original sufficient statistic. Therefore, we need some notion of <em>better</em> when it comes to these types of statistics. That is what the idea of a <strong>minimal sufficient statistic</strong> is.  The idea is to find a sufficient statistic that is more sufficent than the rest - i.e. one that has the property of maximum data reduction? The defintion of the minimal sufficient statistic is as follows:</p>

<p>A sufficient statistic <script type="math/tex">T(\mathbf{X})</script> is a minimal sufficient statistic if, for every other suffcient statistic <script type="math/tex">T'(\mathbf{X})</script>, <script type="math/tex">T(\mathbf{X})</script> is a function of <script type="math/tex">T'(\mathbf{X})</script>.</p>

<p>This definition is fairly unclear, in my humble opinion. I guess not unclear, but fails to provide the intuition behind it. So lets try to explain it a bit better. The idea behind sufficient statistics was to reduce to data into a <strong>coarser</strong> partition, without losing information about the parameter of interest. We now want the coarsest partition of data we can get - the maximum data reduction. So lets consider a function, <script type="math/tex">f(x)</script>. Remember, a function is defined such that, if <script type="math/tex">x_1 = x_2</script>, <script type="math/tex">f(x_1) = f(x_2)</script> - but this does not mean that if <script type="math/tex">f(x_1) = f(x_2)</script>, then <script type="math/tex">x_1 = x_2</script>. What does this mean in our case? It means, if we have a partition, we can make it more coarse (i.e the domain smaller) by finding a function of that partition that maps to a smaller partition. That is, if <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script> are partitions of a sample space <script type="math/tex">\mathcal{X}</script>, then a function can <strong>only</strong> map to a coarser partition. That is, a function will map both to the same partition if they are the same, but may also map them to the same partition if they are different. Also, a function will hit all values in its outcome space. So, at worst, a function will map all partitions in the first partition space to all partions in the other sample space and, at best, will map a few of the original partitions to the same partition. Therefore, a function makes the sample space more coarse. So, if <script type="math/tex">T</script> is a function of all other <script type="math/tex">T'</script>, then it is a partition is the most coarse - it has the maximum reduction of data. This is a nice definition but, in practice, it does not really help us. The following is a theorem that <em>can</em> help:</p>

<p>The ratio <script type="math/tex">\frac{f(\mathbf{x} \vert \theta)}{f(\mathbf{y} \vert \theta)}</script> is free of <script type="math/tex">\theta</script> <em>if and only if</em> <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script>, then then <script type="math/tex">T(\mathbf{X})</script> is a minimal sufficient statistic.</p>

<p>This is helpful! We can use this practically. The proof is a bit difficult, but you can see this definition as arising from the Factorization Theorem. Before we see some examples with some known distributions, lets talk facts about this minimal sufficient statistic.</p>

<p>Is a minimal sufficient statistic unique? <strong>No</strong>. This is the first fact. It turns out that any injective function of <script type="math/tex">T(\mathbf{X})</script> is also a minimal sufficient statistic. Why does this make sense? Intuitively, a one to one function of <script type="math/tex">T(\mathbf{X})</script> maps the partitions created by <script type="math/tex">T(\mathbf{X})</script> uniquely to partitions in a different partitioned space. Since <script type="math/tex">T(\mathbf{X})</script> is a minimal sufficient statistic, it achieves the coarsest possible partition space (i.e with the smallest cardinality), so the mapping of a function of <script type="math/tex">T(\mathbf{X})</script> has to map to the smallest number of possible partitions, by default.</p>

<p>A more formal proof is as follows. To understand why this proof is as follows, remember that <em>only</em> a 1-1 function has an inverse:</p>

<p><strong>Proof</strong>:</p>

<p>Let <script type="math/tex">T^*(\mathbf{X}) = b(T(\mathbf{X}))</script> be a one-to-one function. Then, <script type="math/tex">\exists \; b^{-1}</script> such that <script type="math/tex">b^{-1}(T^*(\mathbf{X})) = T(\mathbf{X})</script>. Thus, knowing <script type="math/tex">T^*(\mathbf{X})</script> assures that we know <script type="math/tex">T(\mathbf{X})</script> via <script type="math/tex">b^{-1}(.)</script>. We can also see that, by the factorization theorem, when <script type="math/tex">T</script> is sufficient, <script type="math/tex">\exists \; h, \; g</script> such that <script type="math/tex">f(\mathbf{x} \vert \theta) = g(T(\mathbf{x}) \vert \theta)h(\mathbf{x}) = g(b^{-1}(T^*(\mathbf{X}))\vert \theta)h(\mathbf{x})</script>. Thus, <script type="math/tex">T^*(\mathbf{X})</script> is sufficient for <script type="math/tex">\theta</script>. This shows that <script type="math/tex">T^*(\mathbf{X})</script> is a sufficient by the Factorization Theorem. Now, assume there is another sufficient statistic <script type="math/tex">T_1(\mathbf{X})</script>. Since <script type="math/tex">T</script> (the original one) is minimal sufficient, then <script type="math/tex">T = q(T_1(\mathbf{X}))</script>. Therefore, <script type="math/tex">T^* = b(T) = b(q(T_1))</script>. Therefore, <script type="math/tex">T^*(\mathbf{X})</script> is a function of any other sufficient statistic, and is thus a minimal sufficient statistic.</p>

<p>This fact actually leads us to the next fact - there is <em>always</em> a one-to-one function between two minimally sufficient statistics. To show this, suppose there are two minimally sufficient statistics <script type="math/tex">\mathbf{T_1}, 
\mathbf{T_2}</script>. Then, <script type="math/tex">\mathbf{T_1} = f(\mathbf{T_2})</script> and <script type="math/tex">\mathbf{T_2} = g(\mathbf{T_1})</script>. Thus, <script type="math/tex">\mathbf{T_1} =f(g(\mathbf{T_1}))</script> which implies that <script type="math/tex">f= g^{-1}</script> and therefore <script type="math/tex">f</script> is one-to-one. The same can be shown for <script type="math/tex">g</script>. What this means practically is that the partition created by a minimal sufficient statistic is unique.</p>

<p>And‚Ä¶ now some totally non-boring to examples, so you can pass your class. ;).</p>

<p>Lets go back to the OG example of this post. Let <script type="math/tex">X_1, \dots, X_n \sim \textrm{Binomial}(n,p)</script>. Is <script type="math/tex">\sum X_i</script> a minimal sufficient statistic? We know it is sufficient because we already proved it so refresh your memory if you already forgot about this. To prove minimal sufficiency, we have to prove it both ways. So first, lets look at the ratio</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{f_{\mathbf{X}}(\mathbf{y} \vert p)} = \frac{\binom{n}{\sum x_i}p^{\sum x_i}(1-p)^{n - \sum x_i}}{\binom{n}{\sum y_i}p^{\sum y_i}(1-p)^{n - \sum y_i}} \propto_{p} p^{\sum x_i - \sum y_i}(1-p)^{\sum y_i - \sum x_i}</script>

<p>Now, if <script type="math/tex">\sum y_i = \sum x_i</script>, then the ratio is 1 and thus free of <script type="math/tex">p</script>. If the ratio is free of <script type="math/tex">p</script> only if <script type="math/tex">\sum y_i = \sum x_i</script>, so then <script type="math/tex">\displaystyle\sum_{i=1}^n X_i</script> is a minimal sufficient statistic for <script type="math/tex">p</script>. Great - now lets look at an example where something might not be a minimal sufficient statistic in this same data. Consider the case where <script type="math/tex">n=3</script> and the associated two-dimensional statistic - <script type="math/tex">T(\mathbf{X}) = (X_1 + X_2, X_3)</script>. Is this sufficient? Yes, it is.</p>

<script type="math/tex; mode=display">f_{\mathbf{X}}( \mathbf{x} \vert p) = \binom{n}{x_1 + x_2 + x_3}p^{x_1 + x_2 + x_3}(1-p)^{3 - x_1 -x_2 - x_3} \propto_p p^{x_1 + x_2}(1-p)^{2 - x_1 - x_2}p^{x_3}(1-p)^{1-x_3}</script>

<p>which, by the Factorization Theorem, is sufficient. Good? But now, is is minimal sufficient? Intuitively, we should say no. Why? You tell me. Okay - I tell you. Consider the minimal sufficient statistic (derived above) <script type="math/tex">X_1 + X_2 + X_3</script>. This statistic takes the partitions (values) of 0,1,2, and 3. Our current statistic has, as an example, partitions (0,1) and (1,0), which are different. However, they both correspond to the <em>coarser</em> partition generated by <script type="math/tex">X_1 + X_2 + X_3</script>. So, there is always a partition of smaller cardinality. Let‚Äôs show this mathematically.</p>

<script type="math/tex; mode=display">\frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{f_{\mathbf{X}}( \mathbf{y} \vert p)} \propto \frac{p^{x_1 + x_2}(1-p)^{2 - x_1 - x_2}p^{x_3}(1-p)^{1-x_3}}{p^{y_1 + y_2}(1-p)^{2 - y_1 - y_2}p^{y_3}(1-p)^{1-y_3}}</script>

<p>So, this is free of <script type="math/tex">p</script> if <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script>. But, is it free of <script type="math/tex">p</script> only if that is the case? No! If <script type="math/tex">y_3  = x_1 + x_2</script> and <script type="math/tex">x_3 = y_1 + y_2</script>, then it is free of <script type="math/tex">p</script> but the statistics are not equal! This is exactly what our intuition above says. If we switch the order of the ordered pair, we map to the same partition in the partition space of another statistic (namely the one defined above). So we can always define a statistic with a coarser partition (a more basic statistic that contains the same information about <script type="math/tex">p</script>).</p>

<h3 id="ancillary-statistics">Ancillary Statistics</h3>

<p>Now - we will talk about ancillary statistics. This mean seem like it is out of left field, and it is, but it is still a pretty basic part of inference (and a type of statistic) so we are going to cover it.</p>

<p>First, lets talk about the word ancillary. The first time I heard this word, actually, was from a professor I had who was from India and had learned English with a British accent, so I now say ancillary anc√≠llary, not ancill√°ry. Anyways - to the point.</p>

<p>A statistic is ancillary if its distribution does not depend on the parameter <script type="math/tex">\theta</script>.</p>

<p>How is this different from a sufficient statistic? At first glance, the definitions seem kind‚Äôve similiar. They aren‚Äôt at all. A sufficient statistic is a statistic that contains all the relevant information about $\theta$ in the data. An ancillary statistic, by defintion, do not contain any information about the parameter at all. I know what you are thinking - so, if they contain no information about the parameter, why in the <em>world</em> should I care about these things? We will get back to that point.</p>

<p>First, some examples. Let <script type="math/tex">X_1, \dots,X_n \sim \mathcal{N}(\mu,\sigma^2)</script> where <script type="math/tex">\sigma^2</script> is known. Consider <script type="math/tex">T_1 = X_1 - \mu</script>. This is not, because the statistic is a function of the parameter <script type="math/tex">\mu</script>. What about <script type="math/tex">T_2 = X_1 - X_2</script>? Yes. Since both are Gaussian, the distribution of <script type="math/tex">T_2</script> is Gaussian with expected value 0 and variance <script type="math/tex">2\sigma^2</script>, by convolusion. Therefore, the distribution of <script type="math/tex">T_2</script> does not depend on <script type="math/tex">\mu</script> and thus is ancillary. Also, remember <script type="math/tex">\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2</script>. So, the sample standard deviation is also an ancillary statistic for <script type="math/tex">\mu</script>! These are some realtively simple examples, and ones that maybe we should have known off the top of our heads (just an assumption, since you are reading this). How about we look at something a bit more difficult?</p>

<p>What if we have <script type="math/tex">X_1, \dots, X_n \sim \mathcal{N}(0,\sigma^2)</script> and we define Y_i to be <script type="math/tex">X_i/sigma</script>. Then, while is not ancillary because it is a function of <script type="math/tex">\sigma^2</script>, it does follow a standard normal distribution now. Therefore, we can show that <script type="math/tex">\frac{X_1}{X_2} = \frac{\sigma Y_1}{\sigma Y_2} = \frac{Y_1}{Y_2}</script> which, from ratio distributions, is distributed Cauchy. So, by using that transform, we have found a way to use the original data to create an ancillary statistic. Notice, actually, that any function of <script type="math/tex">\mathbf{Y}</script> does not depend on <script type="math/tex">\sigma^2</script> at all, so constructing ancillary statistics is not hard at all, in this case.  Now let‚Äôs look at a much more difficult problem.</p>

<p>Let <script type="math/tex">X_1, \dots , X_n \sim \textrm{Uniform}(\theta, \theta +1)</script>. Show that the range, <script type="math/tex">R = X_{(n)} - X_{(1)}</script> is an ancillary statistic. There are two ways to do this - we can go through both. The first is to find the distribution of <script type="math/tex">R</script> and show that it does not depend on <script type="math/tex">\theta</script> - not trivial. To start, we need the joint distribution of the min and the max. This is given in Casella &amp; Berger, Thm. 5.4.6. For any pair of order statistics, the joint distribution is given by</p>

<script type="math/tex; mode=display">f_{X_{(i)},X_{(j)}}{y_1,y_2 \vert \theta} = \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_{X}(y_1)f_{X}(y_2) \left[ F(y_1) \right]^{i-1} \left[ F(y_2) - F(y_1)\right]^{j-i-1}[1-F(y_2)]^{n-j}</script>

<p>Therefore, the joint distrbution of the min and the max is</p>

<script type="math/tex; mode=display">\frac{n!}{(n-2)!}f_X(x_{(1)})f_X(x_{(n)})[F(x_{(n)}) - F(x_{(1)})]^{n-2}</script>

<p>Now, we use our knowledge of the uniform distribution! Note that</p>

<script type="math/tex; mode=display">% <![CDATA[
f_X(x \vert \theta) = I(\theta < x \ \theta + 1) %]]></script>

<p>and that</p>

<script type="math/tex; mode=display">% <![CDATA[
F_X(x \vert \theta ) = 
\begin{cases}
   0 & \;\;\textrm{if} \;\; x \leq \theta \\
   x - \theta & \;\;\textrm{if} \;\; \theta < x < \theta +1 \\
   1 & \;\;\textrm{if} \;\; x \geq \theta + 1 \\
   
\end{cases} %]]></script>

<p>Therefore, the join distribution of <script type="math/tex">X_{(1)}, X_{(n)}</script> is given by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{cases}
n(n-1)(x_{(n)} - x_{(1)})^{n-2} & \;\; \textrm{if} \;\; \theta < x_{(1)} < x_{(n)} < \theta + 1 \\
0 & \;\; \textrm{else}
\end{cases} %]]></script>

<p>Now, we need to find the distribution of the range! To do this, let <script type="math/tex">M = \frac{X_{(1)} + X_{(n)}}{2}</script> be the median. Then, <script type="math/tex">X_{(1)} = \frac{2M - R}{2}</script> and <script type="math/tex">X_{(n)} = \frac{2M+R}{2}</script>. The Jacobian is then</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathbf{J} = 
\left[\begin{matrix}
\frac{\partial X_{(1)}}{\partial R} & \frac{\partial X_{(1)}}{\partial M} \\ 
\frac{\partial X_{(n)}}{\partial R} & \frac{\partial X_{(n)}}{\partial M}
\end{matrix}\right] = 
\left[\begin{matrix}
-\frac{1}{2} & 1 \\ 
\frac{1}{2} & 1
\end{matrix}\right] %]]></script>

<p>and the determinant here is 1. Therfore me can just plug and chug, no need to add in <script type="math/tex">\vert \mathbf{J} \vert</script> to our new pdf. We have the same domain restrictions, just now <script type="math/tex">% <![CDATA[
\theta < \frac{2m - r}{2} < \frac{2m + r}{2} < \theta + 1 %]]></script>. If we fix <script type="math/tex">r</script>, we can see that <script type="math/tex">% <![CDATA[
\theta + r/2 < m < \theta + 1 - r/2 %]]></script> and we can see that the domain of <script type="math/tex">r</script> is <script type="math/tex">(0,1)</script>. This is all the information we now need to know to derive the distribution of <script type="math/tex">R</script>. It follows from this that</p>

<script type="math/tex; mode=display">f_R(r) = \displaystyle\int_{\theta + r/2}^{\theta + 1 - r/2} n(n-1)r^{n-2}dm = n(n-1)r^{n-2}(1-r)</script>

<p>and this doesn‚Äôt depend on <script type="math/tex">\theta</script>! So the range is ancillary. Another thing to notice here - this totally sucked to derive. I know it. You know it. We <em>all</em> know it. Lets do it a way easier way! Define <script type="math/tex">Y =  X_i - \theta</script>. Then,</p>

<script type="math/tex; mode=display">% <![CDATA[
f_Y(y) = I(\theta < y + \theta < \theta + 1) \vert \frac{dx}{dy}\vert = I(0 < y < 1) %]]></script>

<p>which is Uniform(0,1). Wow, so now <script type="math/tex">R = X_{(n)} - X_{(1)} = (Y_{(n)} + \theta )-(Y_{(1)} + \theta) = Y_{(n)} - Y_{(1)}</script> which has nothing to do with <script type="math/tex">\theta</script>, andwe can conclude the range is ancillary. Way easier. And similar to the example above the hard example above. Kinda seems like there might be a point.. there is!</p>

<p>Suppose <script type="math/tex">\mathbf{X} \sim f(x - \theta)</script> where <script type="math/tex">f</script> is an arbitrary distrbution. Define <script type="math/tex">Y</script> as above. Then <script type="math/tex">f_Y(y) = f_X{y + \theta - \theta}</script> which does not depend on <script type="math/tex">\theta</script>. So the range cancels out the <script type="math/tex">\theta</script> as it does above, and we again get a distribution that does not depend on <script type="math/tex">\theta</script>. Seems like for siome distributions, we can just divide the random variables and get an ancillary statistic, and for other types of distributions we can just use the range. This is where the idea of a <em>location-scale family</em> comes in.</p>

<p>If <script type="math/tex">f(x)</script> is a pdf, the <script type="math/tex">\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})</script> is also a pdf, specifically a <em>location-scale family with standard pdf</em> <script type="math/tex">f(x)</script>. Here, <script type="math/tex">\mu</script> is called the location parameter and <script type="math/tex">\sigma</script> is called the scale parameter. For instance, <script type="math/tex">\mathcal{N}(\mu,\sigma^2)</script> is a location-scale family with standard pdf <script type="math/tex">\mathcal{N}(0,1)</script> and location parameter <script type="math/tex">\mu</script> and scale parameter <script type="math/tex">\sigma</script>.</p>

<p>We showed above that the range is an ancillary statistic for a location family. Now, we can show that the ratio <script type="math/tex">X_1/X_n</script> is an ancillary statistic for an arbitrary scale family with pdf <script type="math/tex">f(x/\sigma)/\sigma</script> with <script type="math/tex">\sigma > 0</script>.</p>

<p>Define <script type="math/tex">Y = \frac{X}{\sigma}</script>. Then, <script type="math/tex">X = \sigma Y</script> and <script type="math/tex">dx/dy = \sigma</script>. Thefore, <script type="math/tex">f_Y(y) = \frac{1}{\sigma}f_X(\sigma y/\sigma) \sigma = f(y)</script>. Then, <script type="math/tex">\frac{X_1}{X_n} = \frac{\sigma Y_1}{\sigma Y_n} = \frac{Y_1}{Y_{n}}</script> which does not depend on <script type="math/tex">\sigma</script>.</p>

<p>So its been fun learning about ancillary statistics.. but why does it matter again? Turns out that, while ancillary statistics do not contain any information about <script type="math/tex">\theta</script>, they can help up the precision in our estimation of <script type="math/tex">\theta</script>. How, do you say? Consider the Uniform(<script type="math/tex">\theta, \theta + 1</script>) case, and say we know the range is 0.8. This doesn‚Äôt tell us anything about <script type="math/tex">\theta</script> <em>alone</em>. But say we also know the median is 1. If we <em>only</em> knew the median was 1, we would only know that <script type="math/tex">% <![CDATA[
0 < \theta < 1 %]]></script>.  Now, by knowing the range <em>and</em> the median, we now that <script type="math/tex">X_{(1)} = 0.6</script> and <script type="math/tex">X_{(n)} = 1.4</script>. From this information, we can gather that <script type="math/tex">\theta</script> must be in the range of <script type="math/tex">% <![CDATA[
0.4 < \theta < 0.6 %]]></script> because it can‚Äôt be more than 0.6 (or we wouldn‚Äôt observe the minimum we have) and vice versa for 0.4. So, in summary, ancillary statistics, <em>when combined with other statistics</em>, can improve our understanding of the parameter. However, alone, they do nothing for us. They‚Äôre like a friend‚Äôs friend. Lots of fun to hang out with, but only when your mutual friend is around. Otherwise, things get awkward and you just want to go home.</p>

<h2 id="complete-statistics">Complete Statistics</h2>

<p>So lets tie all this together (not really, but lets wrap this up with final discussion about statistics).</p>

<p>We will end this by talking about <strong>complete statistics</strong>. Sounds cool. What is it? It‚Äôs actually defined through familes of distributions (like we talked about above). Let <script type="math/tex">\mathcal{P} = \{ p(t \vert \theta), \theta \in \Theta \}</script> be a family of distributions for <script type="math/tex">T(\mathbf{X})</script>. If <script type="math/tex">E\left[ g(T) \vert \theta \right] = 0 \; \; \forall \; \theta \; \implies P[g(T) = 0 \vert \theta] = 1 \; \forall \; \theta</script>, then <script type="math/tex">T(\mathbf{X})</script> is called a <strong>complete statistic</strong>! 
Really, completeness is a property of the family of distributions. To call a statistic itself complete is almost misleading. To get some more intuition into this problem, notice that, in the discrete case,</p>

<script type="math/tex; mode=display">E_{\theta}\left[ g(t) \right] = \sum_{i} g(t_i) \times P_{\theta}(T = t_i) = g(t_1) \times p_{\theta}(t_1) + \dots = 0</script>

<p>Therefore, for <script type="math/tex">T</script> to be complete, this must mean that <script type="math/tex">g(t)</script> above is almost surely 0 - i.e there is no non-trivial (occuring with probabilty 0) <script type="math/tex">g(t)</script> that is not zero. This is analagous to the idea of linear independence of vectors. Above, we only have one.. or do we? We actually have a set of equations that corresponds to the number of <script type="math/tex">\theta \in \Theta</script>. So, we have</p>

<script type="math/tex; mode=display">\vec{g(\mathbf{x})} \times \mathcal{P}_{\Theta} = 0</script>

<p>This can only happen when either <script type="math/tex">g(T) = 0</script> or <script type="math/tex">\mathcal{P}</script> does not change across separate values of <script type="math/tex">\theta</script>, or if it changes very simply (i.e scaled by a constant). Therefore, completeness guarantees that distributions parameterized by different values of <script type="math/tex">\theta</script> are distinct. If you change <script type="math/tex">\theta</script>, you change the whole thing. Also, conceptually note that there is a ‚Äúno nonsense‚Äù part of this definition. It says ‚Äúif <script type="math/tex">g</script> is expected to be zero with respect to <script type="math/tex">\theta</script>, then it is just zero‚Äù. That is, there is no part of the distribution of <script type="math/tex">g</script> that does not depend on <script type="math/tex">\theta</script>. No nonsense. One thing to note from this is that, if you can make an ancillary statistic out of <script type="math/tex">T(\mathbf{X})</script>, then it cannot be complete‚Ä¶ which follows immediately from what was stated above. Another thing to note - this is all defined by the family of distributions <script type="math/tex">\mathcal{P}</script>, that depends on the set <script type="math/tex">\Theta</script>. A statistic may be complete for a certain portion of that set, and may not be complete for another.<br />
Let‚Äôs check out some examples. Suppose <script type="math/tex">\mathbf{X} \sim \textrm{Uniform}(\theta,\theta + 1)</script> and <script type="math/tex">T(\mathbf{X}) = (X_{(1)}, X_{(n)})</script>. Is this a complete statistic? We should think, intuitively, that it is not. We can make an ancillary statistic (the range) out of this statistic. A complete statistic has no non-trivial part that doesn‚Äôt depend on <script type="math/tex">\theta</script>, but nothing about the range depends on <script type="math/tex">\theta</script>. Let <script type="math/tex">g(T) = R - E[R]</script>. Now notice that <script type="math/tex">E[R]</script> is a constant with respect to <script type="math/tex">\theta</script>. Therefore, <script type="math/tex">g(T)</script> is a non-zero function, but <script type="math/tex">E[g(T)] \; \forall \; \theta</script>. Therfore, <script type="math/tex">T</script> is not complete!</p>

<p>Now suppose <script type="math/tex">T \sim \textrm{Poisson}(\lambda), \; \lambda \in \{1,2\}</script>. Now, if <script type="math/tex">E_{\lambda=1}[g(t)] = 0</script>, <script type="math/tex">E_{\lambda=1}[g(t)] = 0</script> implies that <script type="math/tex">\sum_{t} \frac{g(t)}{t!} = \sum_{t} \frac{g(t)2^t}{t!} =  0</script> which clearly does not mean that <script type="math/tex">g(t)</script> must be zero, and you can think of <em>lots</em> of examples. So this family is not complete. Notice how we are talking about a family of distributions‚Ä¶ and remember how I said it depended on the range of the parameter. Now, suppose</p>

<script type="math/tex; mode=display">T \sim \textrm{Poisson}(\lambda), \; \; \lambda \in \mathbb{R}^+</script>

<p>Now, suppose there is <script type="math/tex">g</script> such that <script type="math/tex">E[g(T)] = 0</script> for all <script type="math/tex">\lambda</script>. Then,</p>

<script type="math/tex; mode=display">\sum_{t = 0}^{\infty} \frac{g(t)}{t!}\lambda^t = 0 = \sum_{t = 0}^{\infty} \phi(t) \lambda^t = 0</script>

<p>Since <script type="math/tex">\lambda > 0</script>, this can only be 0 if <script type="math/tex">\phi(t) = 0 \; \forall \; t \; \implies \; g(t) = 0 \; \forall t</script>. Bam, complete. By adding more dimensionality to <script type="math/tex">\Omega</script>, the parameter space for <script type="math/tex">\lambda</script>, we make the family complete. Why is this? Becauase the larger the family (i.e the more paramters), the more constraints <script type="math/tex">g</script> must satisfy. At some point, the only such <script type="math/tex">g</script> is the trivial <script type="math/tex">g</script>, 0. This means the family is complete.</p>

<p>Suppose <script type="math/tex">X_1, \dots, X_n \sim \textrm{Uniform}(0,\theta)</script>. Show that <script type="math/tex">X_{(n)}</script> is complete.</p>

<p>The distribution of <script type="math/tex">T</script> is</p>

<script type="math/tex; mode=display">f(t \vert \theta) = n f_X(t) \left[ F_X(t) \right] = n\frac{1}{\theta} \left[ \frac{t}{\theta} \right]^{n-1}</script>

<p>So, assume that <script type="math/tex">E[g(T)] = 0</script>. Then,</p>

<script type="math/tex; mode=display">\displaystyle\int_{0}^{\theta} g(t)\frac{n}{\theta^n}t^{n-1} = 0 \implies \displaystyle\int_{0}^{\theta} g(t)t^{n-1} = 0</script>

<p>and, by the Fundamental Theorem of Calculus,</p>

<script type="math/tex; mode=display">\displaystyle\int_{0}^{\theta} g(t)t^{n-1} = \displaystyle\int_{0}^{\theta} F(t)dt = F(\theta) - F(0) = g(\theta)\theta^{n-1} = 0</script>

<p>so, since <script type="math/tex">\theta > 0 \; , \; g(\theta) = 0</script> for all <script type="math/tex">\theta > 0</script> - concluding the <script type="math/tex">X_{(n)}</script> is complete.</p>

<p>There‚Äôs just a few more things (2) to be said about complete statistics.</p>
<ol>
  <li>If a function of <script type="math/tex">T \; , \; h(T)</script> is ancillary, then <script type="math/tex">T</script> cannot be complete. This goes with what we said before - a complete statistic does not have any unncessary part associated with it. Let <script type="math/tex">g(T) = h(T) - E[h(T)]</script>. Since <script type="math/tex">h(T)</script> is ancillary, then <script type="math/tex">g(T) = 0</script> for all <script type="math/tex">\theta</script>. But, again, <script type="math/tex">g(T)</script> is non-zero. Or, at least, it doesn‚Äôt have to be zero.</li>
  <li>Say <script type="math/tex">T_1(\mathbf{X})</script> is complete. Then <script type="math/tex">h(T) = T_1</script> is also complete. So assume <script type="math/tex">E[g(h(T))] = E[g(T_1)] = 0</script>. Now, since <script type="math/tex">T</script> is complete, <script type="math/tex">P(g(h(T))) = P(g(T_1)) = 1</script> for all <script type="math/tex">\theta</script>.</li>
</ol>

<p>Great! Now, to put some things together: if a minimal sufficient statistic exists, then any complete sufficient statistic is also a minimal sufficient statistic. So, if you find a statistic that is sufficent and complete, it is also minimal sufficient (assuming a minimally sufficient statistic exists). However, a minimal sufficient statistic is not always complete. So, in summary, complete <script type="math/tex">\implies</script> minimal!</p>

<h2 id="more-examples">More Examples</h2>

<p><strong>1</strong>. Let <script type="math/tex">X_1, \dots, X_n</script> but iid from a density function of the form:</p>

<script type="math/tex; mode=display">% <![CDATA[
f(x \vert \sigma) = \frac{1}{\sigma}e^{-\frac{1}{\sigma}(x - \mu)}, \; \; \mu < x, \;\; 0 < \sigma %]]></script>

<p>Find a one-dimensional sufficient statistic for <script type="math/tex">\mu</script> (known <script type="math/tex">\sigma</script>), a one-dimensional sufficient statistic for <script type="math/tex">\sigma</script> (known <script type="math/tex">\mu</script>) and a two-dimensional sufficient statistic for (<script type="math/tex">\mu</script>, <script type="math/tex">\sigma</script>)</p>

<p><strong>Answer:</strong></p>

<script type="math/tex; mode=display">f(\mathbf{x} \vert \sigma) = \displaystyle\prod_{i=1}^n \frac{1}{\sigma}e^{-\frac{1}{\sigma}(x_i - \mu)} = \sigma^{-n}e^{\frac{-1}{\sigma}(\sum x_i - n\mu)}I(x_{(1)} > \mu)</script>

<script type="math/tex; mode=display">\sigma^{-n}e^{\frac{-\sum x_i}{\sigma}} e^{\frac{n \mu}{\sigma}}I(x_{(1)} > \mu)  = h(x)e^{\frac{n \mu}{\sigma}}I(t > \mu) = h(x)g(\mathbf{T} \vert \mu)</script>

<p>so, by the Factorization Theorem, <script type="math/tex">X_{(1)}</script> is a sufficent statistic for <script type="math/tex">\mu</script>. For the second part, note that the likelihood can also be written as</p>

<script type="math/tex; mode=display">h(x)g(\mathbf{T} \vert \sigma)</script>

<p>where <script type="math/tex">h(x) = e^{\frac{n \mu}{\sigma}}I(x_{(1)} > \mu)</script>  and <script type="math/tex">g(\mathbf{T} \vert \sigma) = \sigma^{-n}e^{\frac{-t}{\sigma}}</script> where <script type="math/tex">t = \sum_{i=1}^n x_i</script>. Therefore, by the Factorization Theorem again, <script type="math/tex">\sum_{i=1}^n X_i</script> is a sufficent statistic for <script type="math/tex">\sigma</script>. Furthermore, the likelihood can be written as</p>

<script type="math/tex; mode=display">\sigma^{-n}e^{\frac{-\sum x_i}{\sigma}} e^{\frac{n \mu}{\sigma}}I(x_{(1)} > \mu) = \sigma^{-n}e^{\frac{-t_1}{\sigma}} e^{\frac{n \mu}{\sigma}}I(t_2 > \mu) = h(x)g(\mathbf{T_1},\mathbf{T_2} \vert \mu, \sigma)</script>

<p>so, <script type="math/tex">(\mathbf{T_2}, \mathbf{T_1}) = (X_{(1)}, \sum_{i=1}^n X_i)</script> is sufficient for <script type="math/tex">(\mu,\sigma)</script>.</p>

<p><strong>2</strong>. Let <script type="math/tex">X_1, \dots, X_n \sim \mathcal{N}(0, \sigma^2)</script>.</p>

<p>Show that <script type="math/tex">\sum_{i=1}^n X_i^2</script> is sufficient for <script type="math/tex">\sigma^2</script> and determine whether it is a minimal sufficient statistic or not.</p>

<p><strong>Answer:</strong></p>

<script type="math/tex; mode=display">f(\mathbf{x} \vert \sigma^2) = (\frac{1}{\sqrt{2 \pi \sigma^2}})^n e^{\frac{-\sum x_i^2}{2 \sigma^2}}I(\sigma^2 > 0) = (2\pi)^{-n/2}(\sigma)^{-n}e^{\frac{-\sum x_i^2}{2 \sigma^2}}</script>

<script type="math/tex; mode=display">= (2\pi)^{-n/2}(\sigma)^{-n}e^{\frac{-t}{2 \sigma^2}} = h(\mathbf{x})g(T(\mathbf{X}) \vert \sigma^2)</script>

<p>where <script type="math/tex">t = \sum_{i=1}^n x_i^2</script> so by the Factorization Theorem, <script type="math/tex">T(\mathbf{X}) = \sum_{i=1}^n X_i^2</script> is sufficient for <script type="math/tex">\sigma^2</script>. Now,</p>

<script type="math/tex; mode=display">\frac{f(\mathbf{x} \vert \sigma^2)}{f(\mathbf{y} \vert \sigma^2)} = e^{\frac{-1}{2\sigma^2}(\sum x_i^2 - \sum y_i^2)} = e^{\frac{-1}{2\sigma^2}(T(\mathbf{x}) - T(\mathbf{y}))}</script>

<p>If <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script>, this ratio is 1 which is clearly free of <script type="math/tex">\sigma^2</script>. Now, if the ratio is free of <script type="math/tex">\sigma^2</script>, then it is necessary that <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script>. Thus, <script type="math/tex">T(\mathbf{x}) = \sum X_i^2</script> is a minimal sufficient statistic for <script type="math/tex">\sigma ^2</script>.</p>

<p><strong>3</strong>. Let <script type="math/tex">X_1, \dots, X_n \sim \textrm{Poisson}(\lambda)</script>.</p>

<p>Find a sufficient statistic for <script type="math/tex">\lambda</script> and show that it is a minimal sufficient statistic.</p>

<p><strong>Answer:</strong></p>

<script type="math/tex; mode=display">f(\mathbf{x} \vert \lambda) = \frac{1}{\prod_{i=1}^n x_i !}e^{-n\lambda}\lambda^{\sum x_i} = \frac{1}{\prod_{i=1}^n x_i !}e^{-n\lambda}\lambda^{t} = h(\mathbf{x})g(T(\mathbf{x}) \vert \lambda)</script>

<p>so by the Factorization Theorem (again‚Ä¶), <script type="math/tex">\sum_{i=1}^n X_i</script> is sufficient for <script type="math/tex">\lambda</script>. Now,</p>

<script type="math/tex; mode=display">\frac{f(\mathbf{x} \vert \lambda)}{f(\mathbf{y} \vert \lambda)} \propto_{\lambda} \lambda^{\sum x_i - \sum y_i} = \lambda^{T(\mathbf{x}) - T(\mathbf{y})}</script>

<p>so if <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script>, the ratio is free of <script type="math/tex">\lambda</script>. Furthermore, if the ratio is free of <script type="math/tex">\lambda</script>, then it is necessary that <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script> for the exponent to be 0 and thus have <script type="math/tex">\lambda</script> disappear. Therefore, <script type="math/tex">T(\mathbf{x}) = \sum X_i</script> is also a minimal sufficient statistic for <script type="math/tex">\lambda</script>.</p>

<p><strong>4</strong></p>

<p>Let <script type="math/tex">X_1, \dots, X_n \sim f_X(x \vert \theta)</script> where</p>

<script type="math/tex; mode=display">% <![CDATA[
f_X(x \vert \theta) = \frac{2x}{\theta^2}, \; \; 0 < x < \theta %]]></script>

<p>Find a minimal sufficent statistic for <script type="math/tex">\theta</script></p>

<p><strong>Answer:</strong></p>

<script type="math/tex; mode=display">\frac{f_X(\mathbf{x} \vert \theta)}{f_X(\mathbf{y} \vert \theta)} = \frac{\prod x_i I(x_{(n)} > \theta)}{\prod y_i I(y_{(n)} > \theta)} \propto_{\theta} \frac{I(x_{(n)} > \theta)}{I(y_{(n)} > \theta)}</script>

<p>Therfore, since this ratio doesn‚Äôt depend on <script type="math/tex">\theta</script> if and only if <script type="math/tex">x_{(n)} = y_{(n)}</script>, then <script type="math/tex">X_{(n)}</script> is minmial sufficient.</p>

<p><strong>5</strong></p>

<p>Suppose <script type="math/tex">X_1, \dots , X_n \sim f(x \vert \theta)</script> where</p>

<script type="math/tex; mode=display">f(x \vert \theta) = \theta x^{\theta - 1} \textrm{exp}(-x^{\theta})</script>

<p>with <script type="math/tex">\theta ,\; x \; > \; 0</script>. Show that <script type="math/tex">\frac{\log X_{(n)}}{\log X_{(1)}}</script> is ancillary. Well, let <script type="math/tex">Y = \theta \log(X) \implies X = e^{Y/\theta}</script>. Then,</p>

<script type="math/tex; mode=display">f_Y(y) = f_X(e^{Y/\theta})\vert \frac{dx}{dy} \vert = \theta \textrm{exp}(\frac{y}{\theta}(\theta - 1))\textrm{exp}(-\textrm{exp}(\frac{y}{\theta}(\theta)))</script>

:ET