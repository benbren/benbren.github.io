<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bayesian Estimation | Ben Brennan</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Bayesian Estimation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hi, I’m Ben. This is my website/blog. Views are my own. Thanks for looking." />
<meta property="og:description" content="Hi, I’m Ben. This is my website/blog. Views are my own. Thanks for looking." />
<link rel="canonical" href="http://localhost:4000/2018/07/13/bayesian-estimation.html" />
<meta property="og:url" content="http://localhost:4000/2018/07/13/bayesian-estimation.html" />
<meta property="og:site_name" content="Ben Brennan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-13T01:21:32-04:00" />
<script type="application/ld+json">
{"description":"Hi, I’m Ben. This is my website/blog. Views are my own. Thanks for looking.","datePublished":"2018-07-13T01:21:32-04:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/07/13/bayesian-estimation.html"},"url":"http://localhost:4000/2018/07/13/bayesian-estimation.html","headline":"Bayesian Estimation","dateModified":"2018-07-13T01:21:32-04:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ben Brennan" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ben Brennan</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/">Blog</a><a class="page-link" href="/cv/">CV</a><a class="page-link" href="/donate/">Donate</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bayesian Estimation</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-07-13T01:21:32-04:00" itemprop="datePublished">Jul 13, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>When most people think about statistics, they think about flipping a coin or gambling or having flashbacks to dropping out of high school because they hated the class so much… moral of the story is, it’s just one thing to think about. What if I told you that that wasn’t necessarily true? I am about to tell you that. Statistics is the science of modeling randomness. But what is random, and what is not? That is a fundamental question in statistics and splits the subject into two separate approaches - Bayesian and Frequentist.</p>

<p>What does it mean to say that I am 50% certain it will rain tomorrow? That is not so much a statement of probability, but rather of uncertainty. It will rain or it will not rain, but that outcome is something you are uncertain of. Consider another example: flip a coin and cover it up immediately once it lands, so you don’t know what the result is. What’re the chances that the coin is heads? From a traditional, frequentist perspective, there is no answer to this question. It already happened. There is already a result - you’re just ignorant. But that’s not really how we think, is it? We don’t know what the result is - we are uncertain of it. Let’s consider a medical example. Say you’re getting screened for the flu, and the result comes out positive. Do you actually have the flu? In the frequentist approach, that doesn’t make sense to ask. We do have sensitivity and specificity. Let <script type="math/tex">\theta = 1</script> if you have the flu and <script type="math/tex">\theta = 0</script> if you don’t. Then the sensitivity is the probability the test is positive given that you have the flue, or <script type="math/tex">P(T = 1 \vert \theta = 1)</script> and the specificity is <script type="math/tex">P(T = 0 \vert \theta = 0)</script>, the probability the test is negative when you don’t have the flu. If specificity is not 1, you might get tested positive but not have the disease. It makes sense then to ask ‘“Do I actually have the flu?”, but does it make statistical sense to ask ‘Do I have the flu?’? That is, what is <script type="math/tex">P(\theta =1 \vert T = 1)</script>? A frequentist says ‘Bad question. <script type="math/tex">\theta</script> is fixed’. A Bayesian will come in and say ‘Hold on, <script type="math/tex">\theta</script> is not actually fixed. We can estimate this’</p>

<p>This is the whole premise of Bayesian statistics - to use probability theory to not <em>only</em> quantify probability but also to quantify uncertainty. Much better!! Solving problems that were disregarded earlier - we like that!</p>

<p>Okay, moving on… now that I’ve converted you to the Bayesian approach to statistics…</p>

<p>:)</p>

<p>Let’s talk about the setup. In a frequentist approach, we have a random variable <script type="math/tex">\mathbf{X}</script>, observed data from that random variable, a model, and a parameter <script type="math/tex">\theta \in \Omega</script> that is unknown but <em>fixed</em>. In the Bayesian framework, we don’t consider that italicized part. It’s a subtle change that makes a huge difference. Now, we have</p>

<script type="math/tex; mode=display">\theta \sim \pi(\theta)</script>

<p>where <script type="math/tex">\pi</script> is called a <em>prior distribution</em>. Now, we need to make an adjustment to your original data’s distribution. Now, we have a conditional distribution!</p>

<script type="math/tex; mode=display">\mathbf{X} \vert \theta \sim f(\mathbf{x}\vert \theta)</script>

<p>and, by Bayes rule, the joint distribution of <script type="math/tex">\mathbf{X}</script> and <script type="math/tex">\theta</script> is</p>

<script type="math/tex; mode=display">f(\mathbf{x},\theta) = \pi(\theta)f(\mathbf{x}|\theta)</script>

<p>and, thus, the marginal distribution of  <script type="math/tex">\mathbf{X}</script> is</p>

<script type="math/tex; mode=display">m(\mathbf{x}) = \displaystyle\int_{\theta \in \Omega} {f(\mathbf{x},\theta)d\theta}</script>

<script type="math/tex; mode=display">\pi(\theta \vert \mathbf{x}) = \frac{\pi(\theta)f(\mathbf{x} \vert \theta)}{m(\mathbf{x}) }</script>

<p>We now have all the tools we need to do some basic Bayesian analysis!</p>

<p>and, so, by Bayes rule again, the posterior distribution of <script type="math/tex">\theta</script> is</p>

<p>Let us look at <script type="math/tex">X_1, \dots, X_n \sim Bernoulli(p)</script> where <script type="math/tex">p \sim Beta(\alpha, \beta)</script>. Then we could calculate <script type="math/tex">\pi(\theta \vert \mathbf{x}) = \frac{\pi(p)f(\mathbf{x}\vert \theta)}{m(\mathbf{x}) }</script>, but that would take forever. Does <script type="math/tex">m(\mathbf{x})</script> matter? No. Why? Think about it. We want the posterior distribtion of <script type="math/tex">p</script>, right? So <script type="math/tex">m(\mathbf{x})</script> has <em>literally</em> (millenials…) nothing to do with <script type="math/tex">p</script>, therefore it just acts as a normalizing constant in the equation to make sure that <script type="math/tex">\pi(p \vert \mathbf{x})</script> is a true pdf. Therefore, we can just look at the numerator of the expression and recognize the kernel of the distribution to understand what family the posterior distribution comes from. Soo, in math,</p>

<script type="math/tex; mode=display">\pi(p \vert \mathbf{x}) \propto \pi(p)f(\mathbf{x} \vert p)</script>

<p>We know</p>

<script type="math/tex; mode=display">f(\mathbf{x}\vert p) = \displaystyle \prod_{i=1}^{n} f(\mathbf{x_i}\vert p) = \displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i})</script>

<p>and</p>

<script type="math/tex; mode=display">\pi(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}</script>

<p>so</p>

<script type="math/tex; mode=display">\pi(p \vert \mathbf{x}) \propto\displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i}) \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}</script>

<script type="math/tex; mode=display">= p^{\sum_{i=1}^{n}x_i}(1-p)^{n - \sum_{i=1}^{n}x_i}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}</script>

<p>which is the same as</p>

<script type="math/tex; mode=display">\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha + \sum_{i=1}^{n}x_i -1} (1 - p)^{\beta +n - \sum_{i=1}^{n}x_i - 1}</script>

<p>Looking at the part of this expression that has to do with <script type="math/tex">p</script>, what do we see? It is the kernel of a Beta distribution! So, now we know that <script type="math/tex">\pi(p
\vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)</script>. Cool. Another thing to notice here: the prior and the posterior distribution are from the same family. This, informally, is what you call a <strong>conjugate family</strong>.  Anyways, now that we have the prior distribution, we can do some estimation.</p>

<p>If we remember, one of the frequentist approaches to estimation is Maximum Likelihood. Here, our parameter follows a distribution so it makes sense to just say ‘hey… what’s its expected value?’ . So that’s what people do! The Bayes estimator is literally just <script type="math/tex">E[p\vert \mathbf{x}]</script>. We know <script type="math/tex">\pi(p \vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)</script> . Since we know the expected value of <script type="math/tex">Beta(\alpha,\beta)</script> is <script type="math/tex">\frac{\alpha}{\alpha + \beta}</script>, it follows that</p>

<script type="math/tex; mode=display">E[p \vert \mathbf{x}] = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \sum_{i=1}^{n}x_i + \beta + n - \sum_{i=1}^{n}x_i} = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \beta + n}</script>

<script type="math/tex; mode=display">= \bar{X}(\frac{n}{\alpha + \beta + n}) + \frac{\alpha}{\alpha + \beta}(\frac{\alpha + \beta }{\alpha + \beta + n})</script>

<p>So the Bayes estimator is just a weighted average of the prior mean and the MLE! When n is large, the MLE is weighted much more and, when n is small, we trust the prior distribution’s estimate. This is a very cool result, in my opinion.</p>

<p>Let’s look at another example. Let <script type="math/tex">X_1, \dots , X_n \vert \lambda \sim Poisson(\lambda)</script> and let <script type="math/tex">\pi(\lambda) \sim Gamma(\alpha, \beta)</script>. Then</p>

<script type="math/tex; mode=display">\pi(\lambda \vert \mathbf{x}) \propto f(\mathbf{x}\vert \lambda )\pi(\lambda)</script>

<script type="math/tex; mode=display">= \frac{\lambda^{\sum_{i=1}^{n}x_i} e^{-n\lambda}}{\prod_{i=1}^{n}x_i !} \frac{1}{\Gamma(\alpha)\beta^{\alpha}} \lambda^{\alpha-1}e^{\frac{-\lambda}{\beta}}</script>

<script type="math/tex; mode=display">= \frac{1}{\prod_{i=1}^{n}x_i! \, \Gamma(\alpha)\beta^{\alpha}} \lambda^{\sum_{i=1}^{n}x_i +\alpha -1} e^{-\lambda(n + \frac{1}{\beta})}</script>

<p>and we should recognize the kernel (part with <script type="math/tex">\lambda</script>) of this distribution and conclude that</p>

<script type="math/tex; mode=display">\pi(\lambda\vert \mathbf{x}) \sim Gamma(\sum_{i=1}^{n}x_i + \alpha , (n + \frac{1}{\beta})^{-1})</script>

<p>Another conjugate family! The Bayes estimator here is just</p>

<script type="math/tex; mode=display">E[ \lambda | \mathbf{x}] = \frac{\sum_{i=1}^{n}x_i + \alpha }{n + \frac{1}{\beta}}</script>

<script type="math/tex; mode=display">= \bar{X} (\frac{n}{n + \frac{1}{\beta}}) + \alpha \beta (\frac{1}{n\beta + 1})</script>

<p>In summary, its quite simple to find the posterior distribution in these cases - but this is obviously not true in general. Sometimes the posterior takes an unknown form, and the best we can do is draw samples from this posterior using different methods (Gibbs Sampling, Importance Sampling). These examples above are cases where the posterior follows almost immediately from the joint distribution (conjugate family in both cases, actually) and makes our lives very easy. There is much more to go into about Bayesian Estimation, and I will do so here in the future. For now, you’ve got a good start on it!</p>

  </div><a class="u-url" href="/2018/07/13/bayesian-estimation.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Ben Brennan</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ben Brennan</li><li><a class="u-email" href="mailto:brennben@umich.edu">brennben@umich.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/benbren"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">benbren</span></a></li><li><a href="https://instagram.com/benjuum"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg> <span class="username">benjuum</span></a></li><li><a href="https://www.linkedin.com/in/ben-brenn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">ben-brenn</span></a></li><li><a href="https://www.twitter.com/benbrenn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">benbrenn</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Hi, I&#39;m Ben. This is my website/blog. Views are my own. Thanks for looking.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
