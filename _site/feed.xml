<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-24T21:48:57-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ben Brennan</title><subtitle>Hi, I&apos;m Ben. This is my website/blog. Views are my own. Thanks for looking.</subtitle><entry><title type="html">The Horseshoe Prior</title><link href="http://localhost:4000/2022/10/11/horseshoe.markdown.html" rel="alternate" type="text/html" title="The Horseshoe Prior" /><published>2022-10-11T23:00:00-04:00</published><updated>2022-10-11T23:00:00-04:00</updated><id>http://localhost:4000/2022/10/11/horseshoe.markdown</id><content type="html" xml:base="http://localhost:4000/2022/10/11/horseshoe.markdown.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Turns out there is a lot of data in the world and, turns out, a lot of that data is not really applicable to your problem! You might just say “hey, let’s throw the kitchen sink at this problem and see what comes out” (for instance, neural networks..), but sometimes you want to know what’s actually on the inside (not taking sides, just saying!).  Essentially, sometimes you want to know “what is causing y?” versus “what is y?”. Enter - shrinkage priors.&lt;/p&gt;

&lt;p&gt;In this model, there is two types of parameters that we are interested in: signal and noise. We want to knwo what parameters are having an impact on our outcome and what parameters are not. The gold standard is what is colloquially known as the spike-and-slab prior that places the prior&lt;/p&gt;

\[\beta_i \sim \pi \delta_0  + (1 - \pi)f(\beta_i ; \theta)\]

&lt;p&gt;on the parameters. With probability \(\pi\), the prior is a point-mass centered at zero and with probabilty \(1 - \pi\), the prior is a continuous slab distribution (usually centered at zero e.g. (\(\f = N(0,\theta)\))).&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Intro to Inference</title><link href="http://localhost:4000/2020/01/10/intro-to-inference.html" rel="alternate" type="text/html" title="Intro to Inference" /><published>2020-01-10T12:00:00-05:00</published><updated>2020-01-10T12:00:00-05:00</updated><id>http://localhost:4000/2020/01/10/intro-to-inference</id><content type="html" xml:base="http://localhost:4000/2020/01/10/intro-to-inference.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Lots of people go through life talking about statistics and not actually knowing what a statistic &lt;em&gt;is&lt;/em&gt;, sadly enough. The point of statistics, and statistical inference in general, is to make inferences about a &lt;em&gt;population&lt;/em&gt; based off a sample. In general, you have a population parameter (\(\theta\)) that needs to be estimated (could be high - dimensional - the most we will talk about is &lt;em&gt;max&lt;/em&gt; 2, so no worries) and some data (a random sample, once could say…) \(X_ 1, \dots, X_ n\) from that population, from which we observe the data \(x_ 1, \dots, x_ n\). The goal of statistical inference is to take that sample and make really good educated guesses about the population and also make good, educated guesses about how good and educated your guess is. Make sense? I didn’t think so.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; is the process of drawing information about a population based off a sample (like I said above, with a fancy word defining it). The points is - probability theory is based on knowing \(\theta\). We never actually know \(\theta\), so to do anything in practice we need inference about \(\theta\). Big time important.&lt;/p&gt;

&lt;p&gt;For example, we can make conclusions about the probability we get a certain number of successes in a certain number of trials (i.e Binomial(\(n,p\)), or the probability that a certain metric is less than some value (i.e \(\mathcal{N}(\mu, \sigma^2\))) - but that requires us to know the distribution those values follow in the population (i.e to know the population parameters).&lt;/p&gt;

&lt;p&gt;First - and back to the beginning - what is a statistic?! (FYI - this is less of a post, more of a literal book chapter.. fair warning!)&lt;/p&gt;

&lt;p&gt;A statistics takes a random variable \(\mathbf{X}\) and maps it via some function \(T(.)\). That is,&lt;/p&gt;

\[T(x_1, \dots, x_n) = T(\mathbf{X}): \mathbb{R}^n \rightarrow \mathbb{R}^m\]

&lt;p&gt;A few things. \(T(\mathbf{X})\) is also a random variable, obviously. There is no restriction on \(m\), but I think you probably get the idea that \(m &amp;lt; n\), because it doesn’t really make sense to make our data more complicated. We also don’t want to lose information in this mapping - this is a key point. For instance, if \(T(\mathbf{X}) = X_1\), we have lost so much information contained in our data by using this transformation.&lt;/p&gt;

&lt;p&gt;So, in summary, we want to take our data and use some transformation \(T(\mathbf{X})\) to make that data simpler and no less informative. That is what makes a good statistic good. This is why we use some statistics, and not others - as we will see. Anyways, this seems easy. Not like there’s a million choices for \(T\) or anything…&lt;/p&gt;

&lt;h2 id=&quot;sufficient-statistic&quot;&gt;Sufficient Statistic&lt;/h2&gt;

&lt;h4 id=&quot;definition&quot;&gt;Definition&lt;/h4&gt;

&lt;p&gt;Anyways. Let’s define what a sufficient statistic is. A statistic \(T(\mathbf{X})\) is called a &lt;em&gt;sufficient statistic&lt;/em&gt; for the parameter \(\theta\) if the distribution of \(\mathbf{X}\) given \(T(\mathbf{X})\) does not depend on \(\theta\). Why? This means that, given that we know the value of \(T(\mathbf{X})\), the information left in the sample does not contain any information about \(\theta\). This should be obvious - if the distribution of the data does not depend on the parameter, how can you get information about the parameter from that distribution ya know? Formally, if&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x})\]

&lt;p&gt;then \(T(\mathbf{X})\) is sufficient for \(\theta\).&lt;/p&gt;

&lt;p&gt;Let’s look at an example: assume \(X_1, \dots, X_n \sim \textrm{Bernoulli}(p)\) and \(T(\mathbf{X}) = \sum_{i=1}^n X_i\). Is \(T\) sufficient for \(p\)? Yep. Next.&lt;/p&gt;

&lt;p&gt;Just kidding. Let’s see why.&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t) = \frac{P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t)}{P(T(\mathbf{X}) = t))}\]

\[= \begin{cases} 
	\frac{P(\mathbf{X} = \mathbf{x})}{P(T(\mathbf{X}) = t))} &amp;amp; \textrm{if} \; T(\mathbf{X}) = t \\
	0 &amp;amp; \textrm{else} 
 	\end{cases}\]

&lt;p&gt;(only considering the top case)&lt;/p&gt;

\[= \frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{q_{T(\mathbf{X})}(t \vert p)}\]

&lt;p&gt;Now, notice the distribution of \(T(\mathbf{X}) \sim \textrm{Binomial}(n,p)\). Therefore,&lt;/p&gt;

\[\frac{f_{\mathbf{X}}(\mathbf{x})}{f_{T(\mathbf{X})}(t)} = \frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{t}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{t}}\]

&lt;p&gt;which doesn’t contain \(p\). So now - yep, \(T(\mathbf{X})\) is sufficient for \(p\)!&lt;/p&gt;

&lt;h4 id=&quot;theorem-1&quot;&gt;Theorem 1&lt;/h4&gt;

&lt;p&gt;Now, in typical fashion, lets define like a million more ways to show something is sufficient. The first - shown through example. From above we showed that the original definiton could  boil down to&lt;/p&gt;

\[\frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q_{T(\mathbf{X})}(t \vert \theta)} = \frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q(T(\mathbf{x}) \vert \theta)}\]

&lt;p&gt;so, if this ratio is free of \(\theta\) for all \(\mathbf{x}\) in the support of \(\mathbf{X}\), then \((T(\mathbf{X})\) is sufficient for \(\theta\).&lt;/p&gt;

&lt;p&gt;So, the example above boils down to&lt;/p&gt;

\[\frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{T(\mathbf{x})}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{T(\mathbf{x})}}\]

&lt;p&gt;which does not depend on \(\theta\) so we come to the same conclusion. This seems redundant. It is, really, but the benefit is we don’t have to consider \(T(\mathbf{X}) = t\) and \(T(\mathbf{X}) \neq t\). So, originally, we &lt;em&gt;techincally&lt;/em&gt; had to say that 0 did not depend on \(p\), which is obvious. In this second definition, we don’t even need to consider that.&lt;/p&gt;

&lt;p&gt;Let’s try another example using this second definition of sufficiency. Let’s check whether \(T(\mathbf{X}) = \bar{X}\) is sufficient for \(\mu\) in the normal distribution (with known variance \(\sigma^2\)).&lt;/p&gt;

\[f_X(x \vert \mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x - \mu)^2}{2\sigma^2}\right)\]

&lt;p&gt;Recall that \(\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})\). Therefore,&lt;/p&gt;

\[\frac{f_{\mathbf{X}}(\mathbf{x} \vert \mu)}{q(T(\mathbf{X}) \vert \mu)} = \frac{(\frac{1}{\sqrt{2\pi\sigma^2}})^n\textrm{exp}\left(- \displaystyle\sum_{i=1}^n\frac{(x_i - \mu)^2}{2\sigma^2}\right)}{\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-n(\bar{x} - \mu)^2}{2\sigma^2}\right)} = \mathcal{K} \frac{\textrm{exp}\left(\frac{- \sum_{i=1}^n(x_i^2 - 2x_i\mu)}{2\sigma^2}\right)}{\textrm{exp}\left( \frac{2\bar{x}n \mu - n\bar{x}^2}{2 \sigma^2}\right)}\]

\[= \mathcal{K}\frac{\textrm{exp}\left( \frac{-\sum_{i=1}^{n}x_i^2}{2 \sigma^2}\right)}{\textrm{exp}\left(\frac{-n\bar{x}^2}{2 \sigma^2}\right)}\]

&lt;p&gt;which doesn’t depend on \(\mu\)! So, the sample mean is a sufficient statistic for the population average in a normal distribution - hence why we use it all the time!&lt;/p&gt;

&lt;p&gt;Now, moving on to even more ways to show sufficiency…&lt;/p&gt;

&lt;h4 id=&quot;factorization-theorem&quot;&gt;Factorization Theorem&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Factorization Theorem&lt;/strong&gt; : \(T(\mathbf{X})\) is sufficient if &lt;em&gt;and only if&lt;/em&gt; there exists \(g(t\vert \theta)\) and \(h(\theta)\) such that \(\forall \mathbf{x}, \theta\),&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert \theta) = g(T(\mathbf{x})\vert \theta)h(\mathbf{x})\]

&lt;p&gt;Since this is an &lt;em&gt;iff&lt;/em&gt;, we have to prove it in both directions. First,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Sufficiency \(\implies\) Factorization&lt;/p&gt;

&lt;p&gt;This is the easy case.&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x}\vert \theta) = P(\mathbf{X} = \mathbf{x} \vert \theta)\]

\[= P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t \vert \theta)\]

&lt;p&gt;and, by the definition of conditional probability,&lt;/p&gt;

\[= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t,\theta)\]

&lt;p&gt;and, by sufficiency (i.e. we assume given \(T(\mathbf{X})\), the distribution of the data is free of \(\theta\)).&lt;/p&gt;

\[= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t)\]

\[= g(t \vert \theta)h(\mathbf{x})\]

&lt;p&gt;and we have proved that direction. Now,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Factorization \(\implies\) Sufficiency&lt;/p&gt;

&lt;p&gt;Let \(q(y \vert \theta)\) be the pmf of \(T(\mathbf{X})\) and let \(A_t\) be the set containing all the possible data that yield a statistics \(T(\mathbf{x}) = t\). That is,&lt;/p&gt;

\[A_t = \{ \mathbf{y}: T(\mathbf{y}) = t\}\]

&lt;p&gt;Then,&lt;/p&gt;

\[q(t \vert \theta) = P(T(\mathbf{X}) = t \vert \theta) = \displaystyle\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)\]

&lt;p&gt;So, since we are assuming factorization,&lt;/p&gt;

\[\frac{f_{\mathbf{X}}(\mathbf{x}\vert\theta)}{q(t \vert \theta)} = \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)}\]

\[= \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} g(T(\mathbf{y}) \vert \theta) h(\mathbf{y})}\]

&lt;p&gt;and, since for \(y \in A_t\), \(T(\mathbf{y}) = t = T(\mathbf{x})\)&lt;/p&gt;

\[= \frac{h(\mathbf{x})}{\sum_{y \in A_T}h(\mathbf{y})}\]

&lt;p&gt;and this does not ever depend on \(\theta\), so \(T(\mathbf{X})\) is sufficient and we have completed the proof!&lt;/p&gt;

&lt;p&gt;So, this makes things wicked easy…. let’s go back to the two examples we have done above. First, \(\mathbf{X} \sim \textrm{Bernoulli}(p)\).&lt;/p&gt;

\[f(\mathbf{x} \vert p) = \displaystyle\prod p^{x_i}(1-p)^{1- x_i} = p^{\sum x_i}(1-p)^{n - \sum x_i} = p^t(1-p)^{n-t}\]

&lt;p&gt;and then \(g(t \vert p) = p^t(1-p)^{n-p}\) and \(h(\mathbf{y}) = 1\), then we can see that \(\sum_{i= 1}^n X_i\) is sufficient for \(p\).&lt;/p&gt;

&lt;p&gt;Now, let us consider the second, normal with known variance, example.&lt;/p&gt;

\[f_X(\mathbf{x} \vert \mu) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)\]

\[= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right) \times \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)\]

&lt;p&gt;so \(h(\mathbf{x}) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right)\) and \(g(t \vert \mu) = \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)\) when \(t = \bar{x}\), so the sample mean is sufficient for \(\mu\). Way easier - we like that.&lt;/p&gt;

&lt;p&gt;Let’s try a new example. Consider \(\mathbf{X} \sim \textrm{Uniform}(0,\theta)\). What is a sufficient statistic for \(\theta\)? To assess this, let’s use the factorization theorem. Remember,&lt;/p&gt;

\[f(x \vert \theta) = \begin{cases} 
\frac{1}{\theta} &amp;amp; \textrm{if} \; x \in [0,\theta] \\
0 &amp;amp; \textrm{else}
\end{cases}\]

&lt;p&gt;which is identical to&lt;/p&gt;

\[\frac{1}{\theta}I(0 \leq x) I (x \leq \theta)\]

&lt;p&gt;so, therefore,&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert \theta) = \displaystyle\prod_{i=1}^n \frac{1}{\theta}I(0 \leq x_i) I (x_i \leq \theta) = \theta^{-n}I(x_{(n)} \leq \theta) I(0 \leq x_{(1)})\]

&lt;p&gt;so we can conclude that the max, \(X_{(n)}\), is a sufficient statistic for \(\theta\).&lt;/p&gt;

&lt;p&gt;Now, let’s consider a two-dimensional (&lt;em&gt;whoa&lt;/em&gt;) sufficient statistic! When I learned this, the professor made this seem very straight forward - but I know things get real iffy real quick when the dimension of things increase - so lets take it slow. What we are looking for now is \(T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))\) such that the factorization theorem still applies. Let’s consider a normal distribution with unknown variance. Now, what we need is to find a factorization&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = g(T_1(\mathbf{x}), T_2(\mathbf{X}) \vert \mu, \sigma^2)h(\mathbf{x})\]

&lt;p&gt;This differs from the one dimensional case where we kind of just tossed the parts of the exponent that did not depend on \(\mu\) into the constant and called it a day. Now, we can do that but only for the parts of the exponent that don’t depend on \(\mu\) and \(\sigma^2\) which is none of them so trick-statement you can’t toss anything into the constant from the exponent. Turns out that constant infront isn’t even a constant anymore since it involves \(\sigma^2\). Bummer. All this means, though, is more algebra - which, although it is tedious, is not difficult if you pay attention (which &lt;em&gt;I know&lt;/em&gt; is difficult). So, let’s get to it.&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)\]

\[= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-\sum_{i=1}^n((x_i - \bar{x}) + (\bar{x} - \mu))^2}{2\sigma^2}\right)\]

\[= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 +2 \sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu)  - n(\bar{x} - \mu)^2\right)\right)\]

&lt;p&gt;and now note that 
\(\sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu) = \bar{x}\sum x_i - \mu\sum x_i - n\bar{x} + n\bar{x}\mu = 0\)&lt;/p&gt;

&lt;p&gt;so, therefore,&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 - n(\bar{x} - \mu)^2\right)\right)\]

&lt;p&gt;and by using \(h(\mathbf{x}) = 1\), we have the factorization theorem! Therefore,&lt;/p&gt;

\[T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (\bar{X}, \sum_{i=1}^n (X_i - \bar{X})^2)\]

&lt;p&gt;is a sufficient statistic for \(\mathbf{\phi} = (\mu, \sigma^2)\)&lt;/p&gt;

&lt;p&gt;Let’s try another one - \(\mathbf{X} \sim \textrm{Uniform}(\alpha, \theta)\). Extending our knowledge of the one-dimensional case, this should be fairly simple.&lt;/p&gt;

\[f_{\mathbf{X}}(\mathbf{x} \vert \theta, \alpha) = \displaystyle\prod_{i=1}^n \frac{1}{\theta - \alpha}I(\alpha \leq x_i) I (x_i \leq \theta) = (\theta - \alpha)^{-n}I(x_{(n)} \leq \theta) I(\alpha \leq x_{(1)})\]

&lt;p&gt;so we can conclude that&lt;/p&gt;

\[T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (X_{(1)}, X_{(n)})\]

&lt;p&gt;is suffcient for \(\phi = (\alpha,\theta)\)&lt;/p&gt;

&lt;h2 id=&quot;minimal-sufficient-statistic&quot;&gt;Minimal Sufficient Statistic&lt;/h2&gt;

&lt;p&gt;As you have (hopefully) noticed, sufficient statistics are not even close to unique. For instance, the sample itself is always sufficient. The order statistics are also always sufficient as we can set \(h(\mathbf{x})\) to be 1 and then the joint distribution is the same, just reordered (see the examples section for more detials on this). Also, any one-to-one function of a sufficient statistic is also a sufficient statistic, as that value maps uniquely back to the original sufficient statistic. Therefore, we need some notion of &lt;em&gt;better&lt;/em&gt; when it comes to these types of statistics. That is what the idea of a &lt;strong&gt;minimal sufficient statistic&lt;/strong&gt; is.  The idea is to find a sufficient statistic that is more sufficent than the rest - i.e. one that has the property of maximum data reduction? The defintion of the minimal sufficient statistic is as follows:&lt;/p&gt;

&lt;p&gt;A sufficient statistic \(T(\mathbf{X})\) is a minimal sufficient statistic if, for every other suffcient statistic \(T&apos;(\mathbf{X})\), \(T(\mathbf{X})\) is a function of \(T&apos;(\mathbf{X})\).&lt;/p&gt;

&lt;p&gt;This definition is fairly unclear, in my humble opinion. I guess not unclear, but fails to provide the intuition behind it. So lets try to explain it a bit better. The idea behind sufficient statistics was to reduce to data into a &lt;strong&gt;coarser&lt;/strong&gt; partition, without losing information about the parameter of interest. We now want the coarsest partition of data we can get - the maximum data reduction. So lets consider a function, \(f(x)\). Remember, a function is defined such that, if \(x_1 = x_2\), \(f(x_1) = f(x_2)\) - but this does not mean that if \(f(x_1) = f(x_2)\), then \(x_1 = x_2\). What does this mean in our case? It means, if we have a partition, we can make it more coarse (i.e the domain smaller) by finding a function of that partition that maps to a smaller partition. That is, if \(x_1\) and \(x_2\) are partitions of a sample space \(\mathcal{X}\), then a function can &lt;strong&gt;only&lt;/strong&gt; map to a coarser partition. That is, a function will map both to the same partition if they are the same, but may also map them to the same partition if they are different. Also, a function will hit all values in its outcome space. So, at worst, a function will map all partitions in the first partition space to all partions in the other sample space and, at best, will map a few of the original partitions to the same partition. Therefore, a function makes the sample space more coarse. So, if \(T\) is a function of all other \(T&apos;\), then it is a partition is the most coarse - it has the maximum reduction of data. This is a nice definition but, in practice, it does not really help us. The following is a theorem that &lt;em&gt;can&lt;/em&gt; help:&lt;/p&gt;

&lt;p&gt;The ratio \(\frac{f(\mathbf{x} \vert \theta)}{f(\mathbf{y} \vert \theta)}\) is free of \(\theta\) &lt;em&gt;if and only if&lt;/em&gt; \(T(\mathbf{x}) = T(\mathbf{y})\), then then \(T(\mathbf{X})\) is a minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;This is helpful! We can use this practically. The proof is a bit difficult, but you can see this definition as arising from the Factorization Theorem. Before we see some examples with some known distributions, lets talk facts about this minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;Is a minimal sufficient statistic unique? &lt;strong&gt;No&lt;/strong&gt;. This is the first fact. It turns out that any injective function of \(T(\mathbf{X})\) is also a minimal sufficient statistic. Why does this make sense? Intuitively, a one to one function of \(T(\mathbf{X})\) maps the partitions created by \(T(\mathbf{X})\) uniquely to partitions in a different partitioned space. Since \(T(\mathbf{X})\) is a minimal sufficient statistic, it achieves the coarsest possible partition space (i.e with the smallest cardinality), so the mapping of a function of \(T(\mathbf{X})\) has to map to the smallest number of possible partitions, by default.&lt;/p&gt;

&lt;p&gt;A more formal proof is as follows. To understand why this proof is as follows, remember that &lt;em&gt;only&lt;/em&gt; a 1-1 function has an inverse:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Let \(T^*(\mathbf{X}) = b(T(\mathbf{X}))\) be a one-to-one function. Then, \(\exists \; b^{-1}\) such that \(b^{-1}(T^*(\mathbf{X})) = T(\mathbf{X})\). Thus, knowing \(T^*(\mathbf{X})\) assures that we know \(T(\mathbf{X})\) via \(b^{-1}(.)\). We can also see that, by the factorization theorem, when \(T\) is sufficient, \(\exists \; h, \; g\) such that \(f(\mathbf{x} \vert \theta) = g(T(\mathbf{x}) \vert \theta)h(\mathbf{x}) = g(b^{-1}(T^*(\mathbf{X}))\vert \theta)h(\mathbf{x})\). Thus, \(T^*(\mathbf{X})\) is sufficient for \(\theta\). This shows that \(T^*(\mathbf{X})\) is a sufficient by the Factorization Theorem. Now, assume there is another sufficient statistic \(T_1(\mathbf{X})\). Since \(T\) (the original one) is minimal sufficient, then \(T = q(T_1(\mathbf{X}))\). Therefore, \(T^* = b(T) = b(q(T_1))\). Therefore, \(T^*(\mathbf{X})\) is a function of any other sufficient statistic, and is thus a minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;This fact actually leads us to the next fact - there is &lt;em&gt;always&lt;/em&gt; a one-to-one function between two minimally sufficient statistics. To show this, suppose there are two minimally sufficient statistics \(\mathbf{T_1}, 
\mathbf{T_2}\). Then, \(\mathbf{T_1} = f(\mathbf{T_2})\) and \(\mathbf{T_2} = g(\mathbf{T_1})\). Thus, \(\mathbf{T_1} =f(g(\mathbf{T_1}))\) which implies that \(f= g^{-1}\) and therefore \(f\) is one-to-one. The same can be shown for \(g\). What this means practically is that the partition created by a minimal sufficient statistic is unique.&lt;/p&gt;

&lt;p&gt;And… now some totally non-boring to examples, so you can pass your class. ;).&lt;/p&gt;

&lt;p&gt;Lets go back to the OG example of this post. Let \(X_1, \dots, X_n \sim \textrm{Binomial}(n,p)\). Is \(\sum X_i\) a minimal sufficient statistic? We know it is sufficient because we already proved it so refresh your memory if you already forgot about this. To prove minimal sufficiency, we have to prove it both ways. So first, lets look at the ratio&lt;/p&gt;

\[\frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{f_{\mathbf{X}}(\mathbf{y} \vert p)} = \frac{\binom{n}{\sum x_i}p^{\sum x_i}(1-p)^{n - \sum x_i}}{\binom{n}{\sum y_i}p^{\sum y_i}(1-p)^{n - \sum y_i}} \propto_{p} p^{\sum x_i - \sum y_i}(1-p)^{\sum y_i - \sum x_i}\]

&lt;p&gt;Now, if \(\sum y_i = \sum x_i\), then the ratio is 1 and thus free of \(p\). If the ratio is free of \(p\) only if \(\sum y_i = \sum x_i\), so then \(\displaystyle\sum_{i=1}^n X_i\) is a minimal sufficient statistic for \(p\). Great - now lets look at an example where something might not be a minimal sufficient statistic in this same data. Consider the case where \(n=3\) and the associated two-dimensional statistic - \(T(\mathbf{X}) = (X_1 + X_2, X_3)\). Is this sufficient? Yes, it is.&lt;/p&gt;

\[f_{\mathbf{X}}( \mathbf{x} \vert p) = \binom{n}{x_1 + x_2 + x_3}p^{x_1 + x_2 + x_3}(1-p)^{3 - x_1 -x_2 - x_3} \propto_p p^{x_1 + x_2}(1-p)^{2 - x_1 - x_2}p^{x_3}(1-p)^{1-x_3}\]

&lt;p&gt;which, by the Factorization Theorem, is sufficient. Good? But now, is is minimal sufficient? Intuitively, we should say no. Why? You tell me. Okay - I tell you. Consider the minimal sufficient statistic (derived above) \(X_1 + X_2 + X_3\). This statistic takes the partitions (values) of 0,1,2, and 3. Our current statistic has, as an example, partitions (0,1) and (1,0), which are different. However, they both correspond to the &lt;em&gt;coarser&lt;/em&gt; partition generated by \(X_1 + X_2 + X_3\). So, there is always a partition of smaller cardinality. Let’s show this mathematically.&lt;/p&gt;

\[\frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{f_{\mathbf{X}}( \mathbf{y} \vert p)} \propto \frac{p^{x_1 + x_2}(1-p)^{2 - x_1 - x_2}p^{x_3}(1-p)^{1-x_3}}{p^{y_1 + y_2}(1-p)^{2 - y_1 - y_2}p^{y_3}(1-p)^{1-y_3}}\]

&lt;p&gt;So, this is free of \(p\) if \(T(\mathbf{x}) = T(\mathbf{y})\). But, is it free of \(p\) only if that is the case? No! If \(y_3  = x_1 + x_2\) and \(x_3 = y_1 + y_2\), then it is free of \(p\) but the statistics are not equal! This is exactly what our intuition above says. If we switch the order of the ordered pair, we map to the same partition in the partition space of another statistic (namely the one defined above). So we can always define a statistic with a coarser partition (a more basic statistic that contains the same information about \(p\)).&lt;/p&gt;

&lt;h3 id=&quot;ancillary-statistics&quot;&gt;Ancillary Statistics&lt;/h3&gt;

&lt;p&gt;Now - we will talk about ancillary statistics. This mean seem like it is out of left field, and it is, but it is still a pretty basic part of inference (and a type of statistic) so we are going to cover it.&lt;/p&gt;

&lt;p&gt;First, lets talk about the word ancillary. The first time I heard this word, actually, was from a professor I had who was from India and had learned English with a British accent, so I now say ancillary ancíllary, not ancilláry. Anyways - to the point.&lt;/p&gt;

&lt;p&gt;A statistic is ancillary if its distribution does not depend on the parameter \(\theta\).&lt;/p&gt;

&lt;p&gt;How is this different from a sufficient statistic? At first glance, the definitions seem kind’ve similiar. They aren’t at all. A sufficient statistic is a statistic that contains all the relevant information about \(\theta\) in the data. An ancillary statistic, by defintion, do not contain any information about the parameter at all. I know what you are thinking - so, if they contain no information about the parameter, why in the &lt;em&gt;world&lt;/em&gt; should I care about these things? We will get back to that point.&lt;/p&gt;

&lt;p&gt;First, some examples. Let \(X_1, \dots,X_n \sim \mathcal{N}(\mu,\sigma^2)\) where \(\sigma^2\) is known. Consider \(T_1 = X_1 - \mu\). This is not, because the statistic is a function of the parameter \(\mu\). What about \(T_2 = X_1 - X_2\)? Yes. Since both are Gaussian, the distribution of \(T_2\) is Gaussian with expected value 0 and variance \(2\sigma^2\), by convolusion. Therefore, the distribution of \(T_2\) does not depend on \(\mu\) and thus is ancillary. Also, remember \(\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2\). So, the sample standard deviation is also an ancillary statistic for \(\mu\)! These are some realtively simple examples, and ones that maybe we should have known off the top of our heads (just an assumption, since you are reading this). How about we look at something a bit more difficult?&lt;/p&gt;

&lt;p&gt;What if we have \(X_1, \dots, X_n \sim \mathcal{N}(0,\sigma^2)\) and we define Y_i to be \(X_i \sigma\). Then, while is not ancillary because it is a function of \(\sigma^2\), it does follow a standard normal distribution now. Therefore, we can show that \(\frac{X_1}{X_2} = \frac{\sigma Y_1}{\sigma Y_2} = \frac{Y_1}{Y_2}\) which, from ratio distributions, is distributed Cauchy. So, by using that transform, we have found a way to use the original data to create an ancillary statistic. Notice, actually, that any function of \(\mathbf{Y}\) does not depend on \(\sigma^2\) at all, so constructing ancillary statistics is not hard at all, in this case.  Now let’s look at a much more difficult problem.&lt;/p&gt;

&lt;p&gt;Let \(X_1, \dots , X_n \sim \textrm{Uniform}(\theta, \theta +1)\). Show that the range, \(R = X_{(n)} - X_{(1)}\) is an ancillary statistic. There are two ways to do this - we can go through both. The first is to find the distribution of \(R\) and show that it does not depend on \(\theta\) - not trivial. To start, we need the joint distribution of the min and the max. This is given in Casella &amp;amp; Berger, Thm. 5.4.6. For any pair of order statistics, the joint distribution is given by&lt;/p&gt;

\[f_{X_{(i)},X_{(j)}}{y_1,y_2 \vert \theta} = \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_{X}(y_1)f_{X}(y_2) \left[ F(y_1) \right]^{i-1} \left[ F(y_2) - F(y_1)\right]^{j-i-1}[1-F(y_2)]^{n-j}\]

&lt;p&gt;Therefore, the joint distrbution of the min and the max is&lt;/p&gt;

\[\frac{n!}{(n-2)!}f_X(x_{(1)})f_X(x_{(n)})[F(x_{(n)}) - F(x_{(1)})]^{n-2}\]

&lt;p&gt;Now, we use our knowledge of the uniform distribution! Note that&lt;/p&gt;

\[f_X(x \vert \theta) = I(\theta &amp;lt; x \ \theta + 1)\]

&lt;p&gt;and that&lt;/p&gt;

\[F_X(x \vert \theta ) = 
\begin{cases}
   0 &amp;amp; \;\;\textrm{if} \;\; x \leq \theta \\
   x - \theta &amp;amp; \;\;\textrm{if} \;\; \theta &amp;lt; x &amp;lt; \theta +1 \\
   1 &amp;amp; \;\;\textrm{if} \;\; x \geq \theta + 1 \\
   
\end{cases}\]

&lt;p&gt;Therefore, the join distribution of \(X_{(1)}, X_{(n)}\) is given by&lt;/p&gt;

\[\begin{cases}
n(n-1)(x_{(n)} - x_{(1)})^{n-2} &amp;amp; \;\; \textrm{if} \;\; \theta &amp;lt; x_{(1)} &amp;lt; x_{(n)} &amp;lt; \theta + 1 \\
0 &amp;amp; \;\; \textrm{else}
\end{cases}\]

&lt;p&gt;Now, we need to find the distribution of the range! To do this, let \(M = \frac{X_{(1)} + X_{(n)}}{2}\) be the median. Then, \(X_{(1)} = \frac{2M - R}{2}\) and \(X_{(n)} = \frac{2M+R}{2}\). The Jacobian is then&lt;/p&gt;

\[\mathbf{J} = 
\left[\begin{matrix}
\frac{\partial X_{(1)}}{\partial R} &amp;amp; \frac{\partial X_{(1)}}{\partial M} \\ 
\frac{\partial X_{(n)}}{\partial R} &amp;amp; \frac{\partial X_{(n)}}{\partial M}
\end{matrix}\right] = 
\left[\begin{matrix}
-\frac{1}{2} &amp;amp; 1 \\ 
\frac{1}{2} &amp;amp; 1
\end{matrix}\right]\]

&lt;p&gt;and the determinant here is 1. Therfore me can just plug and chug, no need to add in \(\vert \mathbf{J} \vert\) to our new pdf. We have the same domain restrictions, just now \(\theta &amp;lt; \frac{2m - r}{2} &amp;lt; \frac{2m + r}{2} &amp;lt; \theta + 1\). If we fix \(r\), we can see that \(\theta + r/2 &amp;lt; m &amp;lt; \theta + 1 - r/2\) and we can see that the domain of \(r\) is \((0,1)\). This is all the information we now need to know to derive the distribution of \(R\). It follows from this that&lt;/p&gt;

\[f_R(r) = \displaystyle\int_{\theta + r/2}^{\theta + 1 - r/2} n(n-1)r^{n-2}dm = n(n-1)r^{n-2}(1-r)\]

&lt;p&gt;and this doesn’t depend on \(\theta\)! So the range is ancillary. Another thing to notice here - this totally sucked to derive. I know it. You know it. We &lt;em&gt;all&lt;/em&gt; know it. Lets do it a way easier way! Define \(Y =  X_i - \theta\). Then,&lt;/p&gt;

\[f_Y(y) = I(\theta &amp;lt; y + \theta &amp;lt; \theta + 1) \vert \frac{dx}{dy}\vert = I(0 &amp;lt; y &amp;lt; 1)\]

&lt;p&gt;which is Uniform(0,1). Wow, so now \(R = X_{(n)} - X_{(1)} = (Y_{(n)} + \theta )-(Y_{(1)} + \theta) = Y_{(n)} - Y_{(1)}\) which has nothing to do with \(\theta\), andwe can conclude the range is ancillary. Way easier. And similar to the example above the hard example above. Kinda seems like there might be a point.. there is!&lt;/p&gt;

&lt;p&gt;Suppose \(\mathbf{X} \sim f(x - \theta)\) where \(f\) is an arbitrary distrbution. Define \(Y\) as above. Then \(f_Y(y) = f_X{y + \theta - \theta}\) which does not depend on \(\theta\). So the range cancels out the \(\theta\) as it does above, and we again get a distribution that does not depend on \(\theta\). Seems like for siome distributions, we can just divide the random variables and get an ancillary statistic, and for other types of distributions we can just use the range. This is where the idea of a &lt;em&gt;location-scale family&lt;/em&gt; comes in.&lt;/p&gt;

&lt;p&gt;If \(f(x)\) is a pdf, the \(\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})\) is also a pdf, specifically a &lt;em&gt;location-scale family with standard pdf&lt;/em&gt; \(f(x)\). Here, \(\mu\) is called the location parameter and \(\sigma\) is called the scale parameter. For instance, \(\mathcal{N}(\mu,\sigma^2)\) is a location-scale family with standard pdf \(\mathcal{N}(0,1)\) and location parameter \(\mu\) and scale parameter \(\sigma\).&lt;/p&gt;

&lt;p&gt;We showed above that the range is an ancillary statistic for a location family. Now, we can show that the ratio \(X_1/X_n\) is an ancillary statistic for an arbitrary scale family with pdf \(f(x/\sigma)/\sigma\) with \(\sigma &amp;gt; 0\).&lt;/p&gt;

&lt;p&gt;Define \(Y = \frac{X}{\sigma}\). Then, \(X = \sigma Y\) and \(dx/dy = \sigma\). Thefore, \(f_Y(y) = \frac{1}{\sigma}f_X(\sigma y/\sigma) \sigma = f(y)\). Then, \(\frac{X_1}{X_n} = \frac{\sigma Y_1}{\sigma Y_n} = \frac{Y_1}{Y_{n}}\) which does not depend on \(\sigma\).&lt;/p&gt;

&lt;p&gt;So its been fun learning about ancillary statistics.. but why does it matter again? Turns out that, while ancillary statistics do not contain any information about \(\theta\), they can help up the precision in our estimation of \(\theta\). How, do you say? Consider the Uniform(\(\theta, \theta + 1\)) case, and say we know the range is 0.8. This doesn’t tell us anything about \(\theta\) &lt;em&gt;alone&lt;/em&gt;. But say we also know the median is 1. If we &lt;em&gt;only&lt;/em&gt; knew the median was 1, we would only know that \(0 &amp;lt; \theta &amp;lt; 1\).  Now, by knowing the range &lt;em&gt;and&lt;/em&gt; the median, we now that \(X_{(1)} = 0.6\) and \(X_{(n)} = 1.4\). From this information, we can gather that \(\theta\) must be in the range of \(0.4 &amp;lt; \theta &amp;lt; 0.6\) because it can’t be more than 0.6 (or we wouldn’t observe the minimum we have) and vice versa for 0.4. So, in summary, ancillary statistics, &lt;em&gt;when combined with other statistics&lt;/em&gt;, can improve our understanding of the parameter. However, alone, they do nothing for us. They’re like a friend’s friend. Lots of fun to hang out with, but only when your mutual friend is around. Otherwise, things get awkward and you just want to go home.&lt;/p&gt;

&lt;h2 id=&quot;complete-statistics&quot;&gt;Complete Statistics&lt;/h2&gt;

&lt;p&gt;So lets tie all this together (not really, but lets wrap this up with final discussion about statistics).&lt;/p&gt;

&lt;p&gt;We will end this by talking about &lt;strong&gt;complete statistics&lt;/strong&gt;. Sounds cool. What is it? It’s actually defined through familes of distributions (like we talked about above). Let \(\mathcal{P} = \{ p(t \vert \theta), \theta \in \Theta \}\) be a family of distributions for \(T(\mathbf{X})\). If \(E\left[ g(T) \vert \theta \right] = 0 \; \; \forall \; \theta \; \implies P[g(T) = 0 \vert \theta] = 1 \; \forall \; \theta\), then \(T(\mathbf{X})\) is called a &lt;strong&gt;complete statistic&lt;/strong&gt;! 
Really, completeness is a property of the family of distributions. To call a statistic itself complete is almost misleading. To get some more intuition into this problem, notice that, in the discrete case,&lt;/p&gt;

\[E_{\theta}\left[ g(t) \right] = \sum_{i} g(t_i) \times P_{\theta}(T = t_i) = g(t_1) \times p_{\theta}(t_1) + \dots = 0\]

&lt;p&gt;Therefore, for \(T\) to be complete, this must mean that \(g(t)\) above is almost surely 0 - i.e there is no non-trivial (occuring with probabilty 0) \(g(t)\) that is not zero. This is analagous to the idea of linear independence of vectors. Above, we only have one.. or do we? We actually have a set of equations that corresponds to the number of \(\theta \in \Theta\). So, we have&lt;/p&gt;

\[\vec{g(\mathbf{x})} \times \mathcal{P}_{\Theta} = 0\]

&lt;p&gt;This can only happen when either \(g(T) = 0\) or \(\mathcal{P}\) does not change across separate values of \(\theta\), or if it changes very simply (i.e scaled by a constant). Therefore, completeness guarantees that distributions parameterized by different values of \(\theta\) are distinct. If you change \(\theta\), you change the whole thing. Also, conceptually note that there is a “no nonsense” part of this definition. It says “if \(g\) is expected to be zero with respect to \(\theta\), then it is just zero”. That is, there is no part of the distribution of \(g\) that does not depend on \(\theta\). No nonsense. One thing to note from this is that, if you can make an ancillary statistic out of \(T(\mathbf{X})\), then it cannot be complete… which follows immediately from what was stated above. Another thing to note - this is all defined by the family of distributions \(\mathcal{P}\), that depends on the set \(\Theta\). A statistic may be complete for a certain portion of that set, and may not be complete for another.&lt;br /&gt;
Let’s check out some examples. Suppose \(\mathbf{X} \sim \textrm{Uniform}(\theta,\theta + 1)\) and \(T(\mathbf{X}) = (X_{(1)}, X_{(n)})\). Is this a complete statistic? We should think, intuitively, that it is not. We can make an ancillary statistic (the range) out of this statistic. A complete statistic has no non-trivial part that doesn’t depend on \(\theta\), but nothing about the range depends on \(\theta\). Let \(g(T) = R - E[R]\). Now notice that \(E[R]\) is a constant with respect to \(\theta\). Therefore, \(g(T)\) is a non-zero function, but \(E[g(T)] \; \forall \; \theta\). Therfore, \(T\) is not complete!&lt;/p&gt;

&lt;p&gt;Now suppose \(T \sim \textrm{Poisson}(\lambda), \; \lambda \in \{1,2\}\). Now, if \(E_{\lambda=1}[g(t)] = 0\), \(E_{\lambda=1}[g(t)] = 0\) implies that \(\sum_{t} \frac{g(t)}{t!} = \sum_{t} \frac{g(t)2^t}{t!} =  0\) which clearly does not mean that \(g(t)\) must be zero, and you can think of &lt;em&gt;lots&lt;/em&gt; of examples. So this family is not complete. Notice how we are talking about a family of distributions… and remember how I said it depended on the range of the parameter. Now, suppose&lt;/p&gt;

\[T \sim \textrm{Poisson}(\lambda), \; \; \lambda \in \mathbb{R}^+\]

&lt;p&gt;Now, suppose there is \(g\) such that \(E[g(T)] = 0\) for all \(\lambda\). Then,&lt;/p&gt;

\[\sum_{t = 0}^{\infty} \frac{g(t)}{t!}\lambda^t = 0 = \sum_{t = 0}^{\infty} \phi(t) \lambda^t = 0\]

&lt;p&gt;Since \(\lambda &amp;gt; 0\), this can only be 0 if \(\phi(t) = 0 \; \forall \; t \; \implies \; g(t) = 0 \; \forall t\). Bam, complete. By adding more dimensionality to \(\Omega\), the parameter space for \(\lambda\), we make the family complete. Why is this? Becauase the larger the family (i.e the more paramters), the more constraints \(g\) must satisfy. At some point, the only such \(g\) is the trivial \(g\), 0. This means the family is complete.&lt;/p&gt;

&lt;p&gt;Suppose \(X_1, \dots, X_n \sim \textrm{Uniform}(0,\theta)\). Show that \(X_{(n)}\) is complete.&lt;/p&gt;

&lt;p&gt;The distribution of \(T\) is&lt;/p&gt;

\[f(t \vert \theta) = n f_X(t) \left[ F_X(t) \right] = n\frac{1}{\theta} \left[ \frac{t}{\theta} \right]^{n-1}\]

&lt;p&gt;So, assume that \(E[g(T)] = 0\). Then,&lt;/p&gt;

\[\displaystyle\int_{0}^{\theta} g(t)\frac{n}{\theta^n}t^{n-1} = 0 \implies \displaystyle\int_{0}^{\theta} g(t)t^{n-1} = 0\]

&lt;p&gt;and, by the Fundamental Theorem of Calculus,&lt;/p&gt;

\[\displaystyle\int_{0}^{\theta} g(t)t^{n-1} = \displaystyle\int_{0}^{\theta} F(t)dt = F(\theta) - F(0) = g(\theta)\theta^{n-1} = 0\]

&lt;p&gt;so, since \(\theta &amp;gt; 0 \; , \; g(\theta) = 0\) for all \(\theta &amp;gt; 0\) - concluding the \(X_{(n)}\) is complete.&lt;/p&gt;

&lt;p&gt;There’s just a few more things (2) to be said about complete statistics.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If a function of \(T \; , \; h(T)\) is ancillary, then \(T\) cannot be complete. This goes with what we said before - a complete statistic does not have any unncessary part associated with it. Let \(g(T) = h(T) - E[h(T)]\). Since \(h(T)\) is ancillary, then \(g(T) = 0\) for all \(\theta\). But, again, \(g(T)\) is non-zero. Or, at least, it doesn’t have to be zero.&lt;/li&gt;
  &lt;li&gt;Say \(T_1(\mathbf{X})\) is complete. Then \(h(T) = T_1\) is also complete. So assume \(E[g(h(T))] = E[g(T_1)] = 0\). Now, since \(T\) is complete, \(P(g(h(T))) = P(g(T_1)) = 1\) for all \(\theta\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Great! Now, to put some things together: if a minimal sufficient statistic exists, then any complete sufficient statistic is also a minimal sufficient statistic. So, if you find a statistic that is sufficent and complete, it is also minimal sufficient (assuming a minimally sufficient statistic exists). However, a minimal sufficient statistic is not always complete. So, in summary, complete \(\implies\) minimal!&lt;/p&gt;

&lt;p&gt;So, I think the main question to be answered here (outside of the questions below.. ;)) is “Why does any of this matter?”. That’s a great question, and an important question to ask of all the theory that you learn. I was a math major - and now a graduate student in statistics, and theory without relevance - for me - is still something I struggle with. However, there is relevance here … moving forward, and just in general. In general, what these definitions and theorems are moving us towards is a way to best answer “so what is going on here?” when we see some data. If I gave you a set of numbers, there is a million ways you can analyze it, but some of those ways are more efficient than others. All of this is just one builiding block of inference. It is the beginning of the journey in figuring out the &lt;em&gt;best&lt;/em&gt; way to look at data - and that is important. Some of these (ancillary..) are just tools to make other tools better. Some of these are ways to make our life easier when dealing with data. And some are really awkward, seemingly random definitions (looking at you, complete…) that are best understood in the context of what they can be used for in more advanced inference. So all of this “matters” mostly in the way that a staircase works - its harder to reach the second step if you don’t have the first one.&lt;/p&gt;

&lt;h2 id=&quot;more-examples&quot;&gt;More Examples&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Let \(X_1, \dots, X_n\) but iid from a density function of the form:&lt;/p&gt;

\[f(x \vert \sigma) = \frac{1}{\sigma}e^{-\frac{1}{\sigma}(x - \mu)}, \; \; \mu &amp;lt; x, \;\; 0 &amp;lt; \sigma\]

&lt;p&gt;Find a one-dimensional sufficient statistic for \(\mu\) (known \(\sigma\)), a one-dimensional sufficient statistic for \(\sigma\) (known \(\mu\)) and a two-dimensional sufficient statistic for (\(\mu\), \(\sigma\))&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

\[f(\mathbf{x} \vert \sigma) = \displaystyle\prod_{i=1}^n \frac{1}{\sigma}e^{-\frac{1}{\sigma}(x_i - \mu)} = \sigma^{-n}e^{\frac{-1}{\sigma}(\sum x_i - n\mu)}I(x_{(1)} &amp;gt; \mu)\]

\[\sigma^{-n}e^{\frac{-\sum x_i}{\sigma}} e^{\frac{n \mu}{\sigma}}I(x_{(1)} &amp;gt; \mu)  = h(x)e^{\frac{n \mu}{\sigma}}I(t &amp;gt; \mu) = h(x)g(\mathbf{T} \vert \mu)\]

&lt;p&gt;so, by the Factorization Theorem, \(X_{(1)}\) is a sufficent statistic for \(\mu\). For the second part, note that the likelihood can also be written as&lt;/p&gt;

\[h(x)g(\mathbf{T} \vert \sigma)\]

&lt;p&gt;where \(h(x) = e^{\frac{n \mu}{\sigma}}I(x_{(1)} &amp;gt; \mu)\)  and \(g(\mathbf{T} \vert \sigma) = \sigma^{-n}e^{\frac{-t}{\sigma}}\) where \(t = \sum_{i=1}^n x_i\). Therefore, by the Factorization Theorem again, \(\sum_{i=1}^n X_i\) is a sufficent statistic for \(\sigma\). Furthermore, the likelihood can be written as&lt;/p&gt;

\[\sigma^{-n}e^{\frac{-\sum x_i}{\sigma}} e^{\frac{n \mu}{\sigma}}I(x_{(1)} &amp;gt; \mu) = \sigma^{-n}e^{\frac{-t_1}{\sigma}} e^{\frac{n \mu}{\sigma}}I(t_2 &amp;gt; \mu) = h(x)g(\mathbf{T_1},\mathbf{T_2} \vert \mu, \sigma)\]

&lt;p&gt;so, \((\mathbf{T_2}, \mathbf{T_1}) = (X_{(1)}, \sum_{i=1}^n X_i)\) is sufficient for \((\mu,\sigma)\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Let \(X_1, \dots, X_n \sim \mathcal{N}(0, \sigma^2)\).&lt;/p&gt;

&lt;p&gt;Show that \(\sum_{i=1}^n X_i^2\) is sufficient for \(\sigma^2\) and determine whether it is a minimal sufficient statistic or not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

\[f(\mathbf{x} \vert \sigma^2) = (\frac{1}{\sqrt{2 \pi \sigma^2}})^n e^{\frac{-\sum x_i^2}{2 \sigma^2}}I(\sigma^2 &amp;gt; 0) = (2\pi)^{-n/2}(\sigma)^{-n}e^{\frac{-\sum x_i^2}{2 \sigma^2}}\]

\[= (2\pi)^{-n/2}(\sigma)^{-n}e^{\frac{-t}{2 \sigma^2}} = h(\mathbf{x})g(T(\mathbf{X}) \vert \sigma^2)\]

&lt;p&gt;where \(t = \sum_{i=1}^n x_i^2\) so by the Factorization Theorem, \(T(\mathbf{X}) = \sum_{i=1}^n X_i^2\) is sufficient for \(\sigma^2\). Now,&lt;/p&gt;

\[\frac{f(\mathbf{x} \vert \sigma^2)}{f(\mathbf{y} \vert \sigma^2)} = e^{\frac{-1}{2\sigma^2}(\sum x_i^2 - \sum y_i^2)} = e^{\frac{-1}{2\sigma^2}(T(\mathbf{x}) - T(\mathbf{y}))}\]

&lt;p&gt;If \(T(\mathbf{x}) = T(\mathbf{y})\), this ratio is 1 which is clearly free of \(\sigma^2\). Now, if the ratio is free of \(\sigma^2\), then it is necessary that \(T(\mathbf{x}) = T(\mathbf{y})\). Thus, \(T(\mathbf{x}) = \sum X_i^2\) is a minimal sufficient statistic for \(\sigma ^2\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Let \(X_1, \dots, X_n \sim \textrm{Poisson}(\lambda)\).&lt;/p&gt;

&lt;p&gt;Find a sufficient statistic for \(\lambda\) and show that it is a minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

\[f(\mathbf{x} \vert \lambda) = \frac{1}{\prod_{i=1}^n x_i !}e^{-n\lambda}\lambda^{\sum x_i} = \frac{1}{\prod_{i=1}^n x_i !}e^{-n\lambda}\lambda^{t} = h(\mathbf{x})g(T(\mathbf{x}) \vert \lambda)\]

&lt;p&gt;so by the Factorization Theorem (again…), \(\sum_{i=1}^n X_i\) is sufficient for \(\lambda\). Now,&lt;/p&gt;

\[\frac{f(\mathbf{x} \vert \lambda)}{f(\mathbf{y} \vert \lambda)} \propto_{\lambda} \lambda^{\sum x_i - \sum y_i} = \lambda^{T(\mathbf{x}) - T(\mathbf{y})}\]

&lt;p&gt;so if \(T(\mathbf{x}) = T(\mathbf{y})\), the ratio is free of \(\lambda\). Furthermore, if the ratio is free of \(\lambda\), then it is necessary that \(T(\mathbf{x}) = T(\mathbf{y})\) for the exponent to be 0 and thus have \(\lambda\) disappear. Therefore, \(T(\mathbf{x}) = \sum X_i\) is also a minimal sufficient statistic for \(\lambda\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let \(X_1, \dots, X_n \sim f_X(x \vert \theta)\) where&lt;/p&gt;

\[f_X(x \vert \theta) = \frac{2x}{\theta^2}, \; \; 0 &amp;lt; x &amp;lt; \theta\]

&lt;p&gt;Find a minimal sufficent statistic for \(\theta\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

\[\frac{f_X(\mathbf{x} \vert \theta)}{f_X(\mathbf{y} \vert \theta)} = \frac{\prod x_i I(x_{(n)} &amp;gt; \theta)}{\prod y_i I(y_{(n)} &amp;gt; \theta)} \propto_{\theta} \frac{I(x_{(n)} &amp;gt; \theta)}{I(y_{(n)} &amp;gt; \theta)}\]

&lt;p&gt;Therfore, since this ratio doesn’t depend on \(\theta\) if and only if \(x_{(n)} = y_{(n)}\), then \(X_{(n)}\) is minmial sufficient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose \(X_1, \dots , X_n \sim f(x \vert \theta)\) where&lt;/p&gt;

\[f(x \vert \theta) = \theta x^{\theta - 1} \textrm{exp}(-x^{\theta})\]

&lt;p&gt;with \(\theta ,\; x \; &amp;gt; \; 0\). Show that \(\frac{\log X_{(n)}}{\log X_{(1)}}\) is ancillary. Well, let \(Y = \theta \log(X) \implies X = e^{Y/\theta}\). Then,&lt;/p&gt;

\[f_Y(y) = f_X(e^{Y/\theta})\vert \frac{dx}{dy} \vert = \theta \; \textrm{exp}(\frac{y}{\theta}(\theta - 1))\textrm{exp}(-\textrm{exp}(\frac{y}{\theta}(\theta))) \frac{1}{\theta}e^{y/ \theta}\]

\[= \textrm{exp}(y - e^y)\]

&lt;p&gt;Now, since \(\theta &amp;gt; 0\) and \(\log(*)\) is increasing monotonically, the maximum and minimum values are the same after the mappings. Thus,&lt;/p&gt;

\[\frac{\log X_{(n)}}{\log X_{(1)}} = \frac{Y_{(n)}}{Y_{(1)}}\]

&lt;p&gt;which is ancillary, since the distribution of \(Y\) does not depend on \(\theta\), as we have shown above.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose \(X_1, \dots, X_n \sim \textrm{Uniform}(-\theta, \theta)\). Is \(T(\mathbf{X}) = (X_{(1)}, X_{(n)})\) a complete sufficient statistic? If not, is \(\textrm{max}_i \vert X_i \vert\)?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first question: No, and this should be easy to see. This is a scale family, so we can make an ancillary statistic out of \(\frac{X_{(n)}}{X_{(1)}}\). Just let \(X = \theta Y\) and do the transform. Therefore, this statistic cannot be complete. More cleary,&lt;/p&gt;

\[f_Y(y) = f_X(\theta y) \vert \frac{d \theta y}{dy} \vert I(-\theta &amp;lt; y \theta &amp;lt; \theta) = \frac{1}{2}I(-1 &amp;lt; y &amp;lt; 1)\]

&lt;p&gt;which is Uniform(0,1). Then, let \(g(T) = \frac{X_{(n)}}{X_{(1)}} - E\left[ \frac{X_{(n)}}{X_{(1)}} \right] = \frac{Y_{(n)}}{Y_{(1)}} - E \left[ \frac{Y_{(n)}}{Y_{(1)}} \right]\). Now we see \(E\left[ g(t) \vert \theta \right] = 0\) for all \(\theta\) but it doesn’t mean that \(g(T) = 0\) almost surely.&lt;/p&gt;

&lt;p&gt;Now, for \(Y = \textrm{max}_i \vert X_i \vert\). Lets define the distribution first for \(\vert X \vert\)&lt;/p&gt;

\[F_Y(y) = P(Y &amp;lt; y) = P(- y &amp;lt; X &amp;lt; y ) = P(X &amp;lt; y) - P (X &amp;lt; -y)\]

\[\implies f_y(y) = f_X(y) - f_X(-y)(-1) = f_X(y) - f_X(-y) = \frac{1}{\theta} I(0 &amp;lt; y &amp;lt; \theta)\]

&lt;p&gt;which is Uniform(\(0, \theta\)). Now, we know that \(T = \textrm{max}_i \vert X_i \vert\) is distributed with pdf&lt;/p&gt;

\[f_T(t \vert \theta) = nf(X)[F(x)]^{n-1}\]

&lt;p&gt;and now this is the exact problem we did in the examples above. So this is a complete statistic! Is it sufficient?&lt;/p&gt;

\[f_X(x \vert \theta) = \left[ \frac{1}{2 \theta}\right]^n I(-\theta &amp;lt; \mathbf{x} &amp;lt; \theta) = \left[ \frac{1}{2 \theta}\right]^n I(t &amp;lt; \theta)\]

&lt;p&gt;so it is sufficient by the factorization theorem! Therefore, this is a complete sufficient statistic.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/2019/12/31/gradient-descent.html" rel="alternate" type="text/html" title="Gradient Descent" /><published>2019-12-31T00:21:32-05:00</published><updated>2019-12-31T00:21:32-05:00</updated><id>http://localhost:4000/2019/12/31/gradient-descent</id><content type="html" xml:base="http://localhost:4000/2019/12/31/gradient-descent.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;You’ll remember in OLS, we had something called the &lt;strong&gt;normal equations&lt;/strong&gt; - a nice, succinct, simple formula for calculating out best-fit parameters. You’ll also remember, then, that we had to invert an n by n matrix, \(X^TX\) to get these parameters.. which, if n is large, is computationally very expensive. Today we’ll discuss gradient descent, a less computationally expensive way (for large n) to obtain parameters that minimize our squared error.&lt;/p&gt;

&lt;p&gt;It’s important to first note that gradient descent is not limited to least squares. Gradient descent is just an algorithm that can be used to maximize (or minimize) any convex objective function by calculating the gradient at a point, taking a step in that direction, calculation the gradient again, taking a step in that direction… so on and so forth, until the gradient is basically norm 0. It just so happens that the least squares function is convex, so gradient descent converges to the minimum.&lt;/p&gt;

&lt;p&gt;This algorithm won’t work for non-convex functions as it may only search out a local minimum, not the global minimum. If you’re standing at the base of two hills, the gradient is just going to take you up the steepest hill, not necessarily the tallest one, right? Okay.. anyways, do we remember what the gradient is?&lt;/p&gt;

&lt;p&gt;Let’s say we have&lt;/p&gt;

\[J(\beta_0, \beta_1, \dots, \beta_q): \mathbb{R}^{q+1} \rightarrow \mathbb{R}\]

&lt;p&gt;then, the gradient is defined as&lt;/p&gt;

\[\nabla J = \frac{\partial J}{\partial \beta_0} \hat{e_1} + \dots + \frac{\partial J}{\partial \beta_q} \hat{e_{q+1}} = \begin{bmatrix} \frac{\partial J}{\partial \beta_0} \\ \vdots \\ \frac{\partial J}{\partial \beta_q} \end{bmatrix}\]

&lt;p&gt;so, essentially, each component of the gradient tells you how fast your function changes with respect to the standard basis in each direction. We can find the directional derivative of any unit vector \(\vec{v}\) by \(\nabla J \cdot \vec{v}\)&lt;/p&gt;

\[\nabla J \cdot \vec{v} = \| \nabla J\|\|\vec{v}\|\cos(\theta) =\| \nabla J\|\cos(\theta)\]

&lt;p&gt;since \(\vec{v}\) is a unit vector, where \(\theta\) is the angle between the two vectors. \(\cos(\theta)\) is max when $\theta = 0$ - that is, when \(\vec{v}\) is in the direction of the gradient! So the steepest ascent is in the direction of the gradient, and the steepest descent is in the opposite direction. We should note that the steepest ascent here is limited to unit vectors, so the steepest ascent is really in the direction of \(\frac{\nabla J}{\| \nabla J\|}\), right? But that’s the same direction as the gradient. And we are going to kinda normalize the gradient in our own way later… you will see.&lt;/p&gt;

&lt;p&gt;You can think of the gradient as kind of a trade-off. Say the gradient is \(&amp;lt;3,1,6&amp;gt;\). You’re confined to a circle in terms of where you can move with the gradient (or any vector) starting from a point. That is, there’s a trade-off between directions. You can essentially trade 3 steps in the Y direction for one step in the X direction. So an optimal direction does just that, hence why the optimal direction is the gradient… it takes one 3 steps in the X direction for each step in the Y direction. Any other decision is ‘unfair’, i.e there is a better trade-off that maximizes how steep the gradient could be.&lt;/p&gt;

&lt;p&gt;Anyways. The gradient heads in the direction of the steepest ascent, so an algorithm searching out the lowest point of a function should head in the opposite of the gradient. So let’s consider least squares, where our squared error loss term is a function of \(\mathbf{\beta}\), the vector of parameters. That is,&lt;/p&gt;

\[J(\beta_0, \dots, \beta_q) = \frac{1}{2n} \sum_{i=1}^n (\hat{y_i} - y_i)^2  =\frac{1}{2n} \sum_{i=1}^n ( \beta_0 + \beta_1x_{i1} + \dots + \beta_qx_{iq} - y_i )^2\]

&lt;p&gt;where we just normalized the squared error function. Remember, \(q\) is the number of features, and \(q+1\) is the number of parameters we must estimate. Now, using a bit of calculus, we see that&lt;/p&gt;

\[\frac{\partial J} {\partial \beta_0} = \frac{1}{n} \sum_{i=1}^n ( \hat{y_i} - y_i)\]

&lt;p&gt;and, for \(j = 1,\dots, q\),&lt;/p&gt;

\[\frac{\partial J} {\partial \beta_j} = \frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)x_{ij}\]

&lt;p&gt;so, a gradient descent algorithm would look something like&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;is the norm of the gradient greater than our threshold?
    &lt;ul&gt;
      &lt;li&gt;if yes, move along the gradient for each \(\beta_J\) by some \(\alpha\) and check again&lt;/li&gt;
      &lt;li&gt;if no, return the current point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where \(\alpha\) is a stepsize of your choosing. It is important to note that the choice of the step-size is important. A step-size that is too big will cause the algorithm to jump over the minimum value and maybe even diverge, whereas a step-size that is too small will mean the algorithm takes forever to converge to the minimum value. In one dimension, we can see what is happening below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/lambda_tut.jpg&quot; alt=&quot;Different stepsizes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the step size is small (left), it is converging to the minimum value… but it is gonna take forever (maybe even actually. . .) and on the right, where the step size is too large, the minimum is being overshot and then the larger slope (gradient) is compounding with a large step size to overshoot by even more and so on and so forth as your algorithm sadly diverges to infinity ….. :(&lt;/p&gt;

&lt;p&gt;You can imagine this type of computation is best done in a while-loop. Below is an interpretation of this algorithm in Julia:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;&lt;span class=&quot;n&quot;&gt;gradient_descent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;α&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;#for i in 1:q_plus_one&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#    gradient[i] = α*(error&apos;*X[:,i])&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#end&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first part of this just says ‘okay, did you add an intercept? maybe not.. because you might not think to add the column of ones… we are gonna assume you want to do this so we will do it for you if you did not’. Obviously, this isn’t necessary, and could actually lead to incorrect results if the column  missing isn’t the one of all ones, but it is just to show you that coding is YOUR life, do with it what you will… Anyways. We compute the gradient via the dot product, as you will notice that&lt;/p&gt;

\[\sum_{i=1}^n (\hat{y_i} - y_i) = e^T \cdot \mathbf{1_n}\]

&lt;p&gt;and&lt;/p&gt;

\[\sum_{i=1}^n (\hat{y_i} - y_i)x_{ij} = e^T  \cdot \mathbf{X}[:,j]\]

&lt;p&gt;where \(e\) is the n-dimensional vector of errors and $latex X$ is the feature matrix, with a row of ones so that&lt;/p&gt;

\[\sum_{i=1}^n (\hat{y_1} - y_i) = e^T \cdot \mathbf{X}[:,1] = e^T \cdot \mathbf{1_n}\]

&lt;p&gt;We also add an iteration count (just in case our step size is too small or too large or we totally messed up the code) to stop the loop at a certain point. However, this way of thinking about the algorithm is commented out because it is not efficient. Generally, for-loops should (and usually can) be replaced by matrix multiplication, which is in the code above! Let’s go ahead and see if we can see how you would write that ‘in math’…..&lt;/p&gt;

&lt;p&gt;You can see in the code that&lt;/p&gt;

\[\nabla J = \mathbf{X^T(X\beta - Y)}\]

&lt;p&gt;which is actually just exactly what we wrote above. You’re multiplying the errors by the columns of the feature matrix to get the components of the gradient, so why not just transpose your matrix and use matrix multiplication? Thats what this is saying. Thus,&lt;/p&gt;

\[\mathbf{\beta^{(new)}} = \mathbf{\beta^{(old)}} - \alpha \frac{1}{n} \mathbf{X^T( X\beta - Y)}\]

&lt;p&gt;Perfect. So now we know what it is, how to use it.. but when would you use it? Usually only when you have really large values of n (lots of observations, like millions…) . Essentially, the only reason you use it is time. There are other things you can do here, like use only one observation (which is called Least Mean Squares) which saves even more time but isn’t as great, obviously. There are other issues with gradient descent, like correctly choosing the step-size that, in practice, you really have no way around. There are plenty of ways to choose a step-size, but no set-rule - you’ll have to figure it out on your own (and that takes time, too!). What’s best about gradient descent is that you can use it for a lot of things. Least squares is just one. You can use it for plenty of other things… really, you can use it for whatever problem you have where you need to optimize a function. But I hope learning about it via least squares was helpful!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Introduction to Survival Analysis</title><link href="http://localhost:4000/2019/10/21/introduction-to-survival-analysis.html" rel="alternate" type="text/html" title="Introduction to Survival Analysis" /><published>2019-10-21T23:00:00-04:00</published><updated>2019-10-21T23:00:00-04:00</updated><id>http://localhost:4000/2019/10/21/introduction-to-survival-analysis</id><content type="html" xml:base="http://localhost:4000/2019/10/21/introduction-to-survival-analysis.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Survival Analysis: the branch of statistics that pretty much everyone I know only associates with biostatistics. I get it - that makes sense. Sure, survival implies.. surviving, but lots of things survive. My attempt to be a vegetarian (still going) is something that ‘survives’. Your T.V., your willingness to stay on an email list. Things like that. Survival analysis takes on many different names (for instance, reliability in engineering). Essentially, it deals with data that has ‘failure’ times and ‘censoring’ times - so it takes into account actual and observational failure times. For example, a failure time could be death (actual failure) and a censoring time could be a patient lost to follow-up (observational failure). It’s a cool subject - one that is applicable to almost every field, so let’s dive into some of the basics. The first : \(T\). Big one. The time to event, which is a random variable. The second: \(C\). Also a big one, the time to censoring. And finally, the first and second combined into the third(?): \(X = min(T,C)\) which is the event we actually observe. Ok, now actually finally, we have \(\Delta = I(T \leq C) = I (X = T)\), which is just 1 if we observe the event and 0 if we lose the observation to censoring. Those are the big variables for the introduction, so let’s get into talking about the main functions and some examples.&lt;/p&gt;

&lt;p&gt;There are 3 main functions in survival analysis, and all can be derived from one another. We have the survival function, the hazard function (or hazard rate, or just hazard) and the cumulative hazard function. Starting with the survival function:&lt;/p&gt;

&lt;p&gt;\(S(t) = P(T &amp;gt;t)\) if t is continuos and \(S(t _ j)= P(T &amp;gt; t _ j) = P(T \geq t_{j-1} )\) when t is discrete. How do we define this? We need to define a density first.&lt;/p&gt;

\[f_ T(t) = \lim_{\delta\to 0^+}\frac{1}{\delta}P(t  \leq T \leq t + \delta)\]

&lt;p&gt;when  \(T\) is continuous and, simply,&lt;/p&gt;

\[f_T(t) = P(T = t)\]

&lt;p&gt;when \(T\) is discrete. Now, to define our survival function should be relatively easy.&lt;/p&gt;

&lt;p&gt;\(S(t) = P(T  &amp;gt; t) = \displaystyle\int_ {t}^{\infty}f_T(u)du\) when \(T\) is continuous&lt;/p&gt;

&lt;p&gt;\(S(t) = P(T &amp;gt;t) = \displaystyle\sum_ {n &amp;gt; t}f_T(t)\) when \(T\) is discrete.&lt;/p&gt;

&lt;p&gt;A few things to note about a survival function. \(S(0) = 1\) as we should not have events happening at time 0. \(\lim_ {t\to \infty}S(t) = 0\) as we expect everyone to die or have an event as time goes on. Also, the survival function should be monotone decreasing in \(t\). It is important to note that, in the discrete case, the survival function is right continuous. That is, it has jumps (down) at the values of \(t\) where events happen, and is evaluated and the lower probability of survival for those time periods. More formally \(S(t) = \lim_{x\to t^+}S(x) = S(t^+)\) This is important when we are estimating… the hazard function! The hazard function is the probability that a person dies at time \(t\) GIVEN that they have survived until time \(t\). Let \(\lambda (t)\) denote the survival time.&lt;/p&gt;

\[\lambda(t) = \lim_{\delta \to 0^+}\frac{1}{\delta}P(t \leq T \leq t + \delta  \vert T &amp;gt; t)\]

&lt;p&gt;when \(T\) is continuous and&lt;/p&gt;

\[\lambda(t) = P(T = t \vert T \geq t)\]

&lt;p&gt;when \(T\) is discrete. In general, from the basic probability theory,&lt;/p&gt;

\[P(T =t \vert T \geq t) = \frac{P(T = t, T \geq t)}{P(T \geq t)} = \frac{P(T = t)}{P(T \geq t)} = \frac{f_T(t)}{S(t^-)}\]

&lt;p&gt;Notice that the hazard function, when continuous, is a rate and, when discrete, is a probability. Finally, let us define \(\Lambda(t)\) as the cumulative hazard function. It is pretty much exactly what you would think it was…&lt;/p&gt;

\[\Lambda(t) = \displaystyle\int_{0}^{t}\lambda(u)du\]

&lt;p&gt;when continuous and&lt;/p&gt;

\[\Lambda(t) = \displaystyle\sum_{t_i \leq t} \lambda(t_i)\]

&lt;p&gt;when discrete. Let’s take a look at an interesting problem involving the cumulative hazard function. Say \(T\) follows some distribution with cdf \(F_t(t)\). Then its hazard function follows an exponential(1) distribution. To see this&lt;/p&gt;

\[P(\Lambda(T) \leq t) = P( - \Lambda(T)\geq -t ) = P(e^{-\Lambda(T)} \geq e^{-t}) =\]

\[P(-e^{-\Lambda(T)} \leq -e^{-t}) = P(1 - e^{-\Lambda(T)} \leq 1 - e^{-t}) =\]

\[P(1-S(T) \leq 1- e^{-t}) = P(F_ T(T) \leq 1- e^{-t}) = P(T \leq F_ T^{-1}(1- e^{-t})) = F_ T(F_T^{-1}(1- e^{-t})) = 1- e^{-t}\]

&lt;p&gt;which is the cdf of the exponential(1) distribution! Cool, huh? Anyways.&lt;/p&gt;

&lt;p&gt;It is clear what the link between the hazard function and the cumulative hazard function is, but now let us find the explicit link with the survival function. First (and pretty much only..), remember&lt;/p&gt;

\[\lambda(t) = \frac{f(t)}{S(t^-)}\]

&lt;p&gt;When \(T\) is continuous, this is equivalent to&lt;/p&gt;

\[\lambda(t) = \frac{f(t)}{S(t^-)} = \frac{f(t)}{S(t)} = \frac{-d\log S(t)}{dt}\]

&lt;p&gt;which means&lt;/p&gt;

\[\Lambda(t) = -\log(S(t)\]

&lt;p&gt;and, equivalently,&lt;/p&gt;

\[S(t) = e^{-\Lambda(t)}\]

&lt;p&gt;Nice! Easy. Let’s do an example. Suppose \(T\) is such that its survival function is&lt;/p&gt;

\[S(t) = \frac{64}{(t+8)^2}\]

&lt;p&gt;This is continuous. First, a fact:&lt;/p&gt;

\[E[T] = \displaystyle\int_{0}^{\infty} S(t)dt\]

&lt;p&gt;How to prove?&lt;/p&gt;

\[E[T] = \displaystyle\int_{0}^{\infty} tf(t)dt = \displaystyle\int_{0}^{\infty}\displaystyle\int_{0}^{t}ds f(t)dt = \displaystyle\int_{0}^{\infty}\displaystyle\int_{s}^{\infty}f(t)dtds = \displaystyle\int_{0}^{\infty}1 - F(s) ds  = \displaystyle\int_{0}^{\infty}S(s)ds\]

&lt;p&gt;So, what is our expected survival time in the above case?&lt;/p&gt;

\[E[T] = 64 \displaystyle\int_{0}^{\infty} (t + 8)^{-2} = 64/8 = 8\]

&lt;p&gt;Median survival time? Clearly, that is just where&lt;/p&gt;

\[S(t) = 0.5\]

&lt;p&gt;so&lt;/p&gt;

\[\frac{64}{(t_m +8)^2} = 0.5 \implies \sqrt(128) - 8 = t_m = 3.31\]

&lt;p&gt;Now, let’s estimate the hazard function (and the cumulative hazard).&lt;/p&gt;

\[\Lambda(t) = -\log(S(t)) = 2 \log(t+8) - \log(64)\]

&lt;p&gt;Now, we just take the derivative with respect to \(t\) to find \(\lambda(t)\)&lt;/p&gt;

\[\lambda(t) = \frac{2}{t+8}\]

&lt;p&gt;The discrete case is a liiiiiiitle bit more difficult, but very important for continued study in survival analysis. In practice, unless we have literall infinite observations, we will be dealing with discrete data. Let’s say \(t\) takes on values \(t_ 1, \dots , t_ n\). Also, remember (more basic probability theory) that \(P(AB) = P(A\vert B)P(B)\). If \(B\) is many events, then this formula is recursive. Now, suppose \(t_j \leq t \leq t_{j+1}\)&lt;/p&gt;

\[S(t) = P(T &amp;gt; t) = P(T &amp;gt; t_j, T &amp;gt; T_{j-1},\dots,T &amp;gt; t_1) =\]

\[P(T &amp;gt; t_j \vert T &amp;gt; t_{j-1})\times P(T &amp;gt; t_{j-1} \vert T &amp;gt; t_{j-1})\times \dots \times P(T &amp;gt; t_1)\]

\[= [1 - P(T \leq t_j \vert T &amp;gt; t_{j-1})]\times \dots \times [1 - P(T\leq t_1)]\]

\[= [1 - P(T = t_j \vert T \geq t_{j})]\times \dots \times [1 - P(T = t_1)]\]

\[= \displaystyle\prod_{t_i\leq t} [1 - \lambda(t_i)]\]

&lt;p&gt;Notice, this formula is recursive! \(S(t_j) = [1 - \lambda(t_j)]S(t_{j-1})\) which doesn’t really matter here but .. it makes it easier to compute if, say, you wanted to write up this algorithm to help yourself learn (maybe?).  This actually has an analogous case in the continuous case (if we remember the Taylor Expansion for \(e^{-x}\) LOL). To the rescue again..&lt;/p&gt;

\[e^{-x} = \displaystyle\sum_{n = 0}^{\infty} \frac{(-x)^n}{n!} \approx 1- x\]

&lt;p&gt;when \(x\) is small.&lt;/p&gt;

&lt;p&gt;So, since 
\(\Lambda(T) = \int_{0}^{t}\lambda(u)du\)&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

\[S(t) = e^{-\Lambda(t)}\]

&lt;p&gt;then&lt;/p&gt;

\[S(t) = \displaystyle\prod_{u=0}^t e^{-\lambda(u)du} \approx \displaystyle\prod_{u=0}^t[1 - \lambda(u)du]\]

&lt;p&gt;which is sometimes referred to as the product limit form of the survival function. Anyways.. let’s get to some real examples. First, from data.. let’s think of a good way to estimate \(\lambda(t)\). Seems natural to estimate the hazard at a certain time point by looking at the number of events at a time point relative to the number of people at risk for the event at that time point. So, letting \(D_ J\) denote the number of events at time \(t_ j\) and \(Y _ j\) denote the number at risk (sill being observed, including the people who experience events) for an event at \(t_j\). Then a natural estimate for \(\lambda(t_j)\) is \(\frac{D_j}{Y_j}\). Looking at some data (+ indicates a censored observation)…&lt;/p&gt;

\[2,5^+,8,12^+,15,21^+,25,29,30^+,34\]

&lt;p&gt;Let’s estimate \(S(10)\). Notice, the first term of this estimation will be \(P(T &amp;gt; 10 \vert T &amp;gt; 8)\) which is 1, since nothing happens at 10.. so if someone survives past 8, they will survive past 10. For sure. With probability 1. :) SO,&lt;/p&gt;

\[S(10) = S(8) =  \displaystyle\prod_{t \in \{2,5,8\}}[1 - \lambda(t)]\]

\[= [1- \frac{D_8}{Y_8}][1- \frac{D_5}{Y_5}][1- \frac{D_2}{Y_2}]\]

\[= [1- \frac{1}{8}][1- \frac{0}{9}][1- \frac{1}{10}] = \frac{63}{80}\]

&lt;p&gt;Notice at 5, the observation was censored, so it does not get included in \(D_5\).&lt;/p&gt;

&lt;p&gt;Note that this estimate of our survival function will be piece-wise continuous, right continuous with jumps at event times, and will be 0 IF our last observation is a failure. Why can we do this when we have censored data? Because we assume that \(T\) is independent of \(C\), or else we would have a joint distribution for the hazard. Anyways… this type of estimation is what is referred to as the Kaplan-Meier estimate of the survival function which, along with Nelson-Aalen, is the most popular non-parametric (or just in general) way to estimate the survival function.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Bayesian Estimation</title><link href="http://localhost:4000/2018/07/13/bayesian-estimation.html" rel="alternate" type="text/html" title="Bayesian Estimation" /><published>2018-07-13T01:21:32-04:00</published><updated>2018-07-13T01:21:32-04:00</updated><id>http://localhost:4000/2018/07/13/bayesian-estimation</id><content type="html" xml:base="http://localhost:4000/2018/07/13/bayesian-estimation.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;When most people think about statistics, they think about flipping a coin or gambling or having flashbacks to dropping out of high school because they hated the class so much… moral of the story is, it’s just one thing to think about. What if I told you that that wasn’t necessarily true? I am about to tell you that. Statistics is the science of modeling randomness. But what is random, and what is not? That is a fundamental question in statistics and splits the subject into two separate approaches - Bayesian and Frequentist.&lt;/p&gt;

&lt;p&gt;What does it mean to say that I am 50% certain it will rain tomorrow? That is not so much a statement of probability, but rather of uncertainty. It will rain or it will not rain, but that outcome is something you are uncertain of. Consider another example: flip a coin and cover it up immediately once it lands, so you don’t know what the result is. What’re the chances that the coin is heads? From a traditional, frequentist perspective, there is no answer to this question. It already happened. There is already a result - you’re just ignorant. But that’s not really how we think, is it? We don’t know what the result is - we are uncertain of it. Let’s consider a medical example. Say you’re getting screened for the flu, and the result comes out positive. Do you actually have the flu? In the frequentist approach, that doesn’t make sense to ask. We do have sensitivity and specificity. Let \(\theta = 1\) if you have the flu and \(\theta = 0\) if you don’t. Then the sensitivity is the probability the test is positive given that you have the flue, or \(P(T = 1 \vert \theta = 1)\) and the specificity is \(P(T = 0 \vert \theta = 0)\), the probability the test is negative when you don’t have the flu. If specificity is not 1, you might get tested positive but not have the disease. It makes sense then to ask ‘“Do I actually have the flu?”, but does it make statistical sense to ask ‘Do I have the flu?’? That is, what is \(P(\theta =1 \vert T = 1)\)? A frequentist says ‘Bad question. \(\theta\) is fixed’. A Bayesian will come in and say ‘Hold on, \(\theta\) is not actually fixed. We can estimate this’&lt;/p&gt;

&lt;p&gt;This is the whole premise of Bayesian statistics - to use probability theory to not &lt;em&gt;only&lt;/em&gt; quantify probability but also to quantify uncertainty. Much better!! Solving problems that were disregarded earlier - we like that!&lt;/p&gt;

&lt;p&gt;Okay, moving on… now that I’ve converted you to the Bayesian approach to statistics…&lt;/p&gt;

&lt;p&gt;:)&lt;/p&gt;

&lt;p&gt;Let’s talk about the setup. In a frequentist approach, we have a random variable \(\mathbf{X}\), observed data from that random variable, a model, and a parameter \(\theta \in \Omega\) that is unknown but &lt;em&gt;fixed&lt;/em&gt;. In the Bayesian framework, we don’t consider that italicized part. It’s a subtle change that makes a huge difference. Now, we have&lt;/p&gt;

\[\theta \sim \pi(\theta)\]

&lt;p&gt;where \(\pi\) is called a &lt;em&gt;prior distribution&lt;/em&gt;. Now, we need to make an adjustment to your original data’s distribution. Now, we have a conditional distribution!&lt;/p&gt;

\[\mathbf{X} \vert \theta \sim f(\mathbf{x}\vert \theta)\]

&lt;p&gt;and, by Bayes rule, the joint distribution of \(\mathbf{X}\) and \(\theta\) is&lt;/p&gt;

\[f(\mathbf{x},\theta) = \pi(\theta)f(\mathbf{x}|\theta)\]

&lt;p&gt;and, thus, the marginal distribution of  \(\mathbf{X}\) is&lt;/p&gt;

\[m(\mathbf{x}) = \displaystyle\int_{\theta \in \Omega} {f(\mathbf{x},\theta)d\theta}\]

\[\pi(\theta \vert \mathbf{x}) = \frac{\pi(\theta)f(\mathbf{x} \vert \theta)}{m(\mathbf{x}) }\]

&lt;p&gt;We now have all the tools we need to do some basic Bayesian analysis!&lt;/p&gt;

&lt;p&gt;and, so, by Bayes rule again, the posterior distribution of \(\theta\) is&lt;/p&gt;

&lt;p&gt;Let us look at \(X_1, \dots, X_n \sim Bernoulli(p)\) where \(p \sim Beta(\alpha, \beta)\). Then we could calculate \(\pi(\theta \vert \mathbf{x}) = \frac{\pi(p)f(\mathbf{x}\vert \theta)}{m(\mathbf{x}) }\), but that would take forever. Does \(m(\mathbf{x})\) matter? No. Why? Think about it. We want the posterior distribtion of \(p\), right? So \(m(\mathbf{x})\) has &lt;em&gt;literally&lt;/em&gt; (millenials…) nothing to do with \(p\), therefore it just acts as a normalizing constant in the equation to make sure that \(\pi(p \vert \mathbf{x})\) is a true pdf. Therefore, we can just look at the numerator of the expression and recognize the kernel of the distribution to understand what family the posterior distribution comes from. Soo, in math,&lt;/p&gt;

\[\pi(p \vert \mathbf{x}) \propto \pi(p)f(\mathbf{x} \vert p)\]

&lt;p&gt;We know&lt;/p&gt;

\[f(\mathbf{x}\vert p) = \displaystyle \prod_{i=1}^{n} f(\mathbf{x_i}\vert p) = \displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i})\]

&lt;p&gt;and&lt;/p&gt;

\[\pi(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}\]

&lt;p&gt;so&lt;/p&gt;

\[\pi(p \vert \mathbf{x}) \propto\displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i}) \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}\]

\[= p^{\sum_{i=1}^{n}x_i}(1-p)^{n - \sum_{i=1}^{n}x_i}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}\]

&lt;p&gt;which is the same as&lt;/p&gt;

\[\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha + \sum_{i=1}^{n}x_i -1} (1 - p)^{\beta +n - \sum_{i=1}^{n}x_i - 1}\]

&lt;p&gt;Looking at the part of this expression that has to do with \(p\), what do we see? It is the kernel of a Beta distribution! So, now we know that \(\pi(p
\vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)\). Cool. Another thing to notice here: the prior and the posterior distribution are from the same family. This, informally, is what you call a &lt;strong&gt;conjugate family&lt;/strong&gt;.  Anyways, now that we have the prior distribution, we can do some estimation.&lt;/p&gt;

&lt;p&gt;If we remember, one of the frequentist approaches to estimation is Maximum Likelihood. Here, our parameter follows a distribution so it makes sense to just say ‘hey… what’s its expected value?’ . So that’s what people do! The Bayes estimator is literally just \(E[p\vert \mathbf{x}]\). We know \(\pi(p \vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)\) . Since we know the expected value of \(Beta(\alpha,\beta)\) is \(\frac{\alpha}{\alpha + \beta}\), it follows that&lt;/p&gt;

\[E[p \vert \mathbf{x}] = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \sum_{i=1}^{n}x_i + \beta + n - \sum_{i=1}^{n}x_i} = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \beta + n}\]

\[= \bar{X}(\frac{n}{\alpha + \beta + n}) + \frac{\alpha}{\alpha + \beta}(\frac{\alpha + \beta }{\alpha + \beta + n})\]

&lt;p&gt;So the Bayes estimator is just a weighted average of the prior mean and the MLE! When n is large, the MLE is weighted much more and, when n is small, we trust the prior distribution’s estimate. This is a very cool result, in my opinion.&lt;/p&gt;

&lt;p&gt;Let’s look at another example. Let \(X_1, \dots , X_n \vert \lambda \sim Poisson(\lambda)\) and let \(\pi(\lambda) \sim Gamma(\alpha, \beta)\). Then&lt;/p&gt;

\[\pi(\lambda \vert \mathbf{x}) \propto f(\mathbf{x}\vert \lambda )\pi(\lambda)\]

\[= \frac{\lambda^{\sum_{i=1}^{n}x_i} e^{-n\lambda}}{\prod_{i=1}^{n}x_i !} \frac{1}{\Gamma(\alpha)\beta^{\alpha}} \lambda^{\alpha-1}e^{\frac{-\lambda}{\beta}}\]

\[= \frac{1}{\prod_{i=1}^{n}x_i! \, \Gamma(\alpha)\beta^{\alpha}} \lambda^{\sum_{i=1}^{n}x_i +\alpha -1} e^{-\lambda(n + \frac{1}{\beta})}\]

&lt;p&gt;and we should recognize the kernel (part with \(\lambda\)) of this distribution and conclude that&lt;/p&gt;

\[\pi(\lambda\vert \mathbf{x}) \sim Gamma(\sum_{i=1}^{n}x_i + \alpha , (n + \frac{1}{\beta})^{-1})\]

&lt;p&gt;Another conjugate family! The Bayes estimator here is just&lt;/p&gt;

\[E[ \lambda | \mathbf{x}] = \frac{\sum_{i=1}^{n}x_i + \alpha }{n + \frac{1}{\beta}}\]

\[= \bar{X} (\frac{n}{n + \frac{1}{\beta}}) + \alpha \beta (\frac{1}{n\beta + 1})\]

&lt;p&gt;In summary, its quite simple to find the posterior distribution in these cases - but this is obviously not true in general. Sometimes the posterior takes an unknown form, and the best we can do is draw samples from this posterior using different methods (Gibbs Sampling, Importance Sampling). These examples above are cases where the posterior follows almost immediately from the joint distribution (conjugate family in both cases, actually) and makes our lives very easy. There is much more to go into about Bayesian Estimation, and I will do so here in the future. For now, you’ve got a good start on it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Least Squares via the SVD</title><link href="http://localhost:4000/2018/04/11/least-squares-via-the-svd.html" rel="alternate" type="text/html" title="Least Squares via the SVD" /><published>2018-04-11T23:00:00-04:00</published><updated>2018-04-11T23:00:00-04:00</updated><id>http://localhost:4000/2018/04/11/least-squares-via-the-svd</id><content type="html" xml:base="http://localhost:4000/2018/04/11/least-squares-via-the-svd.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Let’s say you have some matrix of features (covariates) \(A \in \mathbb{R}^{m \times n}\) and a response vector \(y \in \mathbb{R}^m\). We want to find the input vector \(x \in \mathbb{R}^n\) that solves this problem or gives us the best possible solution. How should we define best? That is subjective - but the most common idea is to take the real response \(y_i\) and the predicted response \(A[i,:] x\) and square the difference for each response. We can formalize this problem as&lt;/p&gt;

\[\textrm{argmin} _ x || Ax - y ||_2^2\]

&lt;p&gt;where \(\vert\vert . \vert\vert _ 2\) is the \(\ell_2\) norm - where we are just summing the components squared of the input to the norm (in this case, summing predicted minus real squared: the squared error) and taking the square root. We square it just to get rid of the square root - it is the same problem!&lt;/p&gt;

&lt;p&gt;Sometimes there may not an exact solution to this problem - that’s why all we are trying to do is minimize this sum of squares. Obviously, if there is an exact solution, that is going to minimize the squared error because, well, there is no error in that case!&lt;/p&gt;

&lt;p&gt;When would there be no error? Clearly, this must be the case that \(y \in \mathcal{R}(A)\) for there to be no error. That is, the rank of \(A\) must be \(m\) for there to be no error. Does this guarantee the solution is unique? No. For the solution to be unique, \(\mathcal{N}(A)\) must be \(\phi\). That is, the rank of \(A\) must be \(n\). Thus, for there to be a unique solution with zero error, \(A\) must be square. Notice that a unique solution and a solution with zero error are different concepts. You can have a unique solution where the error is non-zero, and you can have lots (infinite, actually!) solutions that are exact. So, now that we have some intuition into the problem, let’s work on solving it. Let’s think about it geometrically! Let’s consider the 3 \(\times\) 3 matrix&lt;/p&gt;

\[\begin{bmatrix}  1 &amp;amp; 0 &amp;amp; 0 \\  0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 \\ \end{bmatrix}\]

&lt;p&gt;The range of this matrix is the x-y plane, obviously, since its columns span \(\mathbb{R}^2\). So what if we want to solve a problem of the form&lt;/p&gt;

\[\textrm{argmin} _ x || Ax - b ||_2^2\]

&lt;p&gt;where \(b \in \mathbb{R}^3\) and is non-zero in the z-component that is closest to the vector \(b\) that exists in the range of our transformation. Check out this figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/projection_svd.jpg&quot; alt=&quot;Projection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The closest point is always the point directly perpendicular (or orthogonal) to the range of \(A\) - think about the triangle inequality. Like, if the end of your vector was a donkey, and the range of your matrix was some food that donkeys like (any ideas?), the donkey would walk straight to the food in a direct path. That’s the intuition here. So, in other words, to find the answer to our question, we just need to project the vector we are trying to reach on to the range of values that we can reach. We need a projection matrix \(P_ {\mathcal{R}(A)}\). So, how do we get that…. first, we need to find an (orthonormal) basis for \(\mathcal{R}(A)\) and then use that basis to build \(P_{\mathcal{R}(A)}\). So….. how do we get that…&lt;/p&gt;

&lt;p&gt;Let’s introduce an easy way to do this - the Singular Value Decomposition. Every matrix, square or not, has an SVD. For positive semi-definite matrices, the SVD is just the same as the Eigendecomposition. (ED). However, unlike the ED, the SVD is defined for rectangular matrices. You can view the ED as kind of a subset of the SVD. Anyways, here’s the definition.&lt;/p&gt;

\[A \in \mathbb{R}^{m \times n} = U\Sigma V^T = \sum_{i =1}^{r} \sigma_{i}u_{i}v_{i}^{T}\]

&lt;p&gt;where \(U \in \mathbb{R}^{m \times m}\) , \(V \in \mathbb{R}^{n \times n}\) and \(\Sigma \in \mathbb{R}^{m \times n}\) and is diagonal. Both \(U\) and \(V\) are orthogonal matrices, where the columns of \(U\) are the eigenvectors of \(AA^T\) and the columns of \(V\) are the eigenvectors of \(A^TA\). To see this, notice both \(AA^T\) and \(A^TA\) are square, symmetric matrices.&lt;/p&gt;

&lt;p&gt;Without loss of generalizability, consider&lt;/p&gt;

\[A^TA = V \Sigma^T U^TU \Sigma V^T = V \Sigma ^T \Sigma V^T = V \Lambda V^T\]

&lt;p&gt;which is the eigendecomposition. So, clearly, the eigenvectors of \(A^TA\) are found in \(V\). Same can be said of \(U\). Another observation we can make here is that \(\Sigma^T\Sigma = \Lambda\), so the elements of \(\Sigma\) , \(\{\sigma_ i\}_{i=1}^{r}\) are the square roots of the eigenvalues of both \(A^TA\) and \(AA^T\). These values are called the singular values of \(A\)!&lt;/p&gt;

&lt;p&gt;So now, we have a tool to solve our least squares problem. How do we use it? If we want to form a basis for \(\mathcal{R}(A)\). Well, \(\mathcal{R}(A)\) is just the span of the columns of \(A\). Extending this notion to the SVD,&lt;/p&gt;

\[Ax = \sum_ {i=1}^{r} \sigma_ i u_ i v_ i^Tx\]

\[= \sum_ {i=1}^{r} \sigma_ i (v_ i^T x) u_i\]

&lt;p&gt;and notice that \(\sigma_ i ( v_ i^Tx)\) is a scalar. This is just a linear combination of the first \(r\)  columns of \(U\) Therefore, letting \(U_ r\) denote the \(m \times r\) matrix with the first \(r\) columns of \(U\), corresponding to the non-zero singular values, we have shown that \(\mathcal{R}(A) = \mathcal{R}(U_ r)\). Since \(U\) is orthogonal, \(U_ r\) is an orthonormal basis for \(\mathcal{R}(A)\). Using the fact that, if \(B\) is is an orthonormal basis for \(\mathcal{V}\), then \(BB^T = P_ {\mathcal{V}}\), we have that \(U_ rU_ r^T = P_ {\mathcal{R}(A)}\). So, in terms of our problem, the point closest to \(b\) that is in  \(\mathcal{R}(A)\) is \(U_rU_r^Tb\). We are now super close to finding \(\hat{x}\) that minimizes the error in the \(\ell -2\) sense. To get there, we need to define one more thing - the &lt;strong&gt;Monroe-Penrose Pseudo Inverse&lt;/strong&gt;, \(A^+\).&lt;/p&gt;

&lt;p&gt;Definition:&lt;/p&gt;

\[A^+ = V \Sigma ^+ U^T\]

&lt;p&gt;where \(\Sigma^+ \in \mathbb{R}^{n \times m}\) = diag\((\frac{1}{\sigma_ 1},\dots,\frac{1}{\sigma_r},0,\dots,0)\). Using this definition,&lt;/p&gt;

\[AA^+ = U\Sigma V^T V \Sigma ^+ U^T \]

\[= U \Sigma \Sigma^+ U^T\]

\[= U_rU_r^T\]

&lt;p&gt;Therefore, since we know the closest possible point we can reach with \(A\) is \(U_ r U_ r^Tb\), then&lt;/p&gt;

\[U_ r U_ r^Tb = A(A^+b )\]

&lt;p&gt;and it follows immediately that&lt;/p&gt;

\[\hat{x} = A^+b\]

&lt;p&gt;for any matrix \(A \in \mathbb{R}^{m \times n}\). Therefore, we have shown there is a general solution to the least-squares problem&lt;/p&gt;

\[\textrm{argmin} _ x ||Ax - b||_2^2\]

&lt;p&gt;An interesting thing to note here:&lt;/p&gt;

\[|| A\hat{x} - b ||_2^2\]

\[= || b - A\hat{x} ||_2^2\]

\[= || b - AA^+b ||_2^2\]

\[= || (I - AA^+)b ||_2^2\]

&lt;p&gt;Since \(U = [U_ r U_ o]\) where \(U_ o\) is the \(m \times n-r\) matrix with columns corresponding the the zero-valued singular values of \(A\). Therefore,&lt;/p&gt;

\[UU^T = I = [U_ r U_ o] [U_ r U_ o]^T = U_ rU_ r^T + U_ oU_ o^T\]

&lt;p&gt;\(\implies U_ oU_ o^T = I - AA^+ = I - P_ { \mathcal{R}(A)} = P_ {\mathcal{R}^{\perp}(A)}\).&lt;/p&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

\[= || (I - AA^+)b ||_2^2\]

\[= || P_ {\mathcal{R}^{\perp}(A)}b ||_2^2\]

&lt;p&gt;That is, what we are doing is projecting the error onto the the orthogonal complement of \(P_ {\mathcal{R}(A)}\). This is exactly what the figure above is showing! Our error should be directly orthogonal to \(P_{\mathcal{R}(A)}\) for the objective function to be minimized.&lt;/p&gt;

&lt;p&gt;Now we have a bit of intuition into how we can use the Singular Value Decomposition to solve the least squares problem. This helps when you have a feature matrix and outcomes and want to find the specific weights associated with each covariate in the feature matrix. Least squares is used to solve for the parameters in linear regression and can be used to find the weights associated with neural networks… it has lots of applications. Hopefully, now you can apply it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Linear Regression</title><link href="http://localhost:4000/2018/03/10/linear-regression.html" rel="alternate" type="text/html" title="Linear Regression" /><published>2018-03-10T22:00:00-05:00</published><updated>2018-03-10T22:00:00-05:00</updated><id>http://localhost:4000/2018/03/10/linear-regression</id><content type="html" xml:base="http://localhost:4000/2018/03/10/linear-regression.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Linear Regression… the bread and butter of applied statistics.&lt;/p&gt;

&lt;p&gt;At one point or another, you have encountered linear regression. Did you read a statistic in the paper this morning? Was that statistic not a percent? Does anybody read the paper anymore?… Anyways, that number was probably derived via running a regression.&lt;/p&gt;

&lt;p&gt;A linear regression is used to describe an association (NOT A CAUSAL THING) between one (continuous) response variable, \(Y\) and a vector of (continuous or not, whatever) explanatory variables \(\mathbf{X}\). Why is it called linear regression? Well, remember from algebra that a line can be described by&lt;/p&gt;

\[y = mx + b\]

&lt;p&gt;where \(x\) is the independent (explanatory) variable and \(y\) is the dependent (response) variable. We assume there is a number \(m\), the slope, that describes the relationship here. Essentially, we are saying the same thing with linear regression. We want to find the best \(\beta_ 0\) and \(\beta_1\) such that the formula&lt;/p&gt;

\[Y_i = \beta_0 + \beta_1*X_i + \epsilon_i\]

&lt;p&gt;is the best fit given our data. It doesn’t matter if \(X\) is linear, quadratic, exponential.. only that the relationship is linear in the \(\beta\) parameters. \(\epsilon\) is an error term for every \(i\)  observation. So the problem is really trying to find \(\beta_0\) and \(\beta_1\) such that \(\epsilon\) is minimum overall, i.e. taking into account each observation. The math behind linear regression is all about how we best do that. For this post, we will only explore simple linear regression where we have one explanatory variable. This need not need be the case, and we will cover that another time - but it involves some linear algebra and is a little more involved. The rest of this post assumes some knowledge of statistics, just at a basic level like expected values and distributions.&lt;/p&gt;

&lt;p&gt;So, we have this model&lt;/p&gt;

\[Y_i = \beta_0 + \beta_1*X_i + \epsilon_i\]

&lt;p&gt;where \(Y_i\) is the response variable, \(\beta_0\) is the intercept (fixed), \(\beta_1\) is the slope (fixed), \(X_i\) is a covariate (fixed) and \(\epsilon\) is the error term, which is random and unobservable. As always, we need some assumptions in order to make this workable.. We assume that the errors are normally distributed and unrelated… that is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
\[\epsilon_i \sim N(0,\sigma^2)\]
  &lt;/li&gt;
  &lt;li&gt;
\[E[\epsilon_i\epsilon_j] = 0\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These assumptions lead us to some key implicit assumptions of linear regression. &lt;em&gt;Linearity&lt;/em&gt;,  &lt;em&gt;constant variance&lt;/em&gt;, &lt;em&gt;independence&lt;/em&gt; and &lt;em&gt;normally distributed&lt;/em&gt;. That is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
\[E[Y_i \vert X_i ] = \beta_0 + \beta_1X_i\]
  &lt;/li&gt;
  &lt;li&gt;
\[\sigma_i^2 =  V[Y_i \vert X_i] = \sigma^2\]
  &lt;/li&gt;
  &lt;li&gt;
\[Y_i \perp Y_j, i \neq j\]
  &lt;/li&gt;
  &lt;li&gt;
\[Y_i \sim N(\beta_0 + \beta_1X_i,\sigma^2)\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We treat the covariates as fixed, but sometimes they aren’t.  For instance, there could be measurement error in the covariates. Really, the goal is to keep that randomness small.&lt;/p&gt;

&lt;p&gt;Let’s talk about \(\beta_0\) and \(\beta_1\). How should we interpret them? Well, they are part of a linear relationship. We are  going to estimate them such that we fit a line to the data that should give us the expected value of the response given some input parameters.  That is,&lt;/p&gt;

\[E[ Y_i | X_i ] = \hat{\beta_0} + \hat{\beta_1}X_i\]

&lt;p&gt;The little hat things mean they are estimated, not the true parameters… more on that later. So, \(\hat{\beta_0}\) is the average value of \(Y\)  when \(X = 0\). Furthermore, \(\hat{\beta_1}\) is the average change in \(Y\) for a unit increase in \(X\). In general, it is best to not extrapolate beyond what your limits for \(X\) are. If your covariate is age, and you only have observations for people aged 15 - 30, don’t make the assumption that the model works for a 90-year-old… please.&lt;/p&gt;

&lt;p&gt;Okay! Now to the fun stuff… math. We wanna estimate the parameters here. How do we do that? Calculus. :)&lt;/p&gt;

&lt;p&gt;First, we need to define best. What is the &lt;em&gt;best&lt;/em&gt; fit? That’s arbitrary, as there are many ways to do this. We are going to do it via ordinary least squares, or OLS, which is the default way that any linear regression is fit. What this means is that we want to minimize the &lt;strong&gt;Sum of Squared Errors (SSE)&lt;/strong&gt;, or, in math,&lt;/p&gt;

\[\sum_{i=1}^{n} \hat{\epsilon_i}^2\]

&lt;p&gt;So, we need a definition of \(\hat{\epsilon_i}\), Well, it is just \(Y_i - \hat{Y_i}\) or, even better, \(Y_i - (\hat{\beta_0} + \hat{\beta_1}X_i)\) , since that is how we are guessing our reponse variable. This is good because now we have written \(\hat{\epsilon_i}\) as a function of \(\hat{\beta_0}\) and \(\hat{\beta_1}\), which are our tuneable parameters. Now we will need a little bit of calculus. This function will reach a critical value when&lt;/p&gt;

\[\textbf{\(*\)} \frac{\partial SSE}{\partial \beta_0} = 0\]

&lt;p&gt;and&lt;/p&gt;

\[\textbf{\(**\)} \frac{\partial SSE}{\partial \beta_1} = 0\]

&lt;p&gt;Remember? ;)&lt;/p&gt;

&lt;p&gt;Let’s solve (*) first.&lt;/p&gt;

\[\frac{\partial SSE}{\partial \beta_0} = 0\]

\[\implies -2 \sum_{i=1}^{n}(Y_i - \hat{\beta_0} -\hat{\beta_1}X_i) = 0\]

\[\implies \sum_{i=1}^{n}Y_i  - \hat{\beta_1}\sum_{i=1}^{n}X_i= n\hat{\beta_0}\]

\[\implies \hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}\]

&lt;p&gt;So, if we know \(\hat{\beta_1}\), we can estimate \(\hat{\beta_0}\) . Now, looking at (**) and subbing in this result,&lt;/p&gt;

\[\frac{\partial SSE}{\partial \beta_1} = 0\]

\[\implies -2 \sum_{i=1}^{n}X_i (Y_i - \hat{\beta_0} -\hat{\beta_1}X_i) = 0\]

\[\implies -2 \sum_{i=1}^{n}X_i (Y_i -\bar{Y} +\hat{\beta_1}\bar{X} -\hat{\beta_1}X_i) = 0\]

\[\implies \sum_{i=1}^{n}X_i(Y_i - \bar{Y}) = \hat{\beta_1} \sum_{i=1}^{n}X_i(X_i - \bar{X})\]

\[\implies \hat{\beta_1} = \frac{SSXY}{SSX}\]

&lt;p&gt;where \(SSXY = \sum_{i=1}^{n}X_i(Y_i - \bar{Y})\) and \(SSX =  \sum_{i=1}^{n}X_i(X_i - \bar{X})\)&lt;/p&gt;

&lt;p&gt;Awesome! Now, given a vector of covariates and a vector of responses, you can estimate both paramaters quite easily! We didn’t really need to do this though. It is good for understanding… but we didn’t need to do the calculus, we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; to do this!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>