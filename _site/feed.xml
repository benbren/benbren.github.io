<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-01-08T18:10:52-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ben Brennan</title><subtitle>Hi, I'm Ben. This is my website/blog. Views are my own. Thanks for looking.</subtitle><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/2019/12/31/gradient-descent.html" rel="alternate" type="text/html" title="Gradient Descent" /><published>2019-12-31T00:21:32-05:00</published><updated>2019-12-31T00:21:32-05:00</updated><id>http://localhost:4000/2019/12/31/gradient-descent</id><content type="html" xml:base="http://localhost:4000/2019/12/31/gradient-descent.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;You’ll remember in OLS, we had something called the &lt;strong&gt;normal equations&lt;/strong&gt; - a nice, succinct, simple formula for calculating out best-fit parameters. You’ll also remember, then, that we had to invert an n by n matrix, &lt;script type=&quot;math/tex&quot;&gt;X^TX&lt;/script&gt; to get these parameters.. which, if n is large, is computationally very expensive. Today we’ll discuss gradient descent, a less computationally expensive way (for large n) to obtain parameters that minimize our squared error.&lt;/p&gt;

&lt;p&gt;It’s important to first note that gradient descent is not limited to least squares. Gradient descent is just an algorithm that can be used to maximize (or minimize) any convex objective function by calculating the gradient at a point, taking a step in that direction, calculation the gradient again, taking a step in that direction… so on and so forth, until the gradient is basically norm 0. It just so happens that the least squares function is convex, so gradient descent converges to the minimum.&lt;/p&gt;

&lt;p&gt;This algorithm won’t work for non-convex functions as it may only search out a local minimum, not the global minimum. If you’re standing at the base of two hills, the gradient is just going to take you up the steepest hill, not necessarily the tallest one, right? Okay.. anyways, do we remember what the gradient is?&lt;/p&gt;

&lt;p&gt;Let’s say we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\beta_0, \beta_1, \dots, \beta_q): \mathbb{R}^{q+1} \rightarrow \mathbb{R}&lt;/script&gt;

&lt;p&gt;then, the gradient is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \frac{\partial J}{\partial \beta_0} \hat{e_1} + \dots + \frac{\partial J}{\partial \beta_q} \hat{e_{q+1}} = \begin{bmatrix} \frac{\partial J}{\partial \beta_0} \\ \vdots \\ \frac{\partial J}{\partial \beta_q} \end{bmatrix}&lt;/script&gt;

&lt;p&gt;so, essentially, each component of the gradient tells you how fast your function changes with respect to the standard basis in each direction. We can find the directional derivative of any unit vector &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\nabla J \cdot \vec{v}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J \cdot \vec{v} = \| \nabla J\|\|\vec{v}\|\cos(\theta) =\| \nabla J\|\cos(\theta)&lt;/script&gt;

&lt;p&gt;since &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; is a unit vector, where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the angle between the two vectors. &lt;script type=&quot;math/tex&quot;&gt;\cos(\theta)&lt;/script&gt; is max when $\theta = 0$ - that is, when &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; is in the direction of the gradient! So the steepest ascent is in the direction of the gradient, and the steepest descent is in the opposite direction. We should note that the steepest ascent here is limited to unit vectors, so the steepest ascent is really in the direction of &lt;script type=&quot;math/tex&quot;&gt;\frac{\nabla J}{\| \nabla J\|}&lt;/script&gt;, right? But that’s the same direction as the gradient. And we are going to kinda normalize the gradient in our own way later… you will see.&lt;/p&gt;

&lt;p&gt;You can think of the gradient as kind of a trade-off. Say the gradient is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;3,1,6&gt; %]]&gt;&lt;/script&gt;. You’re confined to a circle in terms of where you can move with the gradient (or any vector) starting from a point. That is, there’s a trade-off between directions. You can essentially trade 3 steps in the Y direction for one step in the X direction. So an optimal direction does just that, hence why the optimal direction is the gradient… it takes one 3 steps in the X direction for each step in the Y direction. Any other decision is ‘unfair’, i.e there is a better trade-off that maximizes how steep the gradient could be.&lt;/p&gt;

&lt;p&gt;Anyways. The gradient heads in the direction of the steepest ascent, so an algorithm searching out the lowest point of a function should head in the opposite of the gradient. So let’s consider least squares, where our squared error loss term is a function of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\beta}&lt;/script&gt;, the vector of parameters. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\beta_0, \dots, \beta_q) = \frac{1}{2n} \sum_{i=1}^n (\hat{y_i} - y_i)^2  =\frac{1}{2n} \sum_{i=1}^n ( \beta_0 + \beta_1x_{i1} + \dots + \beta_qx_{iq} - y_i )^2&lt;/script&gt;

&lt;p&gt;where we just normalized the squared error function. Remember, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is the number of features, and &lt;script type=&quot;math/tex&quot;&gt;q+1&lt;/script&gt; is the number of parameters we must estimate. Now, using a bit of calculus, we see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J} {\partial \beta_0} = \frac{1}{n} \sum_{i=1}^n ( \hat{y_i} - y_i)&lt;/script&gt;

&lt;p&gt;and, for &lt;script type=&quot;math/tex&quot;&gt;j = 1,\dots, q&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J} {\partial \beta_j} = \frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)x_{ij}&lt;/script&gt;

&lt;p&gt;so, a gradient descent algorithm would look something like&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;is the norm of the gradient greater than our threshold?
    &lt;ul&gt;
      &lt;li&gt;if yes, move along the gradient for each &lt;script type=&quot;math/tex&quot;&gt;\beta_J&lt;/script&gt; by some &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and check again&lt;/li&gt;
      &lt;li&gt;if no, return the current point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is a stepsize of your choosing. It is important to note that the choice of the step-size is important. A step-size that is too big will cause the algorithm to jump over the minimum value and maybe even diverge, whereas a step-size that is too small will mean the algorithm takes forever to converge to the minimum value. In one dimension, we can see what is happening below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/lambda_tut.jpg&quot; alt=&quot;Different stepsizes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the step size is small (left), it is converging to the minimum value… but it is gonna take forever (maybe even actually. . .) and on the right, where the step size is too large, the minimum is being overshot and then the larger slope (gradient) is compounding with a large step size to overshoot by even more and so on and so forth as your algorithm sadly diverges to infinity ….. :(&lt;/p&gt;

&lt;p&gt;You can imagine this type of computation is best done in a while-loop. Below is an interpretation of this algorithm in Julia:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;&lt;span class=&quot;n&quot;&gt;gradient_descent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;α&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;#for i in 1:q_plus_one&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#    gradient[i] = α*(error'*X[:,i])&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#end&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first part of this just says ‘okay, did you add an intercept? maybe not.. because you might not think to add the column of ones… we are gonna assume you want to do this so we will do it for you if you did not’. Obviously, this isn’t necessary, and could actually lead to incorrect results if the column  missing isn’t the one of all ones, but it is just to show you that coding is YOUR life, do with it what you will… Anyways. We compute the gradient via the dot product, as you will notice that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_i} - y_i) = e^T \cdot \mathbf{1_n}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_i} - y_i)x_{ij} = e^T  \cdot \mathbf{X}[:,j]&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; is the n-dimensional vector of errors and $latex X$ is the feature matrix, with a row of ones so that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_1} - y_i) = e^T \cdot \mathbf{X}[:,1] = e^T \cdot \mathbf{1_n}&lt;/script&gt;

&lt;p&gt;We also add an iteration count (just in case our step size is too small or too large or we totally messed up the code) to stop the loop at a certain point. However, this way of thinking about the algorithm is commented out because it is not efficient. Generally, for-loops should (and usually can) be replaced by matrix multiplication, which is in the code above! Let’s go ahead and see if we can see how you would write that ‘in math’…..&lt;/p&gt;

&lt;p&gt;You can see in the code that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \mathbf{X^T(X\beta - Y)}&lt;/script&gt;

&lt;p&gt;which is actually just exactly what we wrote above. You’re multiplying the errors by the columns of the feature matrix to get the components of the gradient, so why not just transpose your matrix and use matrix multiplication? Thats what this is saying. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\beta^{(new)}} = \mathbf{\beta^{(old)}} - \alpha \frac{1}{n} \mathbf{X^T( X\beta - Y)}&lt;/script&gt;

&lt;p&gt;Perfect. So now we know what it is, how to use it.. but when would you use it? Usually only when you have really large values of n (lots of observations, like millions…) . Essentially, the only reason you use it is time. There are other things you can do here, like use only one observation (which is called Least Mean Squares) which saves even more time but isn’t as great, obviously. There are other issues with gradient descent, like correctly choosing the step-size that, in practice, you really have no way around. There are plenty of ways to choose a step-size, but no set-rule - you’ll have to figure it out on your own (and that takes time, too!). What’s best about gradient descent is that you can use it for a lot of things. Least squares is just one. You can use it for plenty of other things… really, you can use it for whatever problem you have where you need to optimize a function. But I hope learning about it via least squares was helpful!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Introduction to Survival Analysis</title><link href="http://localhost:4000/2019/10/21/introduction-to-survival-analysis.html" rel="alternate" type="text/html" title="Introduction to Survival Analysis" /><published>2019-10-21T23:00:00-04:00</published><updated>2019-10-21T23:00:00-04:00</updated><id>http://localhost:4000/2019/10/21/introduction-to-survival-analysis</id><content type="html" xml:base="http://localhost:4000/2019/10/21/introduction-to-survival-analysis.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Survival Analysis: the branch of statistics that pretty much everyone I know only associates with biostatistics. I get it - that makes sense. Sure, survival implies.. surviving, but lots of things survive. My attempt to be a vegetarian (still going) is something that ‘survives’. Your T.V., your willingness to stay on an email list. Things like that. Survival analysis takes on many different names (for instance, reliability in engineering). Essentially, it deals with data that has ‘failure’ times and ‘censoring’ times - so it takes into account actual and observational failure times. For example, a failure time could be death (actual failure) and a censoring time could be a patient lost to follow-up (observational failure). It’s a cool subject - one that is applicable to almost every field, so let’s dive into some of the basics. The first : &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. Big one. The time to event, which is a random variable. The second: &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;. Also a big one, the time to censoring. And finally, the first and second combined into the third(?): &lt;script type=&quot;math/tex&quot;&gt;X = min(T,C)&lt;/script&gt; which is the event we actually observe. Ok, now actually finally, we have &lt;script type=&quot;math/tex&quot;&gt;\Delta = I(T \leq C) = I (X = T)&lt;/script&gt;, which is just 1 if we observe the event and 0 if we lose the observation to censoring. Those are the big variables for the introduction, so let’s get into talking about the main functions and some examples.&lt;/p&gt;

&lt;p&gt;There are 3 main functions in survival analysis, and all can be derived from one another. We have the survival function, the hazard function (or hazard rate, or just hazard) and the cumulative hazard function. Starting with the survival function:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S(t) = P(T &gt;t)&lt;/script&gt; if t is continuos and &lt;script type=&quot;math/tex&quot;&gt;S(t _ j)= P(T &gt; t _ j) = P(T \geq t_{j-1} )&lt;/script&gt; when t is discrete. How do we define this? We need to define a density first.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_ T(t) = \lim_{\delta\to 0^+}\frac{1}{\delta}P(t  \leq T \leq t + \delta)&lt;/script&gt;

&lt;p&gt;when  &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous and, simply,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_T(t) = P(T = t)&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is discrete. Now, to define our survival function should be relatively easy.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S(t) = P(T  &gt; t) = \displaystyle\int_ {t}^{\infty}f_T(u)du&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S(t) = P(T &gt;t) = \displaystyle\sum_ {n &gt; t}f_T(t)&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is discrete.&lt;/p&gt;

&lt;p&gt;A few things to note about a survival function. &lt;script type=&quot;math/tex&quot;&gt;S(0) = 1&lt;/script&gt; as we should not have events happening at time 0. &lt;script type=&quot;math/tex&quot;&gt;\lim_ {t\to \infty}S(t) = 0&lt;/script&gt; as we expect everyone to die or have an event as time goes on. Also, the survival function should be monotone decreasing in &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. It is important to note that, in the discrete case, the survival function is right continuous. That is, it has jumps (down) at the values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; where events happen, and is evaluated and the lower probability of survival for those time periods. More formally &lt;script type=&quot;math/tex&quot;&gt;S(t) = \lim_{x\to t^+}S(x) = S(t^+)&lt;/script&gt; This is important when we are estimating… the hazard function! The hazard function is the probability that a person dies at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; GIVEN that they have survived until time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;\lambda (t)&lt;/script&gt; denote the survival time.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \lim_{\delta \to 0^+}\frac{1}{\delta}P(t \leq T \leq t + \delta  \vert T &gt; t)&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = P(T = t \vert T \geq t)&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is discrete. In general, from the basic probability theory,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T =t \vert T \geq t) = \frac{P(T = t, T \geq t)}{P(T \geq t)} = \frac{P(T = t)}{P(T \geq t)} = \frac{f_T(t)}{S(t^-)}&lt;/script&gt;

&lt;p&gt;Notice that the hazard function, when continuous, is a rate and, when discrete, is a probability. Finally, let us define &lt;script type=&quot;math/tex&quot;&gt;\Lambda(t)&lt;/script&gt; as the cumulative hazard function. It is pretty much exactly what you would think it was…&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = \displaystyle\int_{0}^{t}\lambda(u)du&lt;/script&gt;

&lt;p&gt;when continuous and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = \displaystyle\sum_{t_i \leq t} \lambda(t_i)&lt;/script&gt;

&lt;p&gt;when discrete. Let’s take a look at an interesting problem involving the cumulative hazard function. Say &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; follows some distribution with cdf &lt;script type=&quot;math/tex&quot;&gt;F_t(t)&lt;/script&gt;. Then its hazard function follows an exponential(1) distribution. To see this&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Lambda(T) \leq t) = P( - \Lambda(T)\geq -t ) = P(e^{-\Lambda(T)} \geq e^{-t}) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(-e^{-\Lambda(T)} \leq -e^{-t}) = P(1 - e^{-\Lambda(T)} \leq 1 - e^{-t}) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(1-S(T) \leq 1- e^{-t}) = P(F_ T(T) \leq 1- e^{-t}) = P(T \leq F_ T^{-1}(1- e^{-t})) = F_ T(F_T^{-1}(1- e^{-t})) = 1- e^{-t}&lt;/script&gt;

&lt;p&gt;which is the cdf of the exponential(1) distribution! Cool, huh? Anyways.&lt;/p&gt;

&lt;p&gt;It is clear what the link between the hazard function and the cumulative hazard function is, but now let us find the explicit link with the survival function. First (and pretty much only..), remember&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \frac{f(t)}{S(t^-)}&lt;/script&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous, this is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \frac{f(t)}{S(t^-)} = \frac{f(t)}{S(t)} = \frac{-d\log S(t)}{dt}&lt;/script&gt;

&lt;p&gt;which means&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = -\log(S(t)&lt;/script&gt;

&lt;p&gt;and, equivalently,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = e^{-\Lambda(t)}&lt;/script&gt;

&lt;p&gt;Nice! Easy. Let’s do an example. Suppose &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is such that its survival function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = \frac{64}{(t+8)^2}&lt;/script&gt;

&lt;p&gt;This is continuous. First, a fact:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[T] = \displaystyle\int_{0}^{\infty} S(t)dt&lt;/script&gt;

&lt;p&gt;How to prove?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[T] = \displaystyle\int_{0}^{\infty} tf(t)dt = \displaystyle\int_{0}^{\infty}\displaystyle\int_{0}^{t}ds f(t)dt = \displaystyle\int_{0}^{\infty}\displaystyle\int_{s}^{\infty}f(t)dtds = \displaystyle\int_{0}^{\infty}1 - F(s) ds  = \displaystyle\int_{0}^{\infty}S(s)ds&lt;/script&gt;

&lt;p&gt;So, what is our expected survival time in the above case?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[T] = 64 \displaystyle\int_{0}^{\infty} (t + 8)^{-2} = 64/8 = 8&lt;/script&gt;

&lt;p&gt;Median survival time? Clearly, that is just where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = 0.5&lt;/script&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{64}{(t_m +8)^2} = 0.5 \implies \sqrt(128) - 8 = t_m = 3.31&lt;/script&gt;

&lt;p&gt;Now, let’s estimate the hazard function (and the cumulative hazard).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = -\log(S(t)) = 2 \log(t+8) - \log(64)&lt;/script&gt;

&lt;p&gt;Now, we just take the derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to find &lt;script type=&quot;math/tex&quot;&gt;\lambda(t)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \frac{2}{t+8}&lt;/script&gt;

&lt;p&gt;The discrete case is a liiiiiiitle bit more difficult, but very important for continued study in survival analysis. In practice, unless we have literall infinite observations, we will be dealing with discrete data. Let’s say &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; takes on values &lt;script type=&quot;math/tex&quot;&gt;t_ 1, \dots , t_ n&lt;/script&gt;. Also, remember (more basic probability theory) that &lt;script type=&quot;math/tex&quot;&gt;P(AB) = P(A\vert B)P(B)&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is many events, then this formula is recursive. Now, suppose &lt;script type=&quot;math/tex&quot;&gt;t_j \leq t \leq t_{j+1}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = P(T &gt; t) = P(T &gt; t_j, T &gt; T_{j-1},\dots,T &gt; t_1) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T &gt; t_j \vert T &gt; t_{j-1})\times P(T &gt; t_{j-1} \vert T &gt; t_{j-1})\times \dots \times P(T &gt; t_1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1 - P(T \leq t_j \vert T &gt; t_{j-1})]\times \dots \times [1 - P(T\leq t_1)]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1 - P(T = t_j \vert T \geq t_{j})]\times \dots \times [1 - P(T = t_1)]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \displaystyle\prod_{t_i\leq t} [1 - \lambda(t_i)]&lt;/script&gt;

&lt;p&gt;Notice, this formula is recursive! &lt;script type=&quot;math/tex&quot;&gt;S(t_j) = [1 - \lambda(t_j)]S(t_{j-1})&lt;/script&gt; which doesn’t really matter here but .. it makes it easier to compute if, say, you wanted to write up this algorithm to help yourself learn (maybe?).  This actually has an analogous case in the continuous case (if we remember the Taylor Expansion for &lt;script type=&quot;math/tex&quot;&gt;e^{-x}&lt;/script&gt; LOL). To the rescue again..&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{-x} = \displaystyle\sum_{n = 0}^{\infty} \frac{(-x)^n}{n!} \approx 1- x&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is small.&lt;/p&gt;

&lt;p&gt;So, since 
&lt;script type=&quot;math/tex&quot;&gt;\Lambda(T) = \int_{0}^{t}\lambda(u)du&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = e^{-\Lambda(t)}&lt;/script&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = \displaystyle\prod_{u=0}^t e^{-\lambda(u)du} \approx \displaystyle\prod_{u=0}^t[1 - \lambda(u)du]&lt;/script&gt;

&lt;p&gt;which is sometimes referred to as the product limit form of the survival function. Anyways.. let’s get to some real examples. First, from data.. let’s think of a good way to estimate &lt;script type=&quot;math/tex&quot;&gt;\lambda(t)&lt;/script&gt;. Seems natural to estimate the hazard at a certain time point by looking at the number of events at a time point relative to the number of people at risk for the event at that time point. So, letting &lt;script type=&quot;math/tex&quot;&gt;D_ J&lt;/script&gt; denote the number of events at time &lt;script type=&quot;math/tex&quot;&gt;t_ j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y _ j&lt;/script&gt; denote the number at risk (sill being observed, including the people who experience events) for an event at &lt;script type=&quot;math/tex&quot;&gt;t_j&lt;/script&gt;. Then a natural estimate for &lt;script type=&quot;math/tex&quot;&gt;\lambda(t_j)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\frac{D_j}{Y_j}&lt;/script&gt;. Looking at some data (+ indicates a censored observation)…&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2,5^+,8,12^+,15,21^+,25,29,30^+,34&lt;/script&gt;

&lt;p&gt;Let’s estimate &lt;script type=&quot;math/tex&quot;&gt;S(10)&lt;/script&gt;. Notice, the first term of this estimation will be &lt;script type=&quot;math/tex&quot;&gt;P(T &gt; 10 \vert T &gt; 8)&lt;/script&gt; which is 1, since nothing happens at 10.. so if someone survives past 8, they will survive past 10. For sure. With probability 1. :) SO,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(10) = S(8) =  \displaystyle\prod_{t \in \{2,5,8\}}[1 - \lambda(t)]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1- \frac{D_8}{Y_8}][1- \frac{D_5}{Y_5}][1- \frac{D_2}{Y_2}]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1- \frac{1}{8}][1- \frac{0}{9}][1- \frac{1}{10}] = \frac{63}{80}&lt;/script&gt;

&lt;p&gt;Notice at 5, the observation was censored, so it does not get included in &lt;script type=&quot;math/tex&quot;&gt;D_5&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Note that this estimate of our survival function will be piece-wise continuous, right continuous with jumps at event times, and will be 0 IF our last observation is a failure. Why can we do this when we have censored data? Because we assume that &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is independent of &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, or else we would have a joint distribution for the hazard. Anyways… this type of estimation is what is referred to as the Kaplan-Meier estimate of the survival function which, along with Nelson-Aalen, is the most popular non-parametric (or just in general) way to estimate the survival function.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Bayesian Estimation</title><link href="http://localhost:4000/2018/07/13/bayesian-estimation.html" rel="alternate" type="text/html" title="Bayesian Estimation" /><published>2018-07-13T01:21:32-04:00</published><updated>2018-07-13T01:21:32-04:00</updated><id>http://localhost:4000/2018/07/13/bayesian-estimation</id><content type="html" xml:base="http://localhost:4000/2018/07/13/bayesian-estimation.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;When most people think about statistics, they think about flipping a coin or gambling or having flashbacks to dropping out of high school because they hated the class so much… moral of the story is, it’s just one thing to think about. What if I told you that that wasn’t necessarily true? I am about to tell you that. Statistics is the science of modeling randomness. But what is random, and what is not? That is a fundamental question in statistics and splits the subject into two separate approaches - Bayesian and Frequentist.&lt;/p&gt;

&lt;p&gt;What does it mean to say that I am 50% certain it will rain tomorrow? That is not so much a statement of probability, but rather of uncertainty. It will rain or it will not rain, but that outcome is something you are uncertain of. Consider another example: flip a coin and cover it up immediately once it lands, so you don’t know what the result is. What’re the chances that the coin is heads? From a traditional, frequentist perspective, there is no answer to this question. It already happened. There is already a result - you’re just ignorant. But that’s not really how we think, is it? We don’t know what the result is - we are uncertain of it. Let’s consider a medical example. Say you’re getting screened for the flu, and the result comes out positive. Do you actually have the flu? In the frequentist approach, that doesn’t make sense to ask. We do have sensitivity and specificity. Let &lt;script type=&quot;math/tex&quot;&gt;\theta = 1&lt;/script&gt; if you have the flu and &lt;script type=&quot;math/tex&quot;&gt;\theta = 0&lt;/script&gt; if you don’t. Then the sensitivity is the probability the test is positive given that you have the flue, or &lt;script type=&quot;math/tex&quot;&gt;P(T = 1 \vert \theta = 1)&lt;/script&gt; and the specificity is &lt;script type=&quot;math/tex&quot;&gt;P(T = 0 \vert \theta = 0)&lt;/script&gt;, the probability the test is negative when you don’t have the flu. If specificity is not 1, you might get tested positive but not have the disease. It makes sense then to ask ‘“Do I actually have the flu?”, but does it make statistical sense to ask ‘Do I have the flu?’? That is, what is &lt;script type=&quot;math/tex&quot;&gt;P(\theta =1 \vert T = 1)&lt;/script&gt;? A frequentist says ‘Bad question. &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is fixed’. A Bayesian will come in and say ‘Hold on, &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is not actually fixed. We can estimate this’&lt;/p&gt;

&lt;p&gt;This is the whole premise of Bayesian statistics - to use probability theory to not &lt;em&gt;only&lt;/em&gt; quantify probability but also to quantify uncertainty. Much better!! Solving problems that were disregarded earlier - we like that!&lt;/p&gt;

&lt;p&gt;Okay, moving on… now that I’ve converted you to the Bayesian approach to statistics…&lt;/p&gt;

&lt;p&gt;:)&lt;/p&gt;

&lt;p&gt;Let’s talk about the setup. In a frequentist approach, we have a random variable &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;, observed data from that random variable, a model, and a parameter &lt;script type=&quot;math/tex&quot;&gt;\theta \in \Omega&lt;/script&gt; that is unknown but &lt;em&gt;fixed&lt;/em&gt;. In the Bayesian framework, we don’t consider that italicized part. It’s a subtle change that makes a huge difference. Now, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \sim \pi(\theta)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is called a &lt;em&gt;prior distribution&lt;/em&gt;. Now, we need to make an adjustment to your original data’s distribution. Now, we have a conditional distribution!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{X} \vert \theta \sim f(\mathbf{x}\vert \theta)&lt;/script&gt;

&lt;p&gt;and, by Bayes rule, the joint distribution of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x},\theta) = \pi(\theta)f(\mathbf{x}|\theta)&lt;/script&gt;

&lt;p&gt;and, thus, the marginal distribution of  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m(\mathbf{x}) = \displaystyle\int_{\theta \in \Omega} {f(\mathbf{x},\theta)d\theta}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\theta \vert \mathbf{x}) = \frac{\pi(\theta)f(\mathbf{x} \vert \theta)}{m(\mathbf{x}) }&lt;/script&gt;

&lt;p&gt;We now have all the tools we need to do some basic Bayesian analysis!&lt;/p&gt;

&lt;p&gt;and, so, by Bayes rule again, the posterior distribution of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is&lt;/p&gt;

&lt;p&gt;Let us look at &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim Bernoulli(p)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;p \sim Beta(\alpha, \beta)&lt;/script&gt;. Then we could calculate &lt;script type=&quot;math/tex&quot;&gt;\pi(\theta \vert \mathbf{x}) = \frac{\pi(p)f(\mathbf{x}\vert \theta)}{m(\mathbf{x}) }&lt;/script&gt;, but that would take forever. Does &lt;script type=&quot;math/tex&quot;&gt;m(\mathbf{x})&lt;/script&gt; matter? No. Why? Think about it. We want the posterior distribtion of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, right? So &lt;script type=&quot;math/tex&quot;&gt;m(\mathbf{x})&lt;/script&gt; has &lt;em&gt;literally&lt;/em&gt; (millenials…) nothing to do with &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, therefore it just acts as a normalizing constant in the equation to make sure that &lt;script type=&quot;math/tex&quot;&gt;\pi(p \vert \mathbf{x})&lt;/script&gt; is a true pdf. Therefore, we can just look at the numerator of the expression and recognize the kernel of the distribution to understand what family the posterior distribution comes from. Soo, in math,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(p \vert \mathbf{x}) \propto \pi(p)f(\mathbf{x} \vert p)&lt;/script&gt;

&lt;p&gt;We know&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x}\vert p) = \displaystyle \prod_{i=1}^{n} f(\mathbf{x_i}\vert p) = \displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i})&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}&lt;/script&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(p \vert \mathbf{x}) \propto\displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i}) \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= p^{\sum_{i=1}^{n}x_i}(1-p)^{n - \sum_{i=1}^{n}x_i}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}&lt;/script&gt;

&lt;p&gt;which is the same as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha + \sum_{i=1}^{n}x_i -1} (1 - p)^{\beta +n - \sum_{i=1}^{n}x_i - 1}&lt;/script&gt;

&lt;p&gt;Looking at the part of this expression that has to do with &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, what do we see? It is the kernel of a Beta distribution! So, now we know that &lt;script type=&quot;math/tex&quot;&gt;\pi(p
\vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)&lt;/script&gt;. Cool. Another thing to notice here: the prior and the posterior distribution are from the same family. This, informally, is what you call a &lt;strong&gt;conjugate family&lt;/strong&gt;.  Anyways, now that we have the prior distribution, we can do some estimation.&lt;/p&gt;

&lt;p&gt;If we remember, one of the frequentist approaches to estimation is Maximum Likelihood. Here, our parameter follows a distribution so it makes sense to just say ‘hey… what’s its expected value?’ . So that’s what people do! The Bayes estimator is literally just &lt;script type=&quot;math/tex&quot;&gt;E[p\vert \mathbf{x}]&lt;/script&gt;. We know &lt;script type=&quot;math/tex&quot;&gt;\pi(p \vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)&lt;/script&gt; . Since we know the expected value of &lt;script type=&quot;math/tex&quot;&gt;Beta(\alpha,\beta)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\frac{\alpha}{\alpha + \beta}&lt;/script&gt;, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[p \vert \mathbf{x}] = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \sum_{i=1}^{n}x_i + \beta + n - \sum_{i=1}^{n}x_i} = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \beta + n}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \bar{X}(\frac{n}{\alpha + \beta + n}) + \frac{\alpha}{\alpha + \beta}(\frac{\alpha + \beta }{\alpha + \beta + n})&lt;/script&gt;

&lt;p&gt;So the Bayes estimator is just a weighted average of the prior mean and the MLE! When n is large, the MLE is weighted much more and, when n is small, we trust the prior distribution’s estimate. This is a very cool result, in my opinion.&lt;/p&gt;

&lt;p&gt;Let’s look at another example. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots , X_n \vert \lambda \sim Poisson(\lambda)&lt;/script&gt; and let &lt;script type=&quot;math/tex&quot;&gt;\pi(\lambda) \sim Gamma(\alpha, \beta)&lt;/script&gt;. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\lambda \vert \mathbf{x}) \propto f(\mathbf{x}\vert \lambda )\pi(\lambda)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{\lambda^{\sum_{i=1}^{n}x_i} e^{-n\lambda}}{\prod_{i=1}^{n}x_i !} \frac{1}{\Gamma(\alpha)\beta^{\alpha}} \lambda^{\alpha-1}e^{\frac{-\lambda}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{1}{\prod_{i=1}^{n}x_i! \, \Gamma(\alpha)\beta^{\alpha}} \lambda^{\sum_{i=1}^{n}x_i +\alpha -1} e^{-\lambda(n + \frac{1}{\beta})}&lt;/script&gt;

&lt;p&gt;and we should recognize the kernel (part with &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;) of this distribution and conclude that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\lambda\vert \mathbf{x}) \sim Gamma(\sum_{i=1}^{n}x_i + \alpha , (n + \frac{1}{\beta})^{-1})&lt;/script&gt;

&lt;p&gt;Another conjugate family! The Bayes estimator here is just&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[ \lambda | \mathbf{x}] = \frac{\sum_{i=1}^{n}x_i + \alpha }{n + \frac{1}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \bar{X} (\frac{n}{n + \frac{1}{\beta}}) + \alpha \beta (\frac{1}{n\beta + 1})&lt;/script&gt;

&lt;p&gt;In summary, its quite simple to find the posterior distribution in these cases - but this is obviously not true in general. Sometimes the posterior takes an unknown form, and the best we can do is draw samples from this posterior using different methods (Gibbs Sampling, Importance Sampling). These examples above are cases where the posterior follows almost immediately from the joint distribution (conjugate family in both cases, actually) and makes our lives very easy. There is much more to go into about Bayesian Estimation, and I will do so here in the future. For now, you’ve got a good start on it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Least Squares via the SVD</title><link href="http://localhost:4000/2018/04/11/least-squares-via-the-svd.html" rel="alternate" type="text/html" title="Least Squares via the SVD" /><published>2018-04-11T23:00:00-04:00</published><updated>2018-04-11T23:00:00-04:00</updated><id>http://localhost:4000/2018/04/11/least-squares-via-the-svd</id><content type="html" xml:base="http://localhost:4000/2018/04/11/least-squares-via-the-svd.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Let’s say you have some matrix of features (covariates) &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbb{R}^{m \times n}&lt;/script&gt; and a response vector &lt;script type=&quot;math/tex&quot;&gt;y \in \mathbb{R}^m&lt;/script&gt;. We want to find the input vector &lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}^n&lt;/script&gt; that solves this problem or gives us the best possible solution. How should we define best? That is subjective - but the most common idea is to take the real response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; and the predicted response &lt;script type=&quot;math/tex&quot;&gt;A[i,:] x&lt;/script&gt; and square the difference for each response. We can formalize this problem as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x || Ax - y ||_2^2&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\vert\vert . \vert\vert _ 2&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm - where we are just summing the components squared of the input to the norm (in this case, summing predicted minus real squared: the squared error) and taking the square root. We square it just to get rid of the square root - it is the same problem!&lt;/p&gt;

&lt;p&gt;Sometimes there may not an exact solution to this problem - that’s why all we are trying to do is minimize this sum of squares. Obviously, if there is an exact solution, that is going to minimize the squared error because, well, there is no error in that case!&lt;/p&gt;

&lt;p&gt;When would there be no error? Clearly, this must be the case that &lt;script type=&quot;math/tex&quot;&gt;y \in \mathcal{R}(A)&lt;/script&gt; for there to be no error. That is, the rank of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; for there to be no error. Does this guarantee the solution is unique? No. For the solution to be unique, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(A)&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. That is, the rank of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. Thus, for there to be a unique solution with zero error, &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be square. Notice that a unique solution and a solution with zero error are different concepts. You can have a unique solution where the error is non-zero, and you can have lots (infinite, actually!) solutions that are exact. So, now that we have some intuition into the problem, let’s work on solving it. Let’s think about it geometrically! Let’s consider the 3 &lt;script type=&quot;math/tex&quot;&gt;\times&lt;/script&gt; 3 matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}  1 &amp; 0 &amp; 0 \\  0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The range of this matrix is the x-y plane, obviously, since its columns span &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt;. So what if we want to solve a problem of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x || Ax - b ||_2^2&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;b \in \mathbb{R}^3&lt;/script&gt; and is non-zero in the z-component that is closest to the vector &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that exists in the range of our transformation. Check out this figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/projection_svd.jpg&quot; alt=&quot;Projection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The closest point is always the point directly perpendicular (or orthogonal) to the range of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; - think about the triangle inequality. Like, if the end of your vector was a donkey, and the range of your matrix was some food that donkeys like (any ideas?), the donkey would walk straight to the food in a direct path. That’s the intuition here. So, in other words, to find the answer to our question, we just need to project the vector we are trying to reach on to the range of values that we can reach. We need a projection matrix &lt;script type=&quot;math/tex&quot;&gt;P_ {\mathcal{R}(A)}&lt;/script&gt;. So, how do we get that…. first, we need to find an (orthonormal) basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; and then use that basis to build &lt;script type=&quot;math/tex&quot;&gt;P_{\mathcal{R}(A)}&lt;/script&gt;. So….. how do we get that…&lt;/p&gt;

&lt;p&gt;Let’s introduce an easy way to do this - the Singular Value Decomposition. Every matrix, square or not, has an SVD. For positive semi-definite matrices, the SVD is just the same as the Eigendecomposition. (ED). However, unlike the ED, the SVD is defined for rectangular matrices. You can view the ED as kind of a subset of the SVD. Anyways, here’s the definition.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A \in \mathbb{R}^{m \times n} = U\Sigma V^T = \sum_{i =1}^{r} \sigma_{i}u_{i}v_{i}^{T}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;U \in \mathbb{R}^{m \times m}&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;V \in \mathbb{R}^{n \times n}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma \in \mathbb{R}^{m \times n}&lt;/script&gt; and is diagonal. Both &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are orthogonal matrices, where the columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; are the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; and the columns of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt;. To see this, notice both &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; are square, symmetric matrices.&lt;/p&gt;

&lt;p&gt;Without loss of generalizability, consider&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TA = V \Sigma^T U^TU \Sigma V^T = V \Sigma ^T \Sigma V^T = V \Lambda V^T&lt;/script&gt;

&lt;p&gt;which is the eigendecomposition. So, clearly, the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; are found in &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;. Same can be said of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;. Another observation we can make here is that &lt;script type=&quot;math/tex&quot;&gt;\Sigma^T\Sigma = \Lambda&lt;/script&gt;, so the elements of &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;\{\sigma_ i\}_{i=1}^{r}&lt;/script&gt; are the square roots of the eigenvalues of both &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt;. These values are called the singular values of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;!&lt;/p&gt;

&lt;p&gt;So now, we have a tool to solve our least squares problem. How do we use it? If we want to form a basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt;. Well, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; is just the span of the columns of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Extending this notion to the SVD,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = \sum_ {i=1}^{r} \sigma_ i u_ i v_ i^Tx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum_ {i=1}^{r} \sigma_ i (v_ i^T x) u_i&lt;/script&gt;

&lt;p&gt;and notice that &lt;script type=&quot;math/tex&quot;&gt;\sigma_ i ( v_ i^Tx)&lt;/script&gt; is a scalar. This is just a linear combination of the first &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;  columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; Therefore, letting &lt;script type=&quot;math/tex&quot;&gt;U_ r&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;m \times r&lt;/script&gt; matrix with the first &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, corresponding to the non-zero singular values, we have shown that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A) = \mathcal{R}(U_ r)&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is orthogonal, &lt;script type=&quot;math/tex&quot;&gt;U_ r&lt;/script&gt; is an orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt;. Using the fact that, if &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is is an orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{V}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;BB^T = P_ {\mathcal{V}}&lt;/script&gt;, we have that &lt;script type=&quot;math/tex&quot;&gt;U_ rU_ r^T = P_ {\mathcal{R}(A)}&lt;/script&gt;. So, in terms of our problem, the point closest to &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that is in  &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;U_rU_r^Tb&lt;/script&gt;. We are now super close to finding &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt; that minimizes the error in the &lt;script type=&quot;math/tex&quot;&gt;\ell -2&lt;/script&gt; sense. To get there, we need to define one more thing - the &lt;strong&gt;Monroe-Penrose Pseudo Inverse&lt;/strong&gt;, &lt;script type=&quot;math/tex&quot;&gt;A^+&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Definition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^+ = V \Sigma ^+ U^T&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Sigma^+ \in \mathbb{R}^{n \times m}&lt;/script&gt; = diag&lt;script type=&quot;math/tex&quot;&gt;(\frac{1}{\sigma_ 1},\dots,\frac{1}{\sigma_r},0,\dots,0)&lt;/script&gt;. Using this definition,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AA^+ = U\Sigma V^T V \Sigma ^+ U^T &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= U \Sigma \Sigma^+ U^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= U_rU_r^T&lt;/script&gt;

&lt;p&gt;Therefore, since we know the closest possible point we can reach with &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;U_ r U_ r^Tb&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_ r U_ r^Tb = A(A^+b )&lt;/script&gt;

&lt;p&gt;and it follows immediately that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} = A^+b&lt;/script&gt;

&lt;p&gt;for any matrix &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbb{R}^{m \times n}&lt;/script&gt;. Therefore, we have shown there is a general solution to the least-squares problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x ||Ax - b||_2^2&lt;/script&gt;

&lt;p&gt;An interesting thing to note here:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|| A\hat{x} - b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || b - A\hat{x} ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || b - AA^+b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || (I - AA^+)b ||_2^2&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;U = [U_ r U_ o]&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;U_ o&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;m \times n-r&lt;/script&gt; matrix with columns corresponding the the zero-valued singular values of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;UU^T = I = [U_ r U_ o] [U_ r U_ o]^T = U_ rU_ r^T + U_ oU_ o^T&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\implies U_ oU_ o^T = I - AA^+ = I - P_ { \mathcal{R}(A)} = P_ {\mathcal{R}^{\perp}(A)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || (I - AA^+)b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || P_ {\mathcal{R}^{\perp}(A)}b ||_2^2&lt;/script&gt;

&lt;p&gt;That is, what we are doing is projecting the error onto the the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;P_ {\mathcal{R}(A)}&lt;/script&gt;. This is exactly what the figure above is showing! Our error should be directly orthogonal to &lt;script type=&quot;math/tex&quot;&gt;P_{\mathcal{R}(A)}&lt;/script&gt; for the objective function to be minimized.&lt;/p&gt;

&lt;p&gt;Now we have a bit of intuition into how we can use the Singular Value Decomposition to solve the least squares problem. This helps when you have a feature matrix and outcomes and want to find the specific weights associated with each covariate in the feature matrix. Least squares is used to solve for the parameters in linear regression and can be used to find the weights associated with neural networks… it has lots of applications. Hopefully, now you can apply it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Linear Regression</title><link href="http://localhost:4000/2018/03/10/linear-regression.html" rel="alternate" type="text/html" title="Linear Regression" /><published>2018-03-10T22:00:00-05:00</published><updated>2018-03-10T22:00:00-05:00</updated><id>http://localhost:4000/2018/03/10/linear-regression</id><content type="html" xml:base="http://localhost:4000/2018/03/10/linear-regression.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Linear Regression… the bread and butter of applied statistics.&lt;/p&gt;

&lt;p&gt;At one point or another, you have encountered linear regression. Did you read a statistic in the paper this morning? Was that statistic not a percent? Does anybody read the paper anymore?… Anyways, that number was probably derived via running a regression.&lt;/p&gt;

&lt;p&gt;A linear regression is used to describe an association (NOT A CAUSAL THING) between one (continuous) response variable, &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and a vector of (continuous or not, whatever) explanatory variables &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;. Why is it called linear regression? Well, remember from algebra that a line can be described by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = mx + b&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the independent (explanatory) variable and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is the dependent (response) variable. We assume there is a number &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;, the slope, that describes the relationship here. Essentially, we are saying the same thing with linear regression. We want to find the best &lt;script type=&quot;math/tex&quot;&gt;\beta_ 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; such that the formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i = \beta_0 + \beta_1*X_i + \epsilon_i&lt;/script&gt;

&lt;p&gt;is the best fit given our data. It doesn’t matter if &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is linear, quadratic, exponential.. only that the relationship is linear in the &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; parameters. &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is an error term for every &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;  observation. So the problem is really trying to find &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is minimum overall, i.e. taking into account each observation. The math behind linear regression is all about how we best do that. For this post, we will only explore simple linear regression where we have one explanatory variable. This need not need be the case, and we will cover that another time - but it involves some linear algebra and is a little more involved. The rest of this post assumes some knowledge of statistics, just at a basic level like expected values and distributions.&lt;/p&gt;

&lt;p&gt;So, we have this model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i = \beta_0 + \beta_1*X_i + \epsilon_i&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; is the response variable, &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; is the intercept (fixed), &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; is the slope (fixed), &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is a covariate (fixed) and &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is the error term, which is random and unobservable. As always, we need some assumptions in order to make this workable.. We assume that the errors are normally distributed and unrelated… that is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon_i \sim N(0,\sigma^2)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E[\epsilon_i\epsilon_j] = 0&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These assumptions lead us to some key implicit assumptions of linear regression. &lt;em&gt;Linearity&lt;/em&gt;,  &lt;em&gt;constant variance&lt;/em&gt;, &lt;em&gt;independence&lt;/em&gt; and &lt;em&gt;normally distributed&lt;/em&gt;. That is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E[Y_i \vert X_i ] = \beta_0 + \beta_1X_i&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_i^2 =  V[Y_i \vert X_i] = \sigma^2&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i \perp Y_j, i \neq j&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i \sim N(\beta_0 + \beta_1X_i,\sigma^2)&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We treat the covariates as fixed, but sometimes they aren’t.  For instance, there could be measurement error in the covariates. Really, the goal is to keep that randomness small.&lt;/p&gt;

&lt;p&gt;Let’s talk about &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;. How should we interpret them? Well, they are part of a linear relationship. We are  going to estimate them such that we fit a line to the data that should give us the expected value of the response given some input parameters.  That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[ Y_i | X_i ] = \hat{\beta_0} + \hat{\beta_1}X_i&lt;/script&gt;

&lt;p&gt;The little hat things mean they are estimated, not the true parameters… more on that later. So, &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_0}&lt;/script&gt; is the average value of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;  when &lt;script type=&quot;math/tex&quot;&gt;X = 0&lt;/script&gt;. Furthermore, &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_1}&lt;/script&gt; is the average change in &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; for a unit increase in &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. In general, it is best to not extrapolate beyond what your limits for &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; are. If your covariate is age, and you only have observations for people aged 15 - 30, don’t make the assumption that the model works for a 90-year-old… please.&lt;/p&gt;

&lt;p&gt;Okay! Now to the fun stuff… math. We wanna estimate the parameters here. How do we do that? Calculus. :)&lt;/p&gt;

&lt;p&gt;First, we need to define best. What is the &lt;em&gt;best&lt;/em&gt; fit? That’s arbitrary, as there are many ways to do this. We are going to do it via ordinary least squares, or OLS, which is the default way that any linear regression is fit. What this means is that we want to minimize the &lt;strong&gt;Sum of Squared Errors (SSE)&lt;/strong&gt;, or, in math,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{n} \hat{\epsilon_i}^2&lt;/script&gt;

&lt;p&gt;So, we need a definition of &lt;script type=&quot;math/tex&quot;&gt;\hat{\epsilon_i}&lt;/script&gt;, Well, it is just &lt;script type=&quot;math/tex&quot;&gt;Y_i - \hat{Y_i}&lt;/script&gt; or, even better, &lt;script type=&quot;math/tex&quot;&gt;Y_i - (\hat{\beta_0} + \hat{\beta_1}X_i)&lt;/script&gt; , since that is how we are guessing our reponse variable. This is good because now we have written &lt;script type=&quot;math/tex&quot;&gt;\hat{\epsilon_i}&lt;/script&gt; as a function of &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_0}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_1}&lt;/script&gt;, which are our tuneable parameters. Now we will need a little bit of calculus. This function will reach a critical value when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{\(*\)} \frac{\partial SSE}{\partial \beta_0} = 0&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{\(**\)} \frac{\partial SSE}{\partial \beta_1} = 0&lt;/script&gt;

&lt;p&gt;Remember? ;)&lt;/p&gt;

&lt;p&gt;Let’s solve (*) first.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial SSE}{\partial \beta_0} = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies -2 \sum_{i=1}^{n}(Y_i - \hat{\beta_0} -\hat{\beta_1}X_i) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \sum_{i=1}^{n}Y_i  - \hat{\beta_1}\sum_{i=1}^{n}X_i= n\hat{\beta_0}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}&lt;/script&gt;

&lt;p&gt;So, if we know &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_1}&lt;/script&gt;, we can estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_0}&lt;/script&gt; . Now, looking at (**) and subbing in this result,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial SSE}{\partial \beta_1} = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies -2 \sum_{i=1}^{n}X_i (Y_i - \hat{\beta_0} -\hat{\beta_1}X_i) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies -2 \sum_{i=1}^{n}X_i (Y_i -\bar{Y} +\hat{\beta_1}\bar{X} -\hat{\beta_1}X_i) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \sum_{i=1}^{n}X_i(Y_i - \bar{Y}) = \hat{\beta_1} \sum_{i=1}^{n}X_i(X_i - \bar{X})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \hat{\beta_1} = \frac{SSXY}{SSX}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;SSXY = \sum_{i=1}^{n}X_i(Y_i - \bar{Y})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;SSX =  \sum_{i=1}^{n}X_i(X_i - \bar{X})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Awesome! Now, given a vector of covariates and a vector of responses, you can estimate both paramaters quite easily! We didn’t really need to do this though. It is good for understanding… but we didn’t need to do the calculus, we have &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; to do this!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>