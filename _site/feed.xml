<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-12-31T14:49:19-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ben Brennan</title><subtitle>Hi, I'm Ben. This is my website/blog. Views are my own. Thanks for looking.</subtitle><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/jekyll/update/2019/12/30/gradient-descent.html" rel="alternate" type="text/html" title="Gradient Descent" /><published>2019-12-30T22:21:32-07:00</published><updated>2019-12-30T22:21:32-07:00</updated><id>http://localhost:4000/jekyll/update/2019/12/30/gradient-descent</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/12/30/gradient-descent.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;You’ll remember in OLS, we had something called the &lt;strong&gt;normal equations&lt;/strong&gt; - a nice, succinct, simple formula for calculating out best-fit parameters. You’ll also remember, then, that we had to invert an n by n matrix, &lt;script type=&quot;math/tex&quot;&gt;X^TX&lt;/script&gt; to get these parameters.. which, if n is large, is computationally very expensive. Today we’ll discuss gradient descent, a less computationally expensive way (for large n) to obtain parameters that minimize our squared error.&lt;/p&gt;

&lt;p&gt;It’s important to first note that gradient descent is not limited to least squares. Gradient descent is just an algorithm that can be used to maximize (or minimize) any convex objective function by calculating the gradient at a point, taking a step in that direction, calculation the gradient again, taking a step in that direction… so on and so forth, until the gradient is basically norm 0. It just so happens that the least squares function is convex, so gradient descent converges to the minimum.&lt;/p&gt;

&lt;p&gt;This algorithm won’t work for non-convex functions as it may only search out a local minimum, not the global minimum. If you’re standing at the base of two hills, the gradient is just going to take you up the steepest hill, not necessarily the tallest one, right? Okay.. anyways, do we remember what the gradient is?&lt;/p&gt;

&lt;p&gt;Let’s say we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\beta_0, \beta_1, \dots, \beta_q): \mathbb{R}^{q+1} \rightarrow \mathbb{R}&lt;/script&gt;

&lt;p&gt;then, the gradient is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \frac{\partial J}{\partial \beta_0} \hat{e_1} + \dots + \frac{\partial J}{\partial \beta_q} \hat{e_{q+1}} = \begin{bmatrix} \frac{\partial J}{\partial \beta_0} \\ \vdots \\ \frac{\partial J}{\partial \beta_q} \end{bmatrix}&lt;/script&gt;

&lt;p&gt;so, essentially, each component of the gradient tells you how fast your function changes with respect to the standard basis in each direction. We can find the directional derivative of any unit vector &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\nabla J \cdot \vec{v}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J \cdot \vec{v} = \| \nabla J\|\|\vec{v}\|\cos(\theta) =\| \nabla J\|\cos(\theta)&lt;/script&gt;

&lt;p&gt;since &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; is a unit vector, where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the angle between the two vectors. &lt;script type=&quot;math/tex&quot;&gt;\cos(\theta)&lt;/script&gt; is max when $\theta = 0$ - that is, when &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; is in the direction of the gradient! So the steepest ascent is in the direction of the gradient, and the steepest descent is in the opposite direction. We should note that the steepest ascent here is limited to unit vectors, so the steepest ascent is really in the direction of &lt;script type=&quot;math/tex&quot;&gt;\frac{\nabla J}{\| \nabla J\|}&lt;/script&gt;, right? But that’s the same direction as the gradient. And we are going to kinda normalize the gradient in our own way later… you will see.&lt;/p&gt;

&lt;p&gt;You can think of the gradient as kind of a trade-off. Say the gradient is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;3,1,6&gt; %]]&gt;&lt;/script&gt;. You’re confined to a circle in terms of where you can move with the gradient (or any vector) starting from a point. That is, there’s a trade-off between directions. You can essentially trade 3 steps in the Y direction for one step in the X direction. So an optimal direction does just that, hence why the optimal direction is the gradient… it takes one 3 steps in the X direction for each step in the Y direction. Any other decision is ‘unfair’, i.e there is a better trade-off that maximizes how steep the gradient could be.&lt;/p&gt;

&lt;p&gt;Anyways. The gradient heads in the direction of the steepest ascent, so an algorithm searching out the lowest point of a function should head in the opposite of the gradient. So let’s consider least squares, where our squared error loss term is a function of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\beta}&lt;/script&gt;, the vector of parameters. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\beta_0, \dots, \beta_q) = \frac{1}{2n} \sum_{i=1}^n (\hat{y_i} - y_i)^2  =\frac{1}{2n} \sum_{i=1}^n ( \beta_0 + \beta_1x_{i1} + \dots + \beta_qx_{iq} - y_i )^2&lt;/script&gt;

&lt;p&gt;where we just normalized the squared error function. Remember, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is the number of features, and &lt;script type=&quot;math/tex&quot;&gt;q+1&lt;/script&gt; is the number of parameters we must estimate. Now, using a bit of calculus, we see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J} {\partial \beta_0} = \frac{1}{n} \sum_{i=1}^n ( \hat{y_i} - y_i)&lt;/script&gt;

&lt;p&gt;and, for &lt;script type=&quot;math/tex&quot;&gt;j = 1,\dots, q&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J} {\partial \beta_j} = \frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)x_{ij}&lt;/script&gt;

&lt;p&gt;so, a gradient descent algorithm would look something like&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;is the norm of the gradient greater than our threshold?
    &lt;ul&gt;
      &lt;li&gt;if yes, move along the gradient for each &lt;script type=&quot;math/tex&quot;&gt;\beta_J&lt;/script&gt; by some &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and check again&lt;/li&gt;
      &lt;li&gt;if no, return the current point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is a stepsize of your choosing. It is important to note that the choice of the step-size is important. A step-size that is too big will cause the algorithm to jump over the minimum value and maybe even diverge, whereas a step-size that is too small will mean the algorithm takes forever to converge to the minimum value. In one dimension, we can see what is happening below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/lambda_tut.jpg&quot; alt=&quot;Different stepsizes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the step size is small (left), it is converging to the minimum value… but it is gonna take forever (maybe even actually. . .) and on the right, where the step size is too large, the minimum is being overshot and then the larger slope (gradient) is compounding with a large step size to overshoot by even more and so on and so forth as your algorithm sadly diverges to infinity ….. :(&lt;/p&gt;

&lt;p&gt;You can imagine this type of computation is best done in a while-loop. Below is an interpretation of this algorithm in Julia:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;&lt;span class=&quot;n&quot;&gt;gradient_descent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;α&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;#for i in 1:q_plus_one&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#    gradient[i] = α*(error'*X[:,i])&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#end&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first part of this just says ‘okay, did you add an intercept? maybe not.. because you might not think to add the column of ones… we are gonna assume you want to do this so we will do it for you if you did not’. Obviously, this isn’t necessary, and could actually lead to incorrect results if the column  missing isn’t the one of all ones, but it is just to show you that coding is YOUR life, do with it what you will… Anyways. We compute the gradient via the dot product, as you will notice that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_i} - y_i) = e^T \cdot \mathbf{1_n}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_i} - y_i)x_{ij} = e^T  \cdot \mathbf{X}[:,j]&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; is the n-dimensional vector of errors and $latex X$ is the feature matrix, with a row of ones so that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_1} - y_i) = e^T \cdot \mathbf{X}[:,1] = e^T \cdot \mathbf{1_n}&lt;/script&gt;

&lt;p&gt;We also add an iteration count (just in case our step size is too small or too large or we totally messed up the code) to stop the loop at a certain point. However, this way of thinking about the algorithm is commented out because it is not efficient. Generally, for-loops should (and usually can) be replaced by matrix multiplication, which is in the code above! Let’s go ahead and see if we can see how you would write that ‘in math’…..&lt;/p&gt;

&lt;p&gt;You can see in the code that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \mathbf{X^T(X\beta - Y)}&lt;/script&gt;

&lt;p&gt;which is actually just exactly what we wrote above. You’re multiplying the errors by the columns of the feature matrix to get the components of the gradient, so why not just transpose your matrix and use matrix multiplication? Thats what this is saying. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\beta^{(new)}} = \mathbf{\beta^{(old)}} - \alpha \frac{1}{n} \mathbf{X^T( X\beta - Y)}&lt;/script&gt;

&lt;p&gt;Perfect. So now we know what it is, how to use it.. but when would you use it? Usually only when you have really large values of n (lots of observations, like millions…) . Essentially, the only reason you use it is time. There are other things you can do here, like use only one observation (which is called Least Mean Squares) which saves even more time but isn’t as great, obviously. There are other issues with gradient descent, like correctly choosing the step-size that, in practice, you really have no way around. There are plenty of ways to choose a step-size, but no set-rule - you’ll have to figure it out on your own (and that takes time, too!). What’s best about gradient descent is that you can use it for a lot of things. Least squares is just one. You can use it for plenty of other things… really, you can use it for whatever problem you have where you need to optimize a function. But I hope learning about it via least squares was helpful!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Least Squares via the SVD</title><link href="http://localhost:4000/jekyll/update/2018/04/11/least-squares-via-the-svd.html" rel="alternate" type="text/html" title="Least Squares via the SVD" /><published>2018-04-11T21:00:00-06:00</published><updated>2018-04-11T21:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/11/least-squares-via-the-svd</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/11/least-squares-via-the-svd.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Let’s say you have some matrix of features (covariates) &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbb{R}^{m \times n}&lt;/script&gt; and a response vector &lt;script type=&quot;math/tex&quot;&gt;y \in \mathbb{R}^m&lt;/script&gt;. We want to find the input vector &lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}^n&lt;/script&gt; that solves this problem or gives us the best possible solution. How should we define best? That is subjective - but the most common idea is to take the real response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; and the predicted response &lt;script type=&quot;math/tex&quot;&gt;A[i,:] x&lt;/script&gt; and square the difference for each response. We can formalize this problem as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x || Ax - y ||_2^2&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\vert\vert . \vert\vert _ 2&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm - where we are just summing the components squared of the input to the norm (in this case, summing predicted minus real squared: the squared error) and taking the square root. We square it just to get rid of the square root - it is the same problem!&lt;/p&gt;

&lt;p&gt;Sometimes there may not an exact solution to this problem - that’s why all we are trying to do is minimize this sum of squares. Obviously, if there is an exact solution, that is going to minimize the squared error because, well, there is no error in that case!&lt;/p&gt;

&lt;p&gt;When would there be no error? Clearly, this must be the case that &lt;script type=&quot;math/tex&quot;&gt;y \in \mathcal{R}(A)&lt;/script&gt; for there to be no error. That is, the rank of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; for there to be no error. Does this guarantee the solution is unique? No. For the solution to be unique, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(A)&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. That is, the rank of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. Thus, for there to be a unique solution with zero error, &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be square. Notice that a unique solution and a solution with zero error are different concepts. You can have a unique solution where the error is non-zero, and you can have lots (infinite, actually!) solutions that are exact. So, now that we have some intuition into the problem, let’s work on solving it. Let’s think about it geometrically! Let’s consider the 3 &lt;script type=&quot;math/tex&quot;&gt;\times&lt;/script&gt; 3 matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}  1 &amp; 0 &amp; 0 \\  0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The range of this matrix is the x-y plane, obviously, since its columns span &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt;. So what if we want to solve a problem of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x || Ax - b ||_2^2&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;b \in \mathbb{R}^3&lt;/script&gt; and is non-zero in the z-component that is closest to the vector &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that exists in the range of our transformation. Check out this figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/projection_svd.jpg&quot; alt=&quot;Projection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The closest point is always the point directly perpendicular (or orthogonal) to the range of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; - think about the triangle inequality. Like, if the end of your vector was a donkey, and the range of your matrix was some food that donkeys like (any ideas?), the donkey would walk straight to the food in a direct path. That’s the intuition here. So, in other words, to find the answer to our question, we just need to project the vector we are trying to reach on to the range of values that we can reach. We need a projection matrix &lt;script type=&quot;math/tex&quot;&gt;P_ {\mathcal{R}(A)}&lt;/script&gt;. So, how do we get that…. first, we need to find an (orthonormal) basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; and then use that basis to build &lt;script type=&quot;math/tex&quot;&gt;P_{\mathcal{R}(A)}&lt;/script&gt;. So….. how do we get that…&lt;/p&gt;

&lt;p&gt;Let’s introduce an easy way to do this - the Singular Value Decomposition. Every matrix, square or not, has an SVD. For positive semi-definite matrices, the SVD is just the same as the Eigendecomposition. (ED). However, unlike the ED, the SVD is defined for rectangular matrices. You can view the ED as kind of a subset of the SVD. Anyways, here’s the definition.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A \in \mathbb{R}^{m \times n} = U\Sigma V^T = \sum_{i =1}^{r} \sigma_{i}u_{i}v_{i}^{T}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;U \in \mathbb{R}^{m \times m}&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;V \in \mathbb{R}^{n \times n}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma \in \mathbb{R}^{m \times n}&lt;/script&gt; and is diagonal. Both &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are orthogonal matrices, where the columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; are the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; and the columns of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt;. To see this, notice both &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; are square, symmetric matrices.&lt;/p&gt;

&lt;p&gt;Without loss of generalizability, consider&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TA = V \Sigma^T U^TU \Sigma V^T = V \Sigma ^T \Sigma V^T = V \Lambda V^T&lt;/script&gt;

&lt;p&gt;which is the eigendecomposition. So, clearly, the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; are found in &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;. Same can be said of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;. Another observation we can make here is that &lt;script type=&quot;math/tex&quot;&gt;\Sigma^T\Sigma = \Lambda&lt;/script&gt;, so the elements of &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;\{\sigma_ i\}_{i=1}^{r}&lt;/script&gt; are the square roots of the eigenvalues of both &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt;. These values are called the singular values of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;!&lt;/p&gt;

&lt;p&gt;So now, we have a tool to solve our least squares problem. How do we use it? If we want to form a basis for $latex \mathcal{R}(A) $. Well, $latex \mathcal{R}(A) $ is just the span of the columns of $latex A$. Extending this notion to the SVD,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = \sum_ {i=1}^{r} \sigma_ i u_ i v_ i^Tx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum_ {i=1}^{r} \sigma_ i (v_ i^T x) u_i&lt;/script&gt;

&lt;p&gt;and notice that &lt;script type=&quot;math/tex&quot;&gt;\sigma_ i ( v_ i^Tx)&lt;/script&gt; is a scalar. This is just a linear combination of the first &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;  columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; Therefore, letting &lt;script type=&quot;math/tex&quot;&gt;U_ r&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;m \times r&lt;/script&gt; matrix with the first &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, corresponding to the non-zero singular values, we have shown that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A) = \mathcal{R}(U_ r)&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is orthogonal, &lt;script type=&quot;math/tex&quot;&gt;U_ r&lt;/script&gt; is an orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt;. Using the fact that, if &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is is an orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{V}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;BB^T = P_ {\mathcal{V}}&lt;/script&gt;, we have that &lt;script type=&quot;math/tex&quot;&gt;U_ rU_ r^T = P_ {\mathcal{R}(A)}&lt;/script&gt;. So, in terms of our problem, the point closest to &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that is in  &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;U_rU_r^Tb&lt;/script&gt;. We are now super close to finding &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt; that minimizes the error in the &lt;script type=&quot;math/tex&quot;&gt;\ell -2&lt;/script&gt; sense. To get there, we need to define one more thing - the &lt;strong&gt;Monroe-Penrose Pseudo Inverse&lt;/strong&gt;, &lt;script type=&quot;math/tex&quot;&gt;A^+&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Definition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^+ = V \Sigma ^+ U^T&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Sigma^+ \in \mathbb{R}^{n \times m}&lt;/script&gt; = diag&lt;script type=&quot;math/tex&quot;&gt;(\frac{1}{\sigma_ 1},\dots,\frac{1}{\sigma_r},0,\dots,0)&lt;/script&gt;. Using this definition,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AA^+ = U\Sigma V^T V \Sigma ^+ U^T &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= U \Sigma \Sigma^+ U^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= U_rU_r^T&lt;/script&gt;

&lt;p&gt;Therefore, since we know the closest possible point we can reach with &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;U_ r U_ r^Tb&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_ r U_ r^Tb = A(A^+b )&lt;/script&gt;

&lt;p&gt;and it follows immediately that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} = A^+b&lt;/script&gt;

&lt;p&gt;for any matrix &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbb{R}^{m \times n}&lt;/script&gt;. Therefore, we have shown there is a general solution to the least-squares problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;argmin_ x ||Ax - b||_2^2&lt;/script&gt;

&lt;p&gt;An interesting thing to note here:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|| A\hat{x} - b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || b - A\hat{x} ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || b - AA^+b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || (I - AA^+)b ||_2^2&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;U = [U_ r U_ o]&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;U_ o&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;m \times n-r&lt;/script&gt; matrix with columns corresponding the the zero-valued singular values of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;UU^T = I = [U_ r U_ o] [U_ r U_ o]^T = U_ rU_ r^T + U_ oU_ o^T&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\implies U_ oU_ o^T = I - AA^+ = I - P_ { \mathcal{R}(A)} = P_ {\mathcal{R}^{\perp}(A)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || (I - AA^+)b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || P_ {\mathcal{R}^{\perp}(A)}b ||_2^2&lt;/script&gt;

&lt;p&gt;That is, what we are doing is projecting the error onto the the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;P_ {\mathcal{R}(A)}&lt;/script&gt;. This is exactly what the figure above is showing! Our error should be directly orthogonal to &lt;script type=&quot;math/tex&quot;&gt;P_{\mathcal{R}(A)}&lt;/script&gt; for the objective function to be minimized.&lt;/p&gt;

&lt;p&gt;Now we have a bit of intuition into how we can use the Singular Value Decomposition to solve the least squares problem. This helps when you have a feature matrix and outcomes and want to find the specific weights associated with each covariate in the feature matrix. Least squares is used to solve for the parameters in linear regression and can be used to find the weights associated with neural networks… it has lots of applications. Hopefully, now you can apply it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>