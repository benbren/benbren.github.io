<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-02-08T17:36:51-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ben Brennan</title><subtitle>Hi, I'm Ben. This is my website/blog. Views are my own. Thanks for looking.</subtitle><entry><title type="html">Intro to Inference</title><link href="http://localhost:4000/2020/01/10/intro-to-inference.html" rel="alternate" type="text/html" title="Intro to Inference" /><published>2020-01-10T12:00:00-05:00</published><updated>2020-01-10T12:00:00-05:00</updated><id>http://localhost:4000/2020/01/10/intro-to-inference</id><content type="html" xml:base="http://localhost:4000/2020/01/10/intro-to-inference.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Lots of people go through life talking about statistics and not actually knowing what a statistic &lt;em&gt;is&lt;/em&gt;, sadly enough. The point of statistics, and statistical inference in general, is to make inferences about a &lt;em&gt;population&lt;/em&gt; based off a sample. In general, you have a population parameter (&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;) that needs to be estimated (could be high - dimensional - the most we will talk about is &lt;em&gt;max&lt;/em&gt; 2, so no worries) and some data (a random sample, once could say…) &lt;script type=&quot;math/tex&quot;&gt;X_ 1, \dots, X_ n&lt;/script&gt; from that population, from which we observe the data &lt;script type=&quot;math/tex&quot;&gt;x_ 1, \dots, x_ n&lt;/script&gt;. The goal of statistical inference is to take that sample and make really good educated guesses about the population and also make good, educated guesses about how good and educated your guess is. Make sense? I didn’t think so.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; is the process of drawing information about a population based off a sample (like I said above, with a fancy word defining it). The points is - probability theory is based on knowing &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. We never actually know &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, so to do anything in practice we need inference about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Big time important.&lt;/p&gt;

&lt;p&gt;For example, we can make conclusions about the probability we get a certain number of successes in a certain number of trials (i.e Binomial(&lt;script type=&quot;math/tex&quot;&gt;n,p&lt;/script&gt;), or the probability that a certain metric is less than some value (i.e &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(\mu, \sigma^2&lt;/script&gt;)) - but that requires us to know the distribution those values follow in the population (i.e to know the population parameters).&lt;/p&gt;

&lt;p&gt;First - and back to the beginning - what is a statistic?! (FYI - this is less of a post, more of a literal book chapter.. fair warning!)&lt;/p&gt;

&lt;p&gt;A statistics takes a random variable &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; and maps it via some function &lt;script type=&quot;math/tex&quot;&gt;T(.)&lt;/script&gt;. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(x_1, \dots, x_n) = T(\mathbf{X}): \mathbb{R}^n \rightarrow \mathbb{R}^m&lt;/script&gt;

&lt;p&gt;A few things. &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is also a random variable, obviously. There is no restriction on &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;, but I think you probably get the idea that &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
m &lt; n %]]&gt;&lt;/script&gt;, because it doesn’t really make sense to make our data more complicated. We also don’t want to lose information in this mapping - this is a key point. For instance, if &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = X_1&lt;/script&gt;, we have lost so much information contained in our data by using this transformation.&lt;/p&gt;

&lt;p&gt;So, in summary, we want to take our data and use some transformation &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; to make that data simpler and no less informative. That is what makes a good statistic good. This is why we use some statistics, and not others - as we will see. Anyways, this seems easy. Not like there’s a million choices for &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; or anything…&lt;/p&gt;

&lt;h2 id=&quot;sufficient-statistic&quot;&gt;Sufficient Statistic&lt;/h2&gt;

&lt;h4 id=&quot;definition&quot;&gt;Definition&lt;/h4&gt;

&lt;p&gt;Anyways. Let’s define what a sufficient statistic is. A statistic &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is called a &lt;em&gt;sufficient statistic&lt;/em&gt; for the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; if the distribution of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; given &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Why? This means that, given that we know the value of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt;, the information left in the sample does not contain any information about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. This should be obvious - if the distribution of the data does not depend on the parameter, how can you get information about the parameter from that distribution ya know? Formally, if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x})&lt;/script&gt;

&lt;p&gt;then &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s look at an example: assume &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \textrm{Bernoulli}(p)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = \sum_{i=1}^n X_i&lt;/script&gt;. Is &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; sufficient for &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;? Yep. Next.&lt;/p&gt;

&lt;p&gt;Just kidding. Let’s see why.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert T(\mathbf{X}) = t) = g(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t) = \frac{P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t)}{P(T(\mathbf{X}) = t))}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
= \begin{cases} 
	\frac{P(\mathbf{X} = \mathbf{x})}{P(T(\mathbf{X}) = t))} &amp; \textrm{if} \; T(\mathbf{X}) = t \\
	0 &amp; \textrm{else} 
 	\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;(only considering the top case)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{q_{T(\mathbf{X})}(t \vert p)}&lt;/script&gt;

&lt;p&gt;Now, notice the distribution of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) \sim \textrm{Binomial}(n,p)&lt;/script&gt;. Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_{\mathbf{X}}(\mathbf{x})}{f_{T(\mathbf{X})}(t)} = \frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{t}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{t}}&lt;/script&gt;

&lt;p&gt;which doesn’t contain &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. So now - yep, &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;!&lt;/p&gt;

&lt;h4 id=&quot;theorem-1&quot;&gt;Theorem 1&lt;/h4&gt;

&lt;p&gt;Now, in typical fashion, lets define like a million more ways to show something is sufficient. The first - shown through example. From above we showed that the original definiton could  boil down to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q_{T(\mathbf{X})}(t \vert \theta)} = \frac{f_{\mathbf{X}}(\mathbf{x} \vert \theta)}{q(T(\mathbf{x}) \vert \theta)}&lt;/script&gt;

&lt;p&gt;so, if this ratio is free of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; in the support of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;(T(\mathbf{X})&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So, the example above boils down to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\displaystyle\prod_{i=1}^n p^{x_i}(1-p)^{1 - x_i}}{\binom{n}{T(\mathbf{x})}p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}} =  \frac{1}{\binom{n}{T(\mathbf{x})}}&lt;/script&gt;

&lt;p&gt;which does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; so we come to the same conclusion. This seems redundant. It is, really, but the benefit is we don’t have to consider &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) \neq t&lt;/script&gt;. So, originally, we &lt;em&gt;techincally&lt;/em&gt; had to say that 0 did not depend on &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, which is obvious. In this second definition, we don’t even need to consider that.&lt;/p&gt;

&lt;p&gt;Let’s try another example using this second definition of sufficiency. Let’s check whether &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = \bar{X}&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; in the normal distribution (with known variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(x \vert \mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x - \mu)^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;p&gt;Recall that &lt;script type=&quot;math/tex&quot;&gt;\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})&lt;/script&gt;. Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_{\mathbf{X}}(\mathbf{x} \vert \mu)}{q(T(\mathbf{X}) \vert \mu)} = \frac{(\frac{1}{\sqrt{2\pi\sigma^2}})^n\textrm{exp}\left(- \displaystyle\sum_{i=1}^n\frac{(x_i - \mu)^2}{2\sigma^2}\right)}{\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-n(\bar{x} - \mu)^2}{2\sigma^2}\right)} = \mathcal{K} \frac{\textrm{exp}\left(\frac{- \sum_{i=1}^n(x_i^2 - 2x_i\mu)}{2\sigma^2}\right)}{\textrm{exp}\left( \frac{2\bar{x}n \mu - n\bar{x}^2}{2 \sigma^2}\right)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \mathcal{K}\frac{\textrm{exp}\left( \frac{-\sum_{i=1}^{n}x_i^2}{2 \sigma^2}\right)}{\textrm{exp}\left(\frac{-n\bar{x}^2}{2 \sigma^2}\right)}&lt;/script&gt;

&lt;p&gt;which doesn’t depend on &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;! So, the sample mean is a sufficient statistic for the population average in a normal distribution - hence why we use it all the time!&lt;/p&gt;

&lt;p&gt;Now, moving on to even more ways to show sufficiency…&lt;/p&gt;

&lt;h4 id=&quot;factorization-theorem&quot;&gt;Factorization Theorem&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Factorization Theorem&lt;/strong&gt; : &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is sufficient if &lt;em&gt;and only if&lt;/em&gt; there exists &lt;script type=&quot;math/tex&quot;&gt;g(t\vert \theta)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;h(\theta)&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\forall \mathbf{x}, \theta&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert \theta) = g(T(\mathbf{x})\vert \theta)h(\mathbf{x})&lt;/script&gt;

&lt;p&gt;Since this is an &lt;em&gt;iff&lt;/em&gt;, we have to prove it in both directions. First,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Sufficiency &lt;script type=&quot;math/tex&quot;&gt;\implies&lt;/script&gt; Factorization&lt;/p&gt;

&lt;p&gt;This is the easy case.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x}\vert \theta) = P(\mathbf{X} = \mathbf{x} \vert \theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t \vert \theta)&lt;/script&gt;

&lt;p&gt;and, by the definition of conditional probability,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t,\theta)&lt;/script&gt;

&lt;p&gt;and, by sufficiency (i.e. we assume given &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt;, the distribution of the data is free of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= P(T(\mathbf{X}) = t \vert \theta)P(\mathbf{X} = \mathbf{x} \vert T(\mathbf{X}) = t)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= g(t \vert \theta)h(\mathbf{x})&lt;/script&gt;

&lt;p&gt;and we have proved that direction. Now,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: Factorization &lt;script type=&quot;math/tex&quot;&gt;\implies&lt;/script&gt; Sufficiency&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;q(y \vert \theta)&lt;/script&gt; be the pmf of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; and let &lt;script type=&quot;math/tex&quot;&gt;A_t&lt;/script&gt; be the set containing all the possible data that yield a statistics &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = t&lt;/script&gt;. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t = \{ \mathbf{y}: T(\mathbf{y}) = t\}&lt;/script&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(t \vert \theta) = P(T(\mathbf{X}) = t \vert \theta) = \displaystyle\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)&lt;/script&gt;

&lt;p&gt;So, since we are assuming factorization,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_{\mathbf{X}}(\mathbf{x}\vert\theta)}{q(t \vert \theta)} = \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} f_{\mathbf{X}}(\mathbf{y}\vert \theta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{g(T(\mathbf{x})\vert \theta)h(\mathbf{x})}{\sum_{\mathbf{y} \in A_t} g(T(\mathbf{y}) \vert \theta) h(\mathbf{y})}&lt;/script&gt;

&lt;p&gt;and, since for &lt;script type=&quot;math/tex&quot;&gt;y \in A_t&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{y}) = t = T(\mathbf{x})&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{h(\mathbf{x})}{\sum_{y \in A_T}h(\mathbf{y})}&lt;/script&gt;

&lt;p&gt;and this does not ever depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, so &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is sufficient and we have completed the proof!&lt;/p&gt;

&lt;p&gt;So, this makes things wicked easy…. let’s go back to the two examples we have done above. First, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} \sim \textrm{Bernoulli}(p)&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x} \vert p) = \displaystyle\prod p^{x_i}(1-p)^{1- x_i} = p^{\sum x_i}(1-p)^{n - \sum x_i} = p^t(1-p)^{n-t}&lt;/script&gt;

&lt;p&gt;and then &lt;script type=&quot;math/tex&quot;&gt;g(t \vert p) = p^t(1-p)^{n-p}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;h(\mathbf{y}) = 1&lt;/script&gt;, then we can see that &lt;script type=&quot;math/tex&quot;&gt;\sum_{i= 1}^n X_i&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, let us consider the second, normal with known variance, example.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(\mathbf{x} \vert \mu) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right) \times \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)&lt;/script&gt;

&lt;p&gt;so &lt;script type=&quot;math/tex&quot;&gt;h(\mathbf{x}) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) ^n \textrm{exp}\left( - \sum x_i^2 \right)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g(t \vert \mu) = \textrm{exp}\left( n(2 \mu \bar{x} - \mu) \right)&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;t = \bar{x}&lt;/script&gt;, so the sample mean is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;. Way easier - we like that.&lt;/p&gt;

&lt;p&gt;Let’s try a new example. Consider &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} \sim \textrm{Uniform}(0,\theta)&lt;/script&gt;. What is a sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;? To assess this, let’s use the factorization theorem. Remember,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x \vert \theta) = \begin{cases} 
\frac{1}{\theta} &amp; \textrm{if} \; x \in [0,\theta] \\
0 &amp; \textrm{else}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is identical to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{\theta}I(0 \leq x) I (x \leq \theta)&lt;/script&gt;

&lt;p&gt;so, therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert \theta) = \displaystyle\prod_{i=1}^n \frac{1}{\theta}I(0 \leq x_i) I (x_i \leq \theta) = \theta^{-n}I(x_{(n)} \leq \theta) I(0 \leq x_{(1)})&lt;/script&gt;

&lt;p&gt;so we can conclude that the max, &lt;script type=&quot;math/tex&quot;&gt;X_{(n)}&lt;/script&gt;, is a sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, let’s consider a two-dimensional (&lt;em&gt;whoa&lt;/em&gt;) sufficient statistic! When I learned this, the professor made this seem very straight forward - but I know things get real iffy real quick when the dimension of things increase - so lets take it slow. What we are looking for now is &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X}))&lt;/script&gt; such that the factorization theorem still applies. Let’s consider a normal distribution with unknown variance. Now, what we need is to find a factorization&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = g(T_1(\mathbf{x}), T_2(\mathbf{X}) \vert \mu, \sigma^2)h(\mathbf{x})&lt;/script&gt;

&lt;p&gt;This differs from the one dimensional case where we kind of just tossed the parts of the exponent that did not depend on &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; into the constant and called it a day. Now, we can do that but only for the parts of the exponent that don’t depend on &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; which is none of them so trick-statement you can’t toss anything into the constant from the exponent. Turns out that constant infront isn’t even a constant anymore since it involves &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;. Bummer. All this means, though, is more algebra - which, although it is tedious, is not difficult if you pay attention (which &lt;em&gt;I know&lt;/em&gt; is difficult). So, let’s get to it.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \displaystyle\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\textrm{exp}\left( \frac{-(x_i - \mu)^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-\sum_{i=1}^n((x_i - \bar{x}) + (\bar{x} - \mu))^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 +2 \sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu)  - n(\bar{x} - \mu)^2\right)\right)&lt;/script&gt;

&lt;p&gt;and now note that 
&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n(x_i - \bar{x})(\bar{x} - \mu) = \bar{x}\sum x_i - \mu\sum x_i - n\bar{x} + n\bar{x}\mu = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;so, therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert \mu, \sigma^2) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) ^n \textrm{exp}\left( \frac{-1}{2\sigma^2}\left(\sum_{i=1}^n(x_i - \bar{x})^2 - n(\bar{x} - \mu)^2\right)\right)&lt;/script&gt;

&lt;p&gt;and by using &lt;script type=&quot;math/tex&quot;&gt;h(\mathbf{x}) = 1&lt;/script&gt;, we have the factorization theorem! Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (\bar{X}, \sum_{i=1}^n (X_i - \bar{X})^2)&lt;/script&gt;

&lt;p&gt;is a sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\phi} = (\mu, \sigma^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Let’s try another one - &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} \sim \textrm{Uniform}(\alpha, \theta)&lt;/script&gt;. Extending our knowledge of the one-dimensional case, this should be fairly simple.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}(\mathbf{x} \vert \theta, \alpha) = \displaystyle\prod_{i=1}^n \frac{1}{\theta - \alpha}I(\alpha \leq x_i) I (x_i \leq \theta) = (\theta - \alpha)^{-n}I(x_{(n)} \leq \theta) I(\alpha \leq x_{(1)})&lt;/script&gt;

&lt;p&gt;so we can conclude that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(\mathbf{X}) = (T_1(\mathbf{X}), T_2(\mathbf{X})) = (X_{(1)}, X_{(n)})&lt;/script&gt;

&lt;p&gt;is suffcient for &lt;script type=&quot;math/tex&quot;&gt;\phi = (\alpha,\theta)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;minimal-sufficient-statistic&quot;&gt;Minimal Sufficient Statistic&lt;/h2&gt;

&lt;p&gt;As you have (hopefully) noticed, sufficient statistics are not even close to unique. For instance, the sample itself is always sufficient. The order statistics are also always sufficient as we can set &lt;script type=&quot;math/tex&quot;&gt;h(\mathbf{x})&lt;/script&gt; to be 1 and then the joint distribution is the same, just reordered (see the examples section for more detials on this). Also, any one-to-one function of a sufficient statistic is also a sufficient statistic, as that value maps uniquely back to the original sufficient statistic. Therefore, we need some notion of &lt;em&gt;better&lt;/em&gt; when it comes to these types of statistics. That is what the idea of a &lt;strong&gt;minimal sufficient statistic&lt;/strong&gt; is.  The idea is to find a sufficient statistic that is more sufficent than the rest - i.e. one that has the property of maximum data reduction? The defintion of the minimal sufficient statistic is as follows:&lt;/p&gt;

&lt;p&gt;A sufficient statistic &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is a minimal sufficient statistic if, for every other suffcient statistic &lt;script type=&quot;math/tex&quot;&gt;T'(\mathbf{X})&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is a function of &lt;script type=&quot;math/tex&quot;&gt;T'(\mathbf{X})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This definition is fairly unclear, in my humble opinion. I guess not unclear, but fails to provide the intuition behind it. So lets try to explain it a bit better. The idea behind sufficient statistics was to reduce to data into a &lt;strong&gt;coarser&lt;/strong&gt; partition, without losing information about the parameter of interest. We now want the coarsest partition of data we can get - the maximum data reduction. So lets consider a function, &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;. Remember, a function is defined such that, if &lt;script type=&quot;math/tex&quot;&gt;x_1 = x_2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(x_1) = f(x_2)&lt;/script&gt; - but this does not mean that if &lt;script type=&quot;math/tex&quot;&gt;f(x_1) = f(x_2)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;x_1 = x_2&lt;/script&gt;. What does this mean in our case? It means, if we have a partition, we can make it more coarse (i.e the domain smaller) by finding a function of that partition that maps to a smaller partition. That is, if &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt; are partitions of a sample space &lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}&lt;/script&gt;, then a function can &lt;strong&gt;only&lt;/strong&gt; map to a coarser partition. That is, a function will map both to the same partition if they are the same, but may also map them to the same partition if they are different. Also, a function will hit all values in its outcome space. So, at worst, a function will map all partitions in the first partition space to all partions in the other sample space and, at best, will map a few of the original partitions to the same partition. Therefore, a function makes the sample space more coarse. So, if &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is a function of all other &lt;script type=&quot;math/tex&quot;&gt;T'&lt;/script&gt;, then it is a partition is the most coarse - it has the maximum reduction of data. This is a nice definition but, in practice, it does not really help us. The following is a theorem that &lt;em&gt;can&lt;/em&gt; help:&lt;/p&gt;

&lt;p&gt;The ratio &lt;script type=&quot;math/tex&quot;&gt;\frac{f(\mathbf{x} \vert \theta)}{f(\mathbf{y} \vert \theta)}&lt;/script&gt; is free of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;em&gt;if and only if&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = T(\mathbf{y})&lt;/script&gt;, then then &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is a minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;This is helpful! We can use this practically. The proof is a bit difficult, but you can see this definition as arising from the Factorization Theorem. Before we see some examples with some known distributions, lets talk facts about this minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;Is a minimal sufficient statistic unique? &lt;strong&gt;No&lt;/strong&gt;. This is the first fact. It turns out that any injective function of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is also a minimal sufficient statistic. Why does this make sense? Intuitively, a one to one function of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; maps the partitions created by &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; uniquely to partitions in a different partitioned space. Since &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is a minimal sufficient statistic, it achieves the coarsest possible partition space (i.e with the smallest cardinality), so the mapping of a function of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; has to map to the smallest number of possible partitions, by default.&lt;/p&gt;

&lt;p&gt;A more formal proof is as follows. To understand why this proof is as follows, remember that &lt;em&gt;only&lt;/em&gt; a 1-1 function has an inverse:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;T^*(\mathbf{X}) = b(T(\mathbf{X}))&lt;/script&gt; be a one-to-one function. Then, &lt;script type=&quot;math/tex&quot;&gt;\exists \; b^{-1}&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;b^{-1}(T^*(\mathbf{X})) = T(\mathbf{X})&lt;/script&gt;. Thus, knowing &lt;script type=&quot;math/tex&quot;&gt;T^*(\mathbf{X})&lt;/script&gt; assures that we know &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; via &lt;script type=&quot;math/tex&quot;&gt;b^{-1}(.)&lt;/script&gt;. We can also see that, by the factorization theorem, when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is sufficient, &lt;script type=&quot;math/tex&quot;&gt;\exists \; h, \; g&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{x} \vert \theta) = g(T(\mathbf{x}) \vert \theta)h(\mathbf{x}) = g(b^{-1}(T^*(\mathbf{X}))\vert \theta)h(\mathbf{x})&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;T^*(\mathbf{X})&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. This shows that &lt;script type=&quot;math/tex&quot;&gt;T^*(\mathbf{X})&lt;/script&gt; is a sufficient by the Factorization Theorem. Now, assume there is another sufficient statistic &lt;script type=&quot;math/tex&quot;&gt;T_1(\mathbf{X})&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; (the original one) is minimal sufficient, then &lt;script type=&quot;math/tex&quot;&gt;T = q(T_1(\mathbf{X}))&lt;/script&gt;. Therefore, &lt;script type=&quot;math/tex&quot;&gt;T^* = b(T) = b(q(T_1))&lt;/script&gt;. Therefore, &lt;script type=&quot;math/tex&quot;&gt;T^*(\mathbf{X})&lt;/script&gt; is a function of any other sufficient statistic, and is thus a minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;This fact actually leads us to the next fact - there is &lt;em&gt;always&lt;/em&gt; a one-to-one function between two minimally sufficient statistics. To show this, suppose there are two minimally sufficient statistics &lt;script type=&quot;math/tex&quot;&gt;\mathbf{T_1}, 
\mathbf{T_2}&lt;/script&gt;. Then, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{T_1} = f(\mathbf{T_2})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{T_2} = g(\mathbf{T_1})&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{T_1} =f(g(\mathbf{T_1}))&lt;/script&gt; which implies that &lt;script type=&quot;math/tex&quot;&gt;f= g^{-1}&lt;/script&gt; and therefore &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is one-to-one. The same can be shown for &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt;. What this means practically is that the partition created by a minimal sufficient statistic is unique.&lt;/p&gt;

&lt;p&gt;And… now some totally non-boring to examples, so you can pass your class. ;).&lt;/p&gt;

&lt;p&gt;Lets go back to the OG example of this post. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \textrm{Binomial}(n,p)&lt;/script&gt;. Is &lt;script type=&quot;math/tex&quot;&gt;\sum X_i&lt;/script&gt; a minimal sufficient statistic? We know it is sufficient because we already proved it so refresh your memory if you already forgot about this. To prove minimal sufficiency, we have to prove it both ways. So first, lets look at the ratio&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{f_{\mathbf{X}}(\mathbf{y} \vert p)} = \frac{\binom{n}{\sum x_i}p^{\sum x_i}(1-p)^{n - \sum x_i}}{\binom{n}{\sum y_i}p^{\sum y_i}(1-p)^{n - \sum y_i}} \propto_{p} p^{\sum x_i - \sum y_i}(1-p)^{\sum y_i - \sum x_i}&lt;/script&gt;

&lt;p&gt;Now, if &lt;script type=&quot;math/tex&quot;&gt;\sum y_i = \sum x_i&lt;/script&gt;, then the ratio is 1 and thus free of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. If the ratio is free of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; only if &lt;script type=&quot;math/tex&quot;&gt;\sum y_i = \sum x_i&lt;/script&gt;, so then &lt;script type=&quot;math/tex&quot;&gt;\displaystyle\sum_{i=1}^n X_i&lt;/script&gt; is a minimal sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. Great - now lets look at an example where something might not be a minimal sufficient statistic in this same data. Consider the case where &lt;script type=&quot;math/tex&quot;&gt;n=3&lt;/script&gt; and the associated two-dimensional statistic - &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = (X_1 + X_2, X_3)&lt;/script&gt;. Is this sufficient? Yes, it is.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{\mathbf{X}}( \mathbf{x} \vert p) = \binom{n}{x_1 + x_2 + x_3}p^{x_1 + x_2 + x_3}(1-p)^{3 - x_1 -x_2 - x_3} \propto_p p^{x_1 + x_2}(1-p)^{2 - x_1 - x_2}p^{x_3}(1-p)^{1-x_3}&lt;/script&gt;

&lt;p&gt;which, by the Factorization Theorem, is sufficient. Good? But now, is is minimal sufficient? Intuitively, we should say no. Why? You tell me. Okay - I tell you. Consider the minimal sufficient statistic (derived above) &lt;script type=&quot;math/tex&quot;&gt;X_1 + X_2 + X_3&lt;/script&gt;. This statistic takes the partitions (values) of 0,1,2, and 3. Our current statistic has, as an example, partitions (0,1) and (1,0), which are different. However, they both correspond to the &lt;em&gt;coarser&lt;/em&gt; partition generated by &lt;script type=&quot;math/tex&quot;&gt;X_1 + X_2 + X_3&lt;/script&gt;. So, there is always a partition of smaller cardinality. Let’s show this mathematically.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_{\mathbf{X}}(\mathbf{x} \vert p)}{f_{\mathbf{X}}( \mathbf{y} \vert p)} \propto \frac{p^{x_1 + x_2}(1-p)^{2 - x_1 - x_2}p^{x_3}(1-p)^{1-x_3}}{p^{y_1 + y_2}(1-p)^{2 - y_1 - y_2}p^{y_3}(1-p)^{1-y_3}}&lt;/script&gt;

&lt;p&gt;So, this is free of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = T(\mathbf{y})&lt;/script&gt;. But, is it free of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; only if that is the case? No! If &lt;script type=&quot;math/tex&quot;&gt;y_3  = x_1 + x_2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_3 = y_1 + y_2&lt;/script&gt;, then it is free of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; but the statistics are not equal! This is exactly what our intuition above says. If we switch the order of the ordered pair, we map to the same partition in the partition space of another statistic (namely the one defined above). So we can always define a statistic with a coarser partition (a more basic statistic that contains the same information about &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;).&lt;/p&gt;

&lt;h3 id=&quot;ancillary-statistics&quot;&gt;Ancillary Statistics&lt;/h3&gt;

&lt;p&gt;Now - we will talk about ancillary statistics. This mean seem like it is out of left field, and it is, but it is still a pretty basic part of inference (and a type of statistic) so we are going to cover it.&lt;/p&gt;

&lt;p&gt;First, lets talk about the word ancillary. The first time I heard this word, actually, was from a professor I had who was from India and had learned English with a British accent, so I now say ancillary ancíllary, not ancilláry. Anyways - to the point.&lt;/p&gt;

&lt;p&gt;A statistic is ancillary if its distribution does not depend on the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;How is this different from a sufficient statistic? At first glance, the definitions seem kind’ve similiar. They aren’t at all. A sufficient statistic is a statistic that contains all the relevant information about $\theta$ in the data. An ancillary statistic, by defintion, do not contain any information about the parameter at all. I know what you are thinking - so, if they contain no information about the parameter, why in the &lt;em&gt;world&lt;/em&gt; should I care about these things? We will get back to that point.&lt;/p&gt;

&lt;p&gt;First, some examples. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots,X_n \sim \mathcal{N}(\mu,\sigma^2)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; is known. Consider &lt;script type=&quot;math/tex&quot;&gt;T_1 = X_1 - \mu&lt;/script&gt;. This is not, because the statistic is a function of the parameter &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;. What about &lt;script type=&quot;math/tex&quot;&gt;T_2 = X_1 - X_2&lt;/script&gt;? Yes. Since both are Gaussian, the distribution of &lt;script type=&quot;math/tex&quot;&gt;T_2&lt;/script&gt; is Gaussian with expected value 0 and variance &lt;script type=&quot;math/tex&quot;&gt;2\sigma^2&lt;/script&gt;, by convolusion. Therefore, the distribution of &lt;script type=&quot;math/tex&quot;&gt;T_2&lt;/script&gt; does not depend on &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and thus is ancillary. Also, remember &lt;script type=&quot;math/tex&quot;&gt;\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2&lt;/script&gt;. So, the sample standard deviation is also an ancillary statistic for &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;! These are some realtively simple examples, and ones that maybe we should have known off the top of our heads (just an assumption, since you are reading this). How about we look at something a bit more difficult?&lt;/p&gt;

&lt;p&gt;What if we have &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \mathcal{N}(0,\sigma^2)&lt;/script&gt; and we define Y_i to be &lt;script type=&quot;math/tex&quot;&gt;X_i/sigma&lt;/script&gt;. Then, while is not ancillary because it is a function of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, it does follow a standard normal distribution now. Therefore, we can show that &lt;script type=&quot;math/tex&quot;&gt;\frac{X_1}{X_2} = \frac{\sigma Y_1}{\sigma Y_2} = \frac{Y_1}{Y_2}&lt;/script&gt; which, from ratio distributions, is distributed Cauchy. So, by using that transform, we have found a way to use the original data to create an ancillary statistic. Notice, actually, that any function of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt; does not depend on &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; at all, so constructing ancillary statistics is not hard at all, in this case.  Now let’s look at a much more difficult problem.&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots , X_n \sim \textrm{Uniform}(\theta, \theta +1)&lt;/script&gt;. Show that the range, &lt;script type=&quot;math/tex&quot;&gt;R = X_{(n)} - X_{(1)}&lt;/script&gt; is an ancillary statistic. There are two ways to do this - we can go through both. The first is to find the distribution of &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; and show that it does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; - not trivial. To start, we need the joint distribution of the min and the max. This is given in Casella &amp;amp; Berger, Thm. 5.4.6. For any pair of order statistics, the joint distribution is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{X_{(i)},X_{(j)}}{y_1,y_2 \vert \theta} = \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_{X}(y_1)f_{X}(y_2) \left[ F(y_1) \right]^{i-1} \left[ F(y_2) - F(y_1)\right]^{j-i-1}[1-F(y_2)]^{n-j}&lt;/script&gt;

&lt;p&gt;Therefore, the joint distrbution of the min and the max is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{n!}{(n-2)!}f_X(x_{(1)})f_X(x_{(n)})[F(x_{(n)}) - F(x_{(1)})]^{n-2}&lt;/script&gt;

&lt;p&gt;Now, we use our knowledge of the uniform distribution! Note that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_X(x \vert \theta) = I(\theta &lt; x \ \theta + 1) %]]&gt;&lt;/script&gt;

&lt;p&gt;and that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
F_X(x \vert \theta ) = 
\begin{cases}
   0 &amp; \;\;\textrm{if} \;\; x \leq \theta \\
   x - \theta &amp; \;\;\textrm{if} \;\; \theta &lt; x &lt; \theta +1 \\
   1 &amp; \;\;\textrm{if} \;\; x \geq \theta + 1 \\
   
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, the join distribution of &lt;script type=&quot;math/tex&quot;&gt;X_{(1)}, X_{(n)}&lt;/script&gt; is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{cases}
n(n-1)(x_{(n)} - x_{(1)})^{n-2} &amp; \;\; \textrm{if} \;\; \theta &lt; x_{(1)} &lt; x_{(n)} &lt; \theta + 1 \\
0 &amp; \;\; \textrm{else}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, we need to find the distribution of the range! To do this, let &lt;script type=&quot;math/tex&quot;&gt;M = \frac{X_{(1)} + X_{(n)}}{2}&lt;/script&gt; be the median. Then, &lt;script type=&quot;math/tex&quot;&gt;X_{(1)} = \frac{2M - R}{2}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_{(n)} = \frac{2M+R}{2}&lt;/script&gt;. The Jacobian is then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{J} = 
\left[\begin{matrix}
\frac{\partial X_{(1)}}{\partial R} &amp; \frac{\partial X_{(1)}}{\partial M} \\ 
\frac{\partial X_{(n)}}{\partial R} &amp; \frac{\partial X_{(n)}}{\partial M}
\end{matrix}\right] = 
\left[\begin{matrix}
-\frac{1}{2} &amp; 1 \\ 
\frac{1}{2} &amp; 1
\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;and the determinant here is 1. Therfore me can just plug and chug, no need to add in &lt;script type=&quot;math/tex&quot;&gt;\vert \mathbf{J} \vert&lt;/script&gt; to our new pdf. We have the same domain restrictions, just now &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\theta &lt; \frac{2m - r}{2} &lt; \frac{2m + r}{2} &lt; \theta + 1 %]]&gt;&lt;/script&gt;. If we fix &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;, we can see that &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\theta + r/2 &lt; m &lt; \theta + 1 - r/2 %]]&gt;&lt;/script&gt; and we can see that the domain of &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt;. This is all the information we now need to know to derive the distribution of &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;. It follows from this that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_R(r) = \displaystyle\int_{\theta + r/2}^{\theta + 1 - r/2} n(n-1)r^{n-2}dm = n(n-1)r^{n-2}(1-r)&lt;/script&gt;

&lt;p&gt;and this doesn’t depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;! So the range is ancillary. Another thing to notice here - this totally sucked to derive. I know it. You know it. We &lt;em&gt;all&lt;/em&gt; know it. Lets do it a way easier way! Define &lt;script type=&quot;math/tex&quot;&gt;Y =  X_i - \theta&lt;/script&gt;. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_Y(y) = I(\theta &lt; y + \theta &lt; \theta + 1) \vert \frac{dx}{dy}\vert = I(0 &lt; y &lt; 1) %]]&gt;&lt;/script&gt;

&lt;p&gt;which is Uniform(0,1). Wow, so now &lt;script type=&quot;math/tex&quot;&gt;R = X_{(n)} - X_{(1)} = (Y_{(n)} + \theta )-(Y_{(1)} + \theta) = Y_{(n)} - Y_{(1)}&lt;/script&gt; which has nothing to do with &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, andwe can conclude the range is ancillary. Way easier. And similar to the example above the hard example above. Kinda seems like there might be a point.. there is!&lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} \sim f(x - \theta)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is an arbitrary distrbution. Define &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as above. Then &lt;script type=&quot;math/tex&quot;&gt;f_Y(y) = f_X{y + \theta - \theta}&lt;/script&gt; which does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. So the range cancels out the &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; as it does above, and we again get a distribution that does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Seems like for siome distributions, we can just divide the random variables and get an ancillary statistic, and for other types of distributions we can just use the range. This is where the idea of a &lt;em&gt;location-scale family&lt;/em&gt; comes in.&lt;/p&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; is a pdf, the &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})&lt;/script&gt; is also a pdf, specifically a &lt;em&gt;location-scale family with standard pdf&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;. Here, &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is called the location parameter and &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; is called the scale parameter. For instance, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(\mu,\sigma^2)&lt;/script&gt; is a location-scale family with standard pdf &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0,1)&lt;/script&gt; and location parameter &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and scale parameter &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We showed above that the range is an ancillary statistic for a location family. Now, we can show that the ratio &lt;script type=&quot;math/tex&quot;&gt;X_1/X_n&lt;/script&gt; is an ancillary statistic for an arbitrary scale family with pdf &lt;script type=&quot;math/tex&quot;&gt;f(x/\sigma)/\sigma&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\sigma &gt; 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Define &lt;script type=&quot;math/tex&quot;&gt;Y = \frac{X}{\sigma}&lt;/script&gt;. Then, &lt;script type=&quot;math/tex&quot;&gt;X = \sigma Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;dx/dy = \sigma&lt;/script&gt;. Thefore, &lt;script type=&quot;math/tex&quot;&gt;f_Y(y) = \frac{1}{\sigma}f_X(\sigma y/\sigma) \sigma = f(y)&lt;/script&gt;. Then, &lt;script type=&quot;math/tex&quot;&gt;\frac{X_1}{X_n} = \frac{\sigma Y_1}{\sigma Y_n} = \frac{Y_1}{Y_{n}}&lt;/script&gt; which does not depend on &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So its been fun learning about ancillary statistics.. but why does it matter again? Turns out that, while ancillary statistics do not contain any information about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, they can help up the precision in our estimation of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. How, do you say? Consider the Uniform(&lt;script type=&quot;math/tex&quot;&gt;\theta, \theta + 1&lt;/script&gt;) case, and say we know the range is 0.8. This doesn’t tell us anything about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;em&gt;alone&lt;/em&gt;. But say we also know the median is 1. If we &lt;em&gt;only&lt;/em&gt; knew the median was 1, we would only know that &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0 &lt; \theta &lt; 1 %]]&gt;&lt;/script&gt;.  Now, by knowing the range &lt;em&gt;and&lt;/em&gt; the median, we now that &lt;script type=&quot;math/tex&quot;&gt;X_{(1)} = 0.6&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_{(n)} = 1.4&lt;/script&gt;. From this information, we can gather that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; must be in the range of &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0.4 &lt; \theta &lt; 0.6 %]]&gt;&lt;/script&gt; because it can’t be more than 0.6 (or we wouldn’t observe the minimum we have) and vice versa for 0.4. So, in summary, ancillary statistics, &lt;em&gt;when combined with other statistics&lt;/em&gt;, can improve our understanding of the parameter. However, alone, they do nothing for us. They’re like a friend’s friend. Lots of fun to hang out with, but only when your mutual friend is around. Otherwise, things get awkward and you just want to go home.&lt;/p&gt;

&lt;h2 id=&quot;complete-statistics&quot;&gt;Complete Statistics&lt;/h2&gt;

&lt;p&gt;So lets tie all this together (not really, but lets wrap this up with final discussion about statistics).&lt;/p&gt;

&lt;p&gt;We will end this by talking about &lt;strong&gt;complete statistics&lt;/strong&gt;. Sounds cool. What is it? It’s actually defined through familes of distributions (like we talked about above). Let &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P} = \{ p(t \vert \theta), \theta \in \Theta \}&lt;/script&gt; be a family of distributions for &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;E\left[ g(T) \vert \theta \right] = 0 \; \; \forall \; \theta \; \implies P[g(T) = 0 \vert \theta] = 1 \; \forall \; \theta&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt; is called a &lt;strong&gt;complete statistic&lt;/strong&gt;! 
Really, completeness is a property of the family of distributions. To call a statistic itself complete is almost misleading. To get some more intuition into this problem, notice that, in the discrete case,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{\theta}\left[ g(t) \right] = \sum_{i} g(t_i) \times P_{\theta}(T = t_i) = g(t_1) \times p_{\theta}(t_1) + \dots = 0&lt;/script&gt;

&lt;p&gt;Therefore, for &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; to be complete, this must mean that &lt;script type=&quot;math/tex&quot;&gt;g(t)&lt;/script&gt; above is almost surely 0 - i.e there is no non-trivial (occuring with probabilty 0) &lt;script type=&quot;math/tex&quot;&gt;g(t)&lt;/script&gt; that is not zero. This is analagous to the idea of linear independence of vectors. Above, we only have one.. or do we? We actually have a set of equations that corresponds to the number of &lt;script type=&quot;math/tex&quot;&gt;\theta \in \Theta&lt;/script&gt;. So, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{g(\mathbf{x})} \times \mathcal{P}_{\Theta} = 0&lt;/script&gt;

&lt;p&gt;This can only happen when either &lt;script type=&quot;math/tex&quot;&gt;g(T) = 0&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}&lt;/script&gt; does not change across separate values of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, or if it changes very simply (i.e scaled by a constant). Therefore, completeness guarantees that distributions parameterized by different values of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; are distinct. If you change &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, you change the whole thing. Also, conceptually note that there is a “no nonsense” part of this definition. It says “if &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is expected to be zero with respect to &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, then it is just zero”. That is, there is no part of the distribution of &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; that does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. No nonsense. One thing to note from this is that, if you can make an ancillary statistic out of &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X})&lt;/script&gt;, then it cannot be complete… which follows immediately from what was stated above. Another thing to note - this is all defined by the family of distributions &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}&lt;/script&gt;, that depends on the set &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;. A statistic may be complete for a certain portion of that set, and may not be complete for another.&lt;br /&gt;
Let’s check out some examples. Suppose &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} \sim \textrm{Uniform}(\theta,\theta + 1)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = (X_{(1)}, X_{(n)})&lt;/script&gt;. Is this a complete statistic? We should think, intuitively, that it is not. We can make an ancillary statistic (the range) out of this statistic. A complete statistic has no non-trivial part that doesn’t depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, but nothing about the range depends on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;g(T) = R - E[R]&lt;/script&gt;. Now notice that &lt;script type=&quot;math/tex&quot;&gt;E[R]&lt;/script&gt; is a constant with respect to &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Therefore, &lt;script type=&quot;math/tex&quot;&gt;g(T)&lt;/script&gt; is a non-zero function, but &lt;script type=&quot;math/tex&quot;&gt;E[g(T)] \; \forall \; \theta&lt;/script&gt;. Therfore, &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is not complete!&lt;/p&gt;

&lt;p&gt;Now suppose &lt;script type=&quot;math/tex&quot;&gt;T \sim \textrm{Poisson}(\lambda), \; \lambda \in \{1,2\}&lt;/script&gt;. Now, if &lt;script type=&quot;math/tex&quot;&gt;E_{\lambda=1}[g(t)] = 0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;E_{\lambda=1}[g(t)] = 0&lt;/script&gt; implies that &lt;script type=&quot;math/tex&quot;&gt;\sum_{t} \frac{g(t)}{t!} = \sum_{t} \frac{g(t)2^t}{t!} =  0&lt;/script&gt; which clearly does not mean that &lt;script type=&quot;math/tex&quot;&gt;g(t)&lt;/script&gt; must be zero, and you can think of &lt;em&gt;lots&lt;/em&gt; of examples. So this family is not complete. Notice how we are talking about a family of distributions… and remember how I said it depended on the range of the parameter. Now, suppose&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T \sim \textrm{Poisson}(\lambda), \; \; \lambda \in \mathbb{R}^+&lt;/script&gt;

&lt;p&gt;Now, suppose there is &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;E[g(T)] = 0&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t = 0}^{\infty} \frac{g(t)}{t!}\lambda^t = 0 = \sum_{t = 0}^{\infty} \phi(t) \lambda^t = 0&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\lambda &gt; 0&lt;/script&gt;, this can only be 0 if &lt;script type=&quot;math/tex&quot;&gt;\phi(t) = 0 \; \forall \; t \; \implies \; g(t) = 0 \; \forall t&lt;/script&gt;. Bam, complete. By adding more dimensionality to &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt;, the parameter space for &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, we make the family complete. Why is this? Becauase the larger the family (i.e the more paramters), the more constraints &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; must satisfy. At some point, the only such &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is the trivial &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt;, 0. This means the family is complete.&lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \textrm{Uniform}(0,\theta)&lt;/script&gt;. Show that &lt;script type=&quot;math/tex&quot;&gt;X_{(n)}&lt;/script&gt; is complete.&lt;/p&gt;

&lt;p&gt;The distribution of &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(t \vert \theta) = n f_X(t) \left[ F_X(t) \right] = n\frac{1}{\theta} \left[ \frac{t}{\theta} \right]^{n-1}&lt;/script&gt;

&lt;p&gt;So, assume that &lt;script type=&quot;math/tex&quot;&gt;E[g(T)] = 0&lt;/script&gt;. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle\int_{0}^{\theta} g(t)\frac{n}{\theta^n}t^{n-1} = 0 \implies \displaystyle\int_{0}^{\theta} g(t)t^{n-1} = 0&lt;/script&gt;

&lt;p&gt;and, by the Fundamental Theorem of Calculus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\displaystyle\int_{0}^{\theta} g(t)t^{n-1} = \displaystyle\int_{0}^{\theta} F(t)dt = F(\theta) - F(0) = g(\theta)\theta^{n-1} = 0&lt;/script&gt;

&lt;p&gt;so, since &lt;script type=&quot;math/tex&quot;&gt;\theta &gt; 0 \; , \; g(\theta) = 0&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\theta &gt; 0&lt;/script&gt; - concluding the &lt;script type=&quot;math/tex&quot;&gt;X_{(n)}&lt;/script&gt; is complete.&lt;/p&gt;

&lt;p&gt;There’s just a few more things (2) to be said about complete statistics.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If a function of &lt;script type=&quot;math/tex&quot;&gt;T \; , \; h(T)&lt;/script&gt; is ancillary, then &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; cannot be complete. This goes with what we said before - a complete statistic does not have any unncessary part associated with it. Let &lt;script type=&quot;math/tex&quot;&gt;g(T) = h(T) - E[h(T)]&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;h(T)&lt;/script&gt; is ancillary, then &lt;script type=&quot;math/tex&quot;&gt;g(T) = 0&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. But, again, &lt;script type=&quot;math/tex&quot;&gt;g(T)&lt;/script&gt; is non-zero. Or, at least, it doesn’t have to be zero.&lt;/li&gt;
  &lt;li&gt;Say &lt;script type=&quot;math/tex&quot;&gt;T_1(\mathbf{X})&lt;/script&gt; is complete. Then &lt;script type=&quot;math/tex&quot;&gt;h(T) = T_1&lt;/script&gt; is also complete. So assume &lt;script type=&quot;math/tex&quot;&gt;E[g(h(T))] = E[g(T_1)] = 0&lt;/script&gt;. Now, since &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is complete, &lt;script type=&quot;math/tex&quot;&gt;P(g(h(T))) = P(g(T_1)) = 1&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Great! Now, to put some things together: if a minimal sufficient statistic exists, then any complete sufficient statistic is also a minimal sufficient statistic. So, if you find a statistic that is sufficent and complete, it is also minimal sufficient (assuming a minimally sufficient statistic exists). However, a minimal sufficient statistic is not always complete. So, in summary, complete &lt;script type=&quot;math/tex&quot;&gt;\implies&lt;/script&gt; minimal!&lt;/p&gt;

&lt;p&gt;So, I think the main question to be answered here (outside of the questions below.. ;)) is “Why does any of this matter?”. That’s a great question, and an important question to ask of all the theory that you learn. I was a math major - and now a graduate student in statistics, and theory without relevance - for me - is still something I struggle with. However, there is relevance here … moving forward, and just in general. In general, what these definitions and theorems are moving us towards is a way to best answer “so what is going on here?” when we see some data. If I gave you a set of numbers, there is a million ways you can analyze it, but some of those ways are more efficient than others. All of this is just one builiding block of inference. It is the beginning of the journey in figuring out the &lt;em&gt;best&lt;/em&gt; way to look at data - and that is important. Some of these (ancillary..) are just tools to make other tools better. Some of these are ways to make our life easier when dealing with data. And some are really awkward, seemingly random definitions (looking at you, complete…) that are best understood in the context of what they can be used for in more advanced inference. So all of this “matters” mostly in the way that a staircase works - its harder to reach the second step if you don’t have the first one.&lt;/p&gt;

&lt;h2 id=&quot;more-examples&quot;&gt;More Examples&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n&lt;/script&gt; but iid from a density function of the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x \vert \sigma) = \frac{1}{\sigma}e^{-\frac{1}{\sigma}(x - \mu)}, \; \; \mu &lt; x, \;\; 0 &lt; \sigma %]]&gt;&lt;/script&gt;

&lt;p&gt;Find a one-dimensional sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; (known &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;), a one-dimensional sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; (known &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;) and a two-dimensional sufficient statistic for (&lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x} \vert \sigma) = \displaystyle\prod_{i=1}^n \frac{1}{\sigma}e^{-\frac{1}{\sigma}(x_i - \mu)} = \sigma^{-n}e^{\frac{-1}{\sigma}(\sum x_i - n\mu)}I(x_{(1)} &gt; \mu)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma^{-n}e^{\frac{-\sum x_i}{\sigma}} e^{\frac{n \mu}{\sigma}}I(x_{(1)} &gt; \mu)  = h(x)e^{\frac{n \mu}{\sigma}}I(t &gt; \mu) = h(x)g(\mathbf{T} \vert \mu)&lt;/script&gt;

&lt;p&gt;so, by the Factorization Theorem, &lt;script type=&quot;math/tex&quot;&gt;X_{(1)}&lt;/script&gt; is a sufficent statistic for &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;. For the second part, note that the likelihood can also be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x)g(\mathbf{T} \vert \sigma)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;h(x) = e^{\frac{n \mu}{\sigma}}I(x_{(1)} &gt; \mu)&lt;/script&gt;  and &lt;script type=&quot;math/tex&quot;&gt;g(\mathbf{T} \vert \sigma) = \sigma^{-n}e^{\frac{-t}{\sigma}}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;t = \sum_{i=1}^n x_i&lt;/script&gt;. Therefore, by the Factorization Theorem again, &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n X_i&lt;/script&gt; is a sufficent statistic for &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;. Furthermore, the likelihood can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma^{-n}e^{\frac{-\sum x_i}{\sigma}} e^{\frac{n \mu}{\sigma}}I(x_{(1)} &gt; \mu) = \sigma^{-n}e^{\frac{-t_1}{\sigma}} e^{\frac{n \mu}{\sigma}}I(t_2 &gt; \mu) = h(x)g(\mathbf{T_1},\mathbf{T_2} \vert \mu, \sigma)&lt;/script&gt;

&lt;p&gt;so, &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{T_2}, \mathbf{T_1}) = (X_{(1)}, \sum_{i=1}^n X_i)&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;(\mu,\sigma)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \mathcal{N}(0, \sigma^2)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Show that &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n X_i^2&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; and determine whether it is a minimal sufficient statistic or not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x} \vert \sigma^2) = (\frac{1}{\sqrt{2 \pi \sigma^2}})^n e^{\frac{-\sum x_i^2}{2 \sigma^2}}I(\sigma^2 &gt; 0) = (2\pi)^{-n/2}(\sigma)^{-n}e^{\frac{-\sum x_i^2}{2 \sigma^2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= (2\pi)^{-n/2}(\sigma)^{-n}e^{\frac{-t}{2 \sigma^2}} = h(\mathbf{x})g(T(\mathbf{X}) \vert \sigma^2)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;t = \sum_{i=1}^n x_i^2&lt;/script&gt; so by the Factorization Theorem, &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = \sum_{i=1}^n X_i^2&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;. Now,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f(\mathbf{x} \vert \sigma^2)}{f(\mathbf{y} \vert \sigma^2)} = e^{\frac{-1}{2\sigma^2}(\sum x_i^2 - \sum y_i^2)} = e^{\frac{-1}{2\sigma^2}(T(\mathbf{x}) - T(\mathbf{y}))}&lt;/script&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = T(\mathbf{y})&lt;/script&gt;, this ratio is 1 which is clearly free of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;. Now, if the ratio is free of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, then it is necessary that &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = T(\mathbf{y})&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = \sum X_i^2&lt;/script&gt; is a minimal sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\sigma ^2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \textrm{Poisson}(\lambda)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Find a sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and show that it is a minimal sufficient statistic.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x} \vert \lambda) = \frac{1}{\prod_{i=1}^n x_i !}e^{-n\lambda}\lambda^{\sum x_i} = \frac{1}{\prod_{i=1}^n x_i !}e^{-n\lambda}\lambda^{t} = h(\mathbf{x})g(T(\mathbf{x}) \vert \lambda)&lt;/script&gt;

&lt;p&gt;so by the Factorization Theorem (again…), &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n X_i&lt;/script&gt; is sufficient for &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. Now,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f(\mathbf{x} \vert \lambda)}{f(\mathbf{y} \vert \lambda)} \propto_{\lambda} \lambda^{\sum x_i - \sum y_i} = \lambda^{T(\mathbf{x}) - T(\mathbf{y})}&lt;/script&gt;

&lt;p&gt;so if &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = T(\mathbf{y})&lt;/script&gt;, the ratio is free of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. Furthermore, if the ratio is free of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, then it is necessary that &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = T(\mathbf{y})&lt;/script&gt; for the exponent to be 0 and thus have &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; disappear. Therefore, &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{x}) = \sum X_i&lt;/script&gt; is also a minimal sufficient statistic for &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim f_X(x \vert \theta)&lt;/script&gt; where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_X(x \vert \theta) = \frac{2x}{\theta^2}, \; \; 0 &lt; x &lt; \theta %]]&gt;&lt;/script&gt;

&lt;p&gt;Find a minimal sufficent statistic for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f_X(\mathbf{x} \vert \theta)}{f_X(\mathbf{y} \vert \theta)} = \frac{\prod x_i I(x_{(n)} &gt; \theta)}{\prod y_i I(y_{(n)} &gt; \theta)} \propto_{\theta} \frac{I(x_{(n)} &gt; \theta)}{I(y_{(n)} &gt; \theta)}&lt;/script&gt;

&lt;p&gt;Therfore, since this ratio doesn’t depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; if and only if &lt;script type=&quot;math/tex&quot;&gt;x_{(n)} = y_{(n)}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;X_{(n)}&lt;/script&gt; is minmial sufficient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots , X_n \sim f(x \vert \theta)&lt;/script&gt; where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x \vert \theta) = \theta x^{\theta - 1} \textrm{exp}(-x^{\theta})&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;\theta ,\; x \; &gt; \; 0&lt;/script&gt;. Show that &lt;script type=&quot;math/tex&quot;&gt;\frac{\log X_{(n)}}{\log X_{(1)}}&lt;/script&gt; is ancillary. Well, let &lt;script type=&quot;math/tex&quot;&gt;Y = \theta \log(X) \implies X = e^{Y/\theta}&lt;/script&gt;. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_Y(y) = f_X(e^{Y/\theta})\vert \frac{dx}{dy} \vert = \theta \; \textrm{exp}(\frac{y}{\theta}(\theta - 1))\textrm{exp}(-\textrm{exp}(\frac{y}{\theta}(\theta))) \frac{1}{\theta}e^{y/ \theta}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \textrm{exp}(y - e^y)&lt;/script&gt;

&lt;p&gt;Now, since &lt;script type=&quot;math/tex&quot;&gt;\theta &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\log(*)&lt;/script&gt; is increasing monotonically, the maximum and minimum values are the same after the mappings. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\log X_{(n)}}{\log X_{(1)}} = \frac{Y_{(n)}}{Y_{(1)}}&lt;/script&gt;

&lt;p&gt;which is ancillary, since the distribution of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; does not depend on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, as we have shown above.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim \textrm{Uniform}(-\theta, \theta)&lt;/script&gt;. Is &lt;script type=&quot;math/tex&quot;&gt;T(\mathbf{X}) = (X_{(1)}, X_{(n)})&lt;/script&gt; a complete sufficient statistic? If not, is &lt;script type=&quot;math/tex&quot;&gt;\textrm{max}_i \vert X_i \vert&lt;/script&gt;?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first question: No, and this should be easy to see. This is a scale family, so we can make an ancillary statistic out of &lt;script type=&quot;math/tex&quot;&gt;\frac{X_{(n)}}{X_{(1)}}&lt;/script&gt;. Just let &lt;script type=&quot;math/tex&quot;&gt;X = \theta Y&lt;/script&gt; and do the transform. Therefore, this statistic cannot be complete. More cleary,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_Y(y) = f_X(\theta y) \vert \frac{d \theta y}{dy} \vert I(-\theta &lt; y \theta &lt; \theta) = \frac{1}{2}I(-1 &lt; y &lt; 1) %]]&gt;&lt;/script&gt;

&lt;p&gt;which is Uniform(0,1). Then, let &lt;script type=&quot;math/tex&quot;&gt;g(T) = \frac{X_{(n)}}{X_{(1)}} - E\left[ \frac{X_{(n)}}{X_{(1)}} \right] = \frac{Y_{(n)}}{Y_{(1)}} - E \left[ \frac{Y_{(n)}}{Y_{(1)}} \right]&lt;/script&gt;. Now we see &lt;script type=&quot;math/tex&quot;&gt;E\left[ g(t) \vert \theta \right] = 0&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; but it doesn’t mean that &lt;script type=&quot;math/tex&quot;&gt;g(T) = 0&lt;/script&gt; almost surely.&lt;/p&gt;

&lt;p&gt;Now, for &lt;script type=&quot;math/tex&quot;&gt;Y = \textrm{max}_i \vert X_i \vert&lt;/script&gt;. Lets define the distribution first for &lt;script type=&quot;math/tex&quot;&gt;\vert X \vert&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
F_Y(y) = P(Y &lt; y) = P(- y &lt; X &lt; y ) = P(X &lt; y) - P (X &lt; -y) %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\implies f_y(y) = f_X(y) - f_X(-y)(-1) = f_X(y) - f_X(-y) = \frac{1}{\theta} I(0 &lt; y &lt; \theta) %]]&gt;&lt;/script&gt;

&lt;p&gt;which is Uniform(&lt;script type=&quot;math/tex&quot;&gt;0, \theta&lt;/script&gt;). Now, we know that &lt;script type=&quot;math/tex&quot;&gt;T = \textrm{max}_i \vert X_i \vert&lt;/script&gt; is distributed with pdf&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_T(t \vert \theta) = nf(X)[F(x)]^{n-1}&lt;/script&gt;

&lt;p&gt;and now this is the exact problem we did in the examples above. So this is a complete statistic! Is it sufficient?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_X(x \vert \theta) = \left[ \frac{1}{2 \theta}\right]^n I(-\theta &lt; \mathbf{x} &lt; \theta) = \left[ \frac{1}{2 \theta}\right]^n I(t &lt; \theta) %]]&gt;&lt;/script&gt;

&lt;p&gt;so it is sufficient by the factorization theorem! Therefore, this is a complete sufficient statistic.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/2019/12/31/gradient-descent.html" rel="alternate" type="text/html" title="Gradient Descent" /><published>2019-12-31T00:21:32-05:00</published><updated>2019-12-31T00:21:32-05:00</updated><id>http://localhost:4000/2019/12/31/gradient-descent</id><content type="html" xml:base="http://localhost:4000/2019/12/31/gradient-descent.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;You’ll remember in OLS, we had something called the &lt;strong&gt;normal equations&lt;/strong&gt; - a nice, succinct, simple formula for calculating out best-fit parameters. You’ll also remember, then, that we had to invert an n by n matrix, &lt;script type=&quot;math/tex&quot;&gt;X^TX&lt;/script&gt; to get these parameters.. which, if n is large, is computationally very expensive. Today we’ll discuss gradient descent, a less computationally expensive way (for large n) to obtain parameters that minimize our squared error.&lt;/p&gt;

&lt;p&gt;It’s important to first note that gradient descent is not limited to least squares. Gradient descent is just an algorithm that can be used to maximize (or minimize) any convex objective function by calculating the gradient at a point, taking a step in that direction, calculation the gradient again, taking a step in that direction… so on and so forth, until the gradient is basically norm 0. It just so happens that the least squares function is convex, so gradient descent converges to the minimum.&lt;/p&gt;

&lt;p&gt;This algorithm won’t work for non-convex functions as it may only search out a local minimum, not the global minimum. If you’re standing at the base of two hills, the gradient is just going to take you up the steepest hill, not necessarily the tallest one, right? Okay.. anyways, do we remember what the gradient is?&lt;/p&gt;

&lt;p&gt;Let’s say we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\beta_0, \beta_1, \dots, \beta_q): \mathbb{R}^{q+1} \rightarrow \mathbb{R}&lt;/script&gt;

&lt;p&gt;then, the gradient is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \frac{\partial J}{\partial \beta_0} \hat{e_1} + \dots + \frac{\partial J}{\partial \beta_q} \hat{e_{q+1}} = \begin{bmatrix} \frac{\partial J}{\partial \beta_0} \\ \vdots \\ \frac{\partial J}{\partial \beta_q} \end{bmatrix}&lt;/script&gt;

&lt;p&gt;so, essentially, each component of the gradient tells you how fast your function changes with respect to the standard basis in each direction. We can find the directional derivative of any unit vector &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\nabla J \cdot \vec{v}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J \cdot \vec{v} = \| \nabla J\|\|\vec{v}\|\cos(\theta) =\| \nabla J\|\cos(\theta)&lt;/script&gt;

&lt;p&gt;since &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; is a unit vector, where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the angle between the two vectors. &lt;script type=&quot;math/tex&quot;&gt;\cos(\theta)&lt;/script&gt; is max when $\theta = 0$ - that is, when &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; is in the direction of the gradient! So the steepest ascent is in the direction of the gradient, and the steepest descent is in the opposite direction. We should note that the steepest ascent here is limited to unit vectors, so the steepest ascent is really in the direction of &lt;script type=&quot;math/tex&quot;&gt;\frac{\nabla J}{\| \nabla J\|}&lt;/script&gt;, right? But that’s the same direction as the gradient. And we are going to kinda normalize the gradient in our own way later… you will see.&lt;/p&gt;

&lt;p&gt;You can think of the gradient as kind of a trade-off. Say the gradient is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;3,1,6&gt; %]]&gt;&lt;/script&gt;. You’re confined to a circle in terms of where you can move with the gradient (or any vector) starting from a point. That is, there’s a trade-off between directions. You can essentially trade 3 steps in the Y direction for one step in the X direction. So an optimal direction does just that, hence why the optimal direction is the gradient… it takes one 3 steps in the X direction for each step in the Y direction. Any other decision is ‘unfair’, i.e there is a better trade-off that maximizes how steep the gradient could be.&lt;/p&gt;

&lt;p&gt;Anyways. The gradient heads in the direction of the steepest ascent, so an algorithm searching out the lowest point of a function should head in the opposite of the gradient. So let’s consider least squares, where our squared error loss term is a function of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\beta}&lt;/script&gt;, the vector of parameters. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\beta_0, \dots, \beta_q) = \frac{1}{2n} \sum_{i=1}^n (\hat{y_i} - y_i)^2  =\frac{1}{2n} \sum_{i=1}^n ( \beta_0 + \beta_1x_{i1} + \dots + \beta_qx_{iq} - y_i )^2&lt;/script&gt;

&lt;p&gt;where we just normalized the squared error function. Remember, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is the number of features, and &lt;script type=&quot;math/tex&quot;&gt;q+1&lt;/script&gt; is the number of parameters we must estimate. Now, using a bit of calculus, we see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J} {\partial \beta_0} = \frac{1}{n} \sum_{i=1}^n ( \hat{y_i} - y_i)&lt;/script&gt;

&lt;p&gt;and, for &lt;script type=&quot;math/tex&quot;&gt;j = 1,\dots, q&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J} {\partial \beta_j} = \frac{1}{n} \sum_{i=1}^n (\hat{y_i} - y_i)x_{ij}&lt;/script&gt;

&lt;p&gt;so, a gradient descent algorithm would look something like&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;is the norm of the gradient greater than our threshold?
    &lt;ul&gt;
      &lt;li&gt;if yes, move along the gradient for each &lt;script type=&quot;math/tex&quot;&gt;\beta_J&lt;/script&gt; by some &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and check again&lt;/li&gt;
      &lt;li&gt;if no, return the current point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is a stepsize of your choosing. It is important to note that the choice of the step-size is important. A step-size that is too big will cause the algorithm to jump over the minimum value and maybe even diverge, whereas a step-size that is too small will mean the algorithm takes forever to converge to the minimum value. In one dimension, we can see what is happening below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/lambda_tut.jpg&quot; alt=&quot;Different stepsizes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the step size is small (left), it is converging to the minimum value… but it is gonna take forever (maybe even actually. . .) and on the right, where the step size is too large, the minimum is being overshot and then the larger slope (gradient) is compounding with a large step size to overshoot by even more and so on and so forth as your algorithm sadly diverges to infinity ….. :(&lt;/p&gt;

&lt;p&gt;You can imagine this type of computation is best done in a while-loop. Below is an interpretation of this algorithm in Julia:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;&lt;span class=&quot;n&quot;&gt;gradient_descent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inter&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_plus_one&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;α&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;#for i in 1:q_plus_one&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#    gradient[i] = α*(error'*X[:,i])&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#end&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;β&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first part of this just says ‘okay, did you add an intercept? maybe not.. because you might not think to add the column of ones… we are gonna assume you want to do this so we will do it for you if you did not’. Obviously, this isn’t necessary, and could actually lead to incorrect results if the column  missing isn’t the one of all ones, but it is just to show you that coding is YOUR life, do with it what you will… Anyways. We compute the gradient via the dot product, as you will notice that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_i} - y_i) = e^T \cdot \mathbf{1_n}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_i} - y_i)x_{ij} = e^T  \cdot \mathbf{X}[:,j]&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; is the n-dimensional vector of errors and $latex X$ is the feature matrix, with a row of ones so that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n (\hat{y_1} - y_i) = e^T \cdot \mathbf{X}[:,1] = e^T \cdot \mathbf{1_n}&lt;/script&gt;

&lt;p&gt;We also add an iteration count (just in case our step size is too small or too large or we totally messed up the code) to stop the loop at a certain point. However, this way of thinking about the algorithm is commented out because it is not efficient. Generally, for-loops should (and usually can) be replaced by matrix multiplication, which is in the code above! Let’s go ahead and see if we can see how you would write that ‘in math’…..&lt;/p&gt;

&lt;p&gt;You can see in the code that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \mathbf{X^T(X\beta - Y)}&lt;/script&gt;

&lt;p&gt;which is actually just exactly what we wrote above. You’re multiplying the errors by the columns of the feature matrix to get the components of the gradient, so why not just transpose your matrix and use matrix multiplication? Thats what this is saying. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\beta^{(new)}} = \mathbf{\beta^{(old)}} - \alpha \frac{1}{n} \mathbf{X^T( X\beta - Y)}&lt;/script&gt;

&lt;p&gt;Perfect. So now we know what it is, how to use it.. but when would you use it? Usually only when you have really large values of n (lots of observations, like millions…) . Essentially, the only reason you use it is time. There are other things you can do here, like use only one observation (which is called Least Mean Squares) which saves even more time but isn’t as great, obviously. There are other issues with gradient descent, like correctly choosing the step-size that, in practice, you really have no way around. There are plenty of ways to choose a step-size, but no set-rule - you’ll have to figure it out on your own (and that takes time, too!). What’s best about gradient descent is that you can use it for a lot of things. Least squares is just one. You can use it for plenty of other things… really, you can use it for whatever problem you have where you need to optimize a function. But I hope learning about it via least squares was helpful!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Introduction to Survival Analysis</title><link href="http://localhost:4000/2019/10/21/introduction-to-survival-analysis.html" rel="alternate" type="text/html" title="Introduction to Survival Analysis" /><published>2019-10-21T23:00:00-04:00</published><updated>2019-10-21T23:00:00-04:00</updated><id>http://localhost:4000/2019/10/21/introduction-to-survival-analysis</id><content type="html" xml:base="http://localhost:4000/2019/10/21/introduction-to-survival-analysis.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Survival Analysis: the branch of statistics that pretty much everyone I know only associates with biostatistics. I get it - that makes sense. Sure, survival implies.. surviving, but lots of things survive. My attempt to be a vegetarian (still going) is something that ‘survives’. Your T.V., your willingness to stay on an email list. Things like that. Survival analysis takes on many different names (for instance, reliability in engineering). Essentially, it deals with data that has ‘failure’ times and ‘censoring’ times - so it takes into account actual and observational failure times. For example, a failure time could be death (actual failure) and a censoring time could be a patient lost to follow-up (observational failure). It’s a cool subject - one that is applicable to almost every field, so let’s dive into some of the basics. The first : &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;. Big one. The time to event, which is a random variable. The second: &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;. Also a big one, the time to censoring. And finally, the first and second combined into the third(?): &lt;script type=&quot;math/tex&quot;&gt;X = min(T,C)&lt;/script&gt; which is the event we actually observe. Ok, now actually finally, we have &lt;script type=&quot;math/tex&quot;&gt;\Delta = I(T \leq C) = I (X = T)&lt;/script&gt;, which is just 1 if we observe the event and 0 if we lose the observation to censoring. Those are the big variables for the introduction, so let’s get into talking about the main functions and some examples.&lt;/p&gt;

&lt;p&gt;There are 3 main functions in survival analysis, and all can be derived from one another. We have the survival function, the hazard function (or hazard rate, or just hazard) and the cumulative hazard function. Starting with the survival function:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S(t) = P(T &gt;t)&lt;/script&gt; if t is continuos and &lt;script type=&quot;math/tex&quot;&gt;S(t _ j)= P(T &gt; t _ j) = P(T \geq t_{j-1} )&lt;/script&gt; when t is discrete. How do we define this? We need to define a density first.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_ T(t) = \lim_{\delta\to 0^+}\frac{1}{\delta}P(t  \leq T \leq t + \delta)&lt;/script&gt;

&lt;p&gt;when  &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous and, simply,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_T(t) = P(T = t)&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is discrete. Now, to define our survival function should be relatively easy.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S(t) = P(T  &gt; t) = \displaystyle\int_ {t}^{\infty}f_T(u)du&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S(t) = P(T &gt;t) = \displaystyle\sum_ {n &gt; t}f_T(t)&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is discrete.&lt;/p&gt;

&lt;p&gt;A few things to note about a survival function. &lt;script type=&quot;math/tex&quot;&gt;S(0) = 1&lt;/script&gt; as we should not have events happening at time 0. &lt;script type=&quot;math/tex&quot;&gt;\lim_ {t\to \infty}S(t) = 0&lt;/script&gt; as we expect everyone to die or have an event as time goes on. Also, the survival function should be monotone decreasing in &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. It is important to note that, in the discrete case, the survival function is right continuous. That is, it has jumps (down) at the values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; where events happen, and is evaluated and the lower probability of survival for those time periods. More formally &lt;script type=&quot;math/tex&quot;&gt;S(t) = \lim_{x\to t^+}S(x) = S(t^+)&lt;/script&gt; This is important when we are estimating… the hazard function! The hazard function is the probability that a person dies at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; GIVEN that they have survived until time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;\lambda (t)&lt;/script&gt; denote the survival time.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \lim_{\delta \to 0^+}\frac{1}{\delta}P(t \leq T \leq t + \delta  \vert T &gt; t)&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = P(T = t \vert T \geq t)&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is discrete. In general, from the basic probability theory,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T =t \vert T \geq t) = \frac{P(T = t, T \geq t)}{P(T \geq t)} = \frac{P(T = t)}{P(T \geq t)} = \frac{f_T(t)}{S(t^-)}&lt;/script&gt;

&lt;p&gt;Notice that the hazard function, when continuous, is a rate and, when discrete, is a probability. Finally, let us define &lt;script type=&quot;math/tex&quot;&gt;\Lambda(t)&lt;/script&gt; as the cumulative hazard function. It is pretty much exactly what you would think it was…&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = \displaystyle\int_{0}^{t}\lambda(u)du&lt;/script&gt;

&lt;p&gt;when continuous and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = \displaystyle\sum_{t_i \leq t} \lambda(t_i)&lt;/script&gt;

&lt;p&gt;when discrete. Let’s take a look at an interesting problem involving the cumulative hazard function. Say &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; follows some distribution with cdf &lt;script type=&quot;math/tex&quot;&gt;F_t(t)&lt;/script&gt;. Then its hazard function follows an exponential(1) distribution. To see this&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\Lambda(T) \leq t) = P( - \Lambda(T)\geq -t ) = P(e^{-\Lambda(T)} \geq e^{-t}) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(-e^{-\Lambda(T)} \leq -e^{-t}) = P(1 - e^{-\Lambda(T)} \leq 1 - e^{-t}) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(1-S(T) \leq 1- e^{-t}) = P(F_ T(T) \leq 1- e^{-t}) = P(T \leq F_ T^{-1}(1- e^{-t})) = F_ T(F_T^{-1}(1- e^{-t})) = 1- e^{-t}&lt;/script&gt;

&lt;p&gt;which is the cdf of the exponential(1) distribution! Cool, huh? Anyways.&lt;/p&gt;

&lt;p&gt;It is clear what the link between the hazard function and the cumulative hazard function is, but now let us find the explicit link with the survival function. First (and pretty much only..), remember&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \frac{f(t)}{S(t^-)}&lt;/script&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is continuous, this is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \frac{f(t)}{S(t^-)} = \frac{f(t)}{S(t)} = \frac{-d\log S(t)}{dt}&lt;/script&gt;

&lt;p&gt;which means&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = -\log(S(t)&lt;/script&gt;

&lt;p&gt;and, equivalently,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = e^{-\Lambda(t)}&lt;/script&gt;

&lt;p&gt;Nice! Easy. Let’s do an example. Suppose &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is such that its survival function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = \frac{64}{(t+8)^2}&lt;/script&gt;

&lt;p&gt;This is continuous. First, a fact:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[T] = \displaystyle\int_{0}^{\infty} S(t)dt&lt;/script&gt;

&lt;p&gt;How to prove?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[T] = \displaystyle\int_{0}^{\infty} tf(t)dt = \displaystyle\int_{0}^{\infty}\displaystyle\int_{0}^{t}ds f(t)dt = \displaystyle\int_{0}^{\infty}\displaystyle\int_{s}^{\infty}f(t)dtds = \displaystyle\int_{0}^{\infty}1 - F(s) ds  = \displaystyle\int_{0}^{\infty}S(s)ds&lt;/script&gt;

&lt;p&gt;So, what is our expected survival time in the above case?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[T] = 64 \displaystyle\int_{0}^{\infty} (t + 8)^{-2} = 64/8 = 8&lt;/script&gt;

&lt;p&gt;Median survival time? Clearly, that is just where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = 0.5&lt;/script&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{64}{(t_m +8)^2} = 0.5 \implies \sqrt(128) - 8 = t_m = 3.31&lt;/script&gt;

&lt;p&gt;Now, let’s estimate the hazard function (and the cumulative hazard).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(t) = -\log(S(t)) = 2 \log(t+8) - \log(64)&lt;/script&gt;

&lt;p&gt;Now, we just take the derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to find &lt;script type=&quot;math/tex&quot;&gt;\lambda(t)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t) = \frac{2}{t+8}&lt;/script&gt;

&lt;p&gt;The discrete case is a liiiiiiitle bit more difficult, but very important for continued study in survival analysis. In practice, unless we have literall infinite observations, we will be dealing with discrete data. Let’s say &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; takes on values &lt;script type=&quot;math/tex&quot;&gt;t_ 1, \dots , t_ n&lt;/script&gt;. Also, remember (more basic probability theory) that &lt;script type=&quot;math/tex&quot;&gt;P(AB) = P(A\vert B)P(B)&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is many events, then this formula is recursive. Now, suppose &lt;script type=&quot;math/tex&quot;&gt;t_j \leq t \leq t_{j+1}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = P(T &gt; t) = P(T &gt; t_j, T &gt; T_{j-1},\dots,T &gt; t_1) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T &gt; t_j \vert T &gt; t_{j-1})\times P(T &gt; t_{j-1} \vert T &gt; t_{j-1})\times \dots \times P(T &gt; t_1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1 - P(T \leq t_j \vert T &gt; t_{j-1})]\times \dots \times [1 - P(T\leq t_1)]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1 - P(T = t_j \vert T \geq t_{j})]\times \dots \times [1 - P(T = t_1)]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \displaystyle\prod_{t_i\leq t} [1 - \lambda(t_i)]&lt;/script&gt;

&lt;p&gt;Notice, this formula is recursive! &lt;script type=&quot;math/tex&quot;&gt;S(t_j) = [1 - \lambda(t_j)]S(t_{j-1})&lt;/script&gt; which doesn’t really matter here but .. it makes it easier to compute if, say, you wanted to write up this algorithm to help yourself learn (maybe?).  This actually has an analogous case in the continuous case (if we remember the Taylor Expansion for &lt;script type=&quot;math/tex&quot;&gt;e^{-x}&lt;/script&gt; LOL). To the rescue again..&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{-x} = \displaystyle\sum_{n = 0}^{\infty} \frac{(-x)^n}{n!} \approx 1- x&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is small.&lt;/p&gt;

&lt;p&gt;So, since 
&lt;script type=&quot;math/tex&quot;&gt;\Lambda(T) = \int_{0}^{t}\lambda(u)du&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = e^{-\Lambda(t)}&lt;/script&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = \displaystyle\prod_{u=0}^t e^{-\lambda(u)du} \approx \displaystyle\prod_{u=0}^t[1 - \lambda(u)du]&lt;/script&gt;

&lt;p&gt;which is sometimes referred to as the product limit form of the survival function. Anyways.. let’s get to some real examples. First, from data.. let’s think of a good way to estimate &lt;script type=&quot;math/tex&quot;&gt;\lambda(t)&lt;/script&gt;. Seems natural to estimate the hazard at a certain time point by looking at the number of events at a time point relative to the number of people at risk for the event at that time point. So, letting &lt;script type=&quot;math/tex&quot;&gt;D_ J&lt;/script&gt; denote the number of events at time &lt;script type=&quot;math/tex&quot;&gt;t_ j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y _ j&lt;/script&gt; denote the number at risk (sill being observed, including the people who experience events) for an event at &lt;script type=&quot;math/tex&quot;&gt;t_j&lt;/script&gt;. Then a natural estimate for &lt;script type=&quot;math/tex&quot;&gt;\lambda(t_j)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\frac{D_j}{Y_j}&lt;/script&gt;. Looking at some data (+ indicates a censored observation)…&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2,5^+,8,12^+,15,21^+,25,29,30^+,34&lt;/script&gt;

&lt;p&gt;Let’s estimate &lt;script type=&quot;math/tex&quot;&gt;S(10)&lt;/script&gt;. Notice, the first term of this estimation will be &lt;script type=&quot;math/tex&quot;&gt;P(T &gt; 10 \vert T &gt; 8)&lt;/script&gt; which is 1, since nothing happens at 10.. so if someone survives past 8, they will survive past 10. For sure. With probability 1. :) SO,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(10) = S(8) =  \displaystyle\prod_{t \in \{2,5,8\}}[1 - \lambda(t)]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1- \frac{D_8}{Y_8}][1- \frac{D_5}{Y_5}][1- \frac{D_2}{Y_2}]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= [1- \frac{1}{8}][1- \frac{0}{9}][1- \frac{1}{10}] = \frac{63}{80}&lt;/script&gt;

&lt;p&gt;Notice at 5, the observation was censored, so it does not get included in &lt;script type=&quot;math/tex&quot;&gt;D_5&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Note that this estimate of our survival function will be piece-wise continuous, right continuous with jumps at event times, and will be 0 IF our last observation is a failure. Why can we do this when we have censored data? Because we assume that &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is independent of &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, or else we would have a joint distribution for the hazard. Anyways… this type of estimation is what is referred to as the Kaplan-Meier estimate of the survival function which, along with Nelson-Aalen, is the most popular non-parametric (or just in general) way to estimate the survival function.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Bayesian Estimation</title><link href="http://localhost:4000/2018/07/13/bayesian-estimation.html" rel="alternate" type="text/html" title="Bayesian Estimation" /><published>2018-07-13T01:21:32-04:00</published><updated>2018-07-13T01:21:32-04:00</updated><id>http://localhost:4000/2018/07/13/bayesian-estimation</id><content type="html" xml:base="http://localhost:4000/2018/07/13/bayesian-estimation.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;When most people think about statistics, they think about flipping a coin or gambling or having flashbacks to dropping out of high school because they hated the class so much… moral of the story is, it’s just one thing to think about. What if I told you that that wasn’t necessarily true? I am about to tell you that. Statistics is the science of modeling randomness. But what is random, and what is not? That is a fundamental question in statistics and splits the subject into two separate approaches - Bayesian and Frequentist.&lt;/p&gt;

&lt;p&gt;What does it mean to say that I am 50% certain it will rain tomorrow? That is not so much a statement of probability, but rather of uncertainty. It will rain or it will not rain, but that outcome is something you are uncertain of. Consider another example: flip a coin and cover it up immediately once it lands, so you don’t know what the result is. What’re the chances that the coin is heads? From a traditional, frequentist perspective, there is no answer to this question. It already happened. There is already a result - you’re just ignorant. But that’s not really how we think, is it? We don’t know what the result is - we are uncertain of it. Let’s consider a medical example. Say you’re getting screened for the flu, and the result comes out positive. Do you actually have the flu? In the frequentist approach, that doesn’t make sense to ask. We do have sensitivity and specificity. Let &lt;script type=&quot;math/tex&quot;&gt;\theta = 1&lt;/script&gt; if you have the flu and &lt;script type=&quot;math/tex&quot;&gt;\theta = 0&lt;/script&gt; if you don’t. Then the sensitivity is the probability the test is positive given that you have the flue, or &lt;script type=&quot;math/tex&quot;&gt;P(T = 1 \vert \theta = 1)&lt;/script&gt; and the specificity is &lt;script type=&quot;math/tex&quot;&gt;P(T = 0 \vert \theta = 0)&lt;/script&gt;, the probability the test is negative when you don’t have the flu. If specificity is not 1, you might get tested positive but not have the disease. It makes sense then to ask ‘“Do I actually have the flu?”, but does it make statistical sense to ask ‘Do I have the flu?’? That is, what is &lt;script type=&quot;math/tex&quot;&gt;P(\theta =1 \vert T = 1)&lt;/script&gt;? A frequentist says ‘Bad question. &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is fixed’. A Bayesian will come in and say ‘Hold on, &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is not actually fixed. We can estimate this’&lt;/p&gt;

&lt;p&gt;This is the whole premise of Bayesian statistics - to use probability theory to not &lt;em&gt;only&lt;/em&gt; quantify probability but also to quantify uncertainty. Much better!! Solving problems that were disregarded earlier - we like that!&lt;/p&gt;

&lt;p&gt;Okay, moving on… now that I’ve converted you to the Bayesian approach to statistics…&lt;/p&gt;

&lt;p&gt;:)&lt;/p&gt;

&lt;p&gt;Let’s talk about the setup. In a frequentist approach, we have a random variable &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;, observed data from that random variable, a model, and a parameter &lt;script type=&quot;math/tex&quot;&gt;\theta \in \Omega&lt;/script&gt; that is unknown but &lt;em&gt;fixed&lt;/em&gt;. In the Bayesian framework, we don’t consider that italicized part. It’s a subtle change that makes a huge difference. Now, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \sim \pi(\theta)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is called a &lt;em&gt;prior distribution&lt;/em&gt;. Now, we need to make an adjustment to your original data’s distribution. Now, we have a conditional distribution!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{X} \vert \theta \sim f(\mathbf{x}\vert \theta)&lt;/script&gt;

&lt;p&gt;and, by Bayes rule, the joint distribution of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x},\theta) = \pi(\theta)f(\mathbf{x}|\theta)&lt;/script&gt;

&lt;p&gt;and, thus, the marginal distribution of  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m(\mathbf{x}) = \displaystyle\int_{\theta \in \Omega} {f(\mathbf{x},\theta)d\theta}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\theta \vert \mathbf{x}) = \frac{\pi(\theta)f(\mathbf{x} \vert \theta)}{m(\mathbf{x}) }&lt;/script&gt;

&lt;p&gt;We now have all the tools we need to do some basic Bayesian analysis!&lt;/p&gt;

&lt;p&gt;and, so, by Bayes rule again, the posterior distribution of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is&lt;/p&gt;

&lt;p&gt;Let us look at &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots, X_n \sim Bernoulli(p)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;p \sim Beta(\alpha, \beta)&lt;/script&gt;. Then we could calculate &lt;script type=&quot;math/tex&quot;&gt;\pi(\theta \vert \mathbf{x}) = \frac{\pi(p)f(\mathbf{x}\vert \theta)}{m(\mathbf{x}) }&lt;/script&gt;, but that would take forever. Does &lt;script type=&quot;math/tex&quot;&gt;m(\mathbf{x})&lt;/script&gt; matter? No. Why? Think about it. We want the posterior distribtion of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, right? So &lt;script type=&quot;math/tex&quot;&gt;m(\mathbf{x})&lt;/script&gt; has &lt;em&gt;literally&lt;/em&gt; (millenials…) nothing to do with &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, therefore it just acts as a normalizing constant in the equation to make sure that &lt;script type=&quot;math/tex&quot;&gt;\pi(p \vert \mathbf{x})&lt;/script&gt; is a true pdf. Therefore, we can just look at the numerator of the expression and recognize the kernel of the distribution to understand what family the posterior distribution comes from. Soo, in math,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(p \vert \mathbf{x}) \propto \pi(p)f(\mathbf{x} \vert p)&lt;/script&gt;

&lt;p&gt;We know&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x}\vert p) = \displaystyle \prod_{i=1}^{n} f(\mathbf{x_i}\vert p) = \displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i})&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(p) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}&lt;/script&gt;

&lt;p&gt;so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(p \vert \mathbf{x}) \propto\displaystyle \prod_{i=1}^{n} (p^{x_i}(1 - p)^{1-x_i}) \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= p^{\sum_{i=1}^{n}x_i}(1-p)^{n - \sum_{i=1}^{n}x_i}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha -1}(1 - p)^{\beta -1}&lt;/script&gt;

&lt;p&gt;which is the same as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} p^{\alpha + \sum_{i=1}^{n}x_i -1} (1 - p)^{\beta +n - \sum_{i=1}^{n}x_i - 1}&lt;/script&gt;

&lt;p&gt;Looking at the part of this expression that has to do with &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, what do we see? It is the kernel of a Beta distribution! So, now we know that &lt;script type=&quot;math/tex&quot;&gt;\pi(p
\vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)&lt;/script&gt;. Cool. Another thing to notice here: the prior and the posterior distribution are from the same family. This, informally, is what you call a &lt;strong&gt;conjugate family&lt;/strong&gt;.  Anyways, now that we have the prior distribution, we can do some estimation.&lt;/p&gt;

&lt;p&gt;If we remember, one of the frequentist approaches to estimation is Maximum Likelihood. Here, our parameter follows a distribution so it makes sense to just say ‘hey… what’s its expected value?’ . So that’s what people do! The Bayes estimator is literally just &lt;script type=&quot;math/tex&quot;&gt;E[p\vert \mathbf{x}]&lt;/script&gt;. We know &lt;script type=&quot;math/tex&quot;&gt;\pi(p \vert \mathbf{x}) \sim Beta(\alpha + \sum_{i=1}^{n}x_i, n + \beta - \sum_{i=1}^{n}x_i)&lt;/script&gt; . Since we know the expected value of &lt;script type=&quot;math/tex&quot;&gt;Beta(\alpha,\beta)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\frac{\alpha}{\alpha + \beta}&lt;/script&gt;, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[p \vert \mathbf{x}] = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \sum_{i=1}^{n}x_i + \beta + n - \sum_{i=1}^{n}x_i} = \frac{\alpha + \sum_{i=1}^{n}x_i}{\alpha + \beta + n}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \bar{X}(\frac{n}{\alpha + \beta + n}) + \frac{\alpha}{\alpha + \beta}(\frac{\alpha + \beta }{\alpha + \beta + n})&lt;/script&gt;

&lt;p&gt;So the Bayes estimator is just a weighted average of the prior mean and the MLE! When n is large, the MLE is weighted much more and, when n is small, we trust the prior distribution’s estimate. This is a very cool result, in my opinion.&lt;/p&gt;

&lt;p&gt;Let’s look at another example. Let &lt;script type=&quot;math/tex&quot;&gt;X_1, \dots , X_n \vert \lambda \sim Poisson(\lambda)&lt;/script&gt; and let &lt;script type=&quot;math/tex&quot;&gt;\pi(\lambda) \sim Gamma(\alpha, \beta)&lt;/script&gt;. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\lambda \vert \mathbf{x}) \propto f(\mathbf{x}\vert \lambda )\pi(\lambda)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{\lambda^{\sum_{i=1}^{n}x_i} e^{-n\lambda}}{\prod_{i=1}^{n}x_i !} \frac{1}{\Gamma(\alpha)\beta^{\alpha}} \lambda^{\alpha-1}e^{\frac{-\lambda}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{1}{\prod_{i=1}^{n}x_i! \, \Gamma(\alpha)\beta^{\alpha}} \lambda^{\sum_{i=1}^{n}x_i +\alpha -1} e^{-\lambda(n + \frac{1}{\beta})}&lt;/script&gt;

&lt;p&gt;and we should recognize the kernel (part with &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;) of this distribution and conclude that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(\lambda\vert \mathbf{x}) \sim Gamma(\sum_{i=1}^{n}x_i + \alpha , (n + \frac{1}{\beta})^{-1})&lt;/script&gt;

&lt;p&gt;Another conjugate family! The Bayes estimator here is just&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[ \lambda | \mathbf{x}] = \frac{\sum_{i=1}^{n}x_i + \alpha }{n + \frac{1}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \bar{X} (\frac{n}{n + \frac{1}{\beta}}) + \alpha \beta (\frac{1}{n\beta + 1})&lt;/script&gt;

&lt;p&gt;In summary, its quite simple to find the posterior distribution in these cases - but this is obviously not true in general. Sometimes the posterior takes an unknown form, and the best we can do is draw samples from this posterior using different methods (Gibbs Sampling, Importance Sampling). These examples above are cases where the posterior follows almost immediately from the joint distribution (conjugate family in both cases, actually) and makes our lives very easy. There is much more to go into about Bayesian Estimation, and I will do so here in the future. For now, you’ve got a good start on it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Least Squares via the SVD</title><link href="http://localhost:4000/2018/04/11/least-squares-via-the-svd.html" rel="alternate" type="text/html" title="Least Squares via the SVD" /><published>2018-04-11T23:00:00-04:00</published><updated>2018-04-11T23:00:00-04:00</updated><id>http://localhost:4000/2018/04/11/least-squares-via-the-svd</id><content type="html" xml:base="http://localhost:4000/2018/04/11/least-squares-via-the-svd.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Let’s say you have some matrix of features (covariates) &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbb{R}^{m \times n}&lt;/script&gt; and a response vector &lt;script type=&quot;math/tex&quot;&gt;y \in \mathbb{R}^m&lt;/script&gt;. We want to find the input vector &lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}^n&lt;/script&gt; that solves this problem or gives us the best possible solution. How should we define best? That is subjective - but the most common idea is to take the real response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; and the predicted response &lt;script type=&quot;math/tex&quot;&gt;A[i,:] x&lt;/script&gt; and square the difference for each response. We can formalize this problem as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x || Ax - y ||_2^2&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\vert\vert . \vert\vert _ 2&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm - where we are just summing the components squared of the input to the norm (in this case, summing predicted minus real squared: the squared error) and taking the square root. We square it just to get rid of the square root - it is the same problem!&lt;/p&gt;

&lt;p&gt;Sometimes there may not an exact solution to this problem - that’s why all we are trying to do is minimize this sum of squares. Obviously, if there is an exact solution, that is going to minimize the squared error because, well, there is no error in that case!&lt;/p&gt;

&lt;p&gt;When would there be no error? Clearly, this must be the case that &lt;script type=&quot;math/tex&quot;&gt;y \in \mathcal{R}(A)&lt;/script&gt; for there to be no error. That is, the rank of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; for there to be no error. Does this guarantee the solution is unique? No. For the solution to be unique, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(A)&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. That is, the rank of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. Thus, for there to be a unique solution with zero error, &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; must be square. Notice that a unique solution and a solution with zero error are different concepts. You can have a unique solution where the error is non-zero, and you can have lots (infinite, actually!) solutions that are exact. So, now that we have some intuition into the problem, let’s work on solving it. Let’s think about it geometrically! Let’s consider the 3 &lt;script type=&quot;math/tex&quot;&gt;\times&lt;/script&gt; 3 matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}  1 &amp; 0 &amp; 0 \\  0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The range of this matrix is the x-y plane, obviously, since its columns span &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2&lt;/script&gt;. So what if we want to solve a problem of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x || Ax - b ||_2^2&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;b \in \mathbb{R}^3&lt;/script&gt; and is non-zero in the z-component that is closest to the vector &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that exists in the range of our transformation. Check out this figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pics/projection_svd.jpg&quot; alt=&quot;Projection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The closest point is always the point directly perpendicular (or orthogonal) to the range of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; - think about the triangle inequality. Like, if the end of your vector was a donkey, and the range of your matrix was some food that donkeys like (any ideas?), the donkey would walk straight to the food in a direct path. That’s the intuition here. So, in other words, to find the answer to our question, we just need to project the vector we are trying to reach on to the range of values that we can reach. We need a projection matrix &lt;script type=&quot;math/tex&quot;&gt;P_ {\mathcal{R}(A)}&lt;/script&gt;. So, how do we get that…. first, we need to find an (orthonormal) basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; and then use that basis to build &lt;script type=&quot;math/tex&quot;&gt;P_{\mathcal{R}(A)}&lt;/script&gt;. So….. how do we get that…&lt;/p&gt;

&lt;p&gt;Let’s introduce an easy way to do this - the Singular Value Decomposition. Every matrix, square or not, has an SVD. For positive semi-definite matrices, the SVD is just the same as the Eigendecomposition. (ED). However, unlike the ED, the SVD is defined for rectangular matrices. You can view the ED as kind of a subset of the SVD. Anyways, here’s the definition.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A \in \mathbb{R}^{m \times n} = U\Sigma V^T = \sum_{i =1}^{r} \sigma_{i}u_{i}v_{i}^{T}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;U \in \mathbb{R}^{m \times m}&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;V \in \mathbb{R}^{n \times n}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma \in \mathbb{R}^{m \times n}&lt;/script&gt; and is diagonal. Both &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are orthogonal matrices, where the columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; are the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; and the columns of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt;. To see this, notice both &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; are square, symmetric matrices.&lt;/p&gt;

&lt;p&gt;Without loss of generalizability, consider&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TA = V \Sigma^T U^TU \Sigma V^T = V \Sigma ^T \Sigma V^T = V \Lambda V^T&lt;/script&gt;

&lt;p&gt;which is the eigendecomposition. So, clearly, the eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; are found in &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;. Same can be said of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;. Another observation we can make here is that &lt;script type=&quot;math/tex&quot;&gt;\Sigma^T\Sigma = \Lambda&lt;/script&gt;, so the elements of &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;\{\sigma_ i\}_{i=1}^{r}&lt;/script&gt; are the square roots of the eigenvalues of both &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt;. These values are called the singular values of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;!&lt;/p&gt;

&lt;p&gt;So now, we have a tool to solve our least squares problem. How do we use it? If we want to form a basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt;. Well, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; is just the span of the columns of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Extending this notion to the SVD,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = \sum_ {i=1}^{r} \sigma_ i u_ i v_ i^Tx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \sum_ {i=1}^{r} \sigma_ i (v_ i^T x) u_i&lt;/script&gt;

&lt;p&gt;and notice that &lt;script type=&quot;math/tex&quot;&gt;\sigma_ i ( v_ i^Tx)&lt;/script&gt; is a scalar. This is just a linear combination of the first &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;  columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; Therefore, letting &lt;script type=&quot;math/tex&quot;&gt;U_ r&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;m \times r&lt;/script&gt; matrix with the first &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; columns of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, corresponding to the non-zero singular values, we have shown that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A) = \mathcal{R}(U_ r)&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is orthogonal, &lt;script type=&quot;math/tex&quot;&gt;U_ r&lt;/script&gt; is an orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt;. Using the fact that, if &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is is an orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{V}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;BB^T = P_ {\mathcal{V}}&lt;/script&gt;, we have that &lt;script type=&quot;math/tex&quot;&gt;U_ rU_ r^T = P_ {\mathcal{R}(A)}&lt;/script&gt;. So, in terms of our problem, the point closest to &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that is in  &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(A)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;U_rU_r^Tb&lt;/script&gt;. We are now super close to finding &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt; that minimizes the error in the &lt;script type=&quot;math/tex&quot;&gt;\ell -2&lt;/script&gt; sense. To get there, we need to define one more thing - the &lt;strong&gt;Monroe-Penrose Pseudo Inverse&lt;/strong&gt;, &lt;script type=&quot;math/tex&quot;&gt;A^+&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Definition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^+ = V \Sigma ^+ U^T&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Sigma^+ \in \mathbb{R}^{n \times m}&lt;/script&gt; = diag&lt;script type=&quot;math/tex&quot;&gt;(\frac{1}{\sigma_ 1},\dots,\frac{1}{\sigma_r},0,\dots,0)&lt;/script&gt;. Using this definition,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AA^+ = U\Sigma V^T V \Sigma ^+ U^T &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= U \Sigma \Sigma^+ U^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= U_rU_r^T&lt;/script&gt;

&lt;p&gt;Therefore, since we know the closest possible point we can reach with &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;U_ r U_ r^Tb&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_ r U_ r^Tb = A(A^+b )&lt;/script&gt;

&lt;p&gt;and it follows immediately that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x} = A^+b&lt;/script&gt;

&lt;p&gt;for any matrix &lt;script type=&quot;math/tex&quot;&gt;A \in \mathbb{R}^{m \times n}&lt;/script&gt;. Therefore, we have shown there is a general solution to the least-squares problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{argmin} _ x ||Ax - b||_2^2&lt;/script&gt;

&lt;p&gt;An interesting thing to note here:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|| A\hat{x} - b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || b - A\hat{x} ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || b - AA^+b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || (I - AA^+)b ||_2^2&lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;U = [U_ r U_ o]&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;U_ o&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;m \times n-r&lt;/script&gt; matrix with columns corresponding the the zero-valued singular values of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;UU^T = I = [U_ r U_ o] [U_ r U_ o]^T = U_ rU_ r^T + U_ oU_ o^T&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\implies U_ oU_ o^T = I - AA^+ = I - P_ { \mathcal{R}(A)} = P_ {\mathcal{R}^{\perp}(A)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || (I - AA^+)b ||_2^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= || P_ {\mathcal{R}^{\perp}(A)}b ||_2^2&lt;/script&gt;

&lt;p&gt;That is, what we are doing is projecting the error onto the the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;P_ {\mathcal{R}(A)}&lt;/script&gt;. This is exactly what the figure above is showing! Our error should be directly orthogonal to &lt;script type=&quot;math/tex&quot;&gt;P_{\mathcal{R}(A)}&lt;/script&gt; for the objective function to be minimized.&lt;/p&gt;

&lt;p&gt;Now we have a bit of intuition into how we can use the Singular Value Decomposition to solve the least squares problem. This helps when you have a feature matrix and outcomes and want to find the specific weights associated with each covariate in the feature matrix. Least squares is used to solve for the parameters in linear regression and can be used to find the weights associated with neural networks… it has lots of applications. Hopefully, now you can apply it!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Linear Regression</title><link href="http://localhost:4000/2018/03/10/linear-regression.html" rel="alternate" type="text/html" title="Linear Regression" /><published>2018-03-10T22:00:00-05:00</published><updated>2018-03-10T22:00:00-05:00</updated><id>http://localhost:4000/2018/03/10/linear-regression</id><content type="html" xml:base="http://localhost:4000/2018/03/10/linear-regression.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;Linear Regression… the bread and butter of applied statistics.&lt;/p&gt;

&lt;p&gt;At one point or another, you have encountered linear regression. Did you read a statistic in the paper this morning? Was that statistic not a percent? Does anybody read the paper anymore?… Anyways, that number was probably derived via running a regression.&lt;/p&gt;

&lt;p&gt;A linear regression is used to describe an association (NOT A CAUSAL THING) between one (continuous) response variable, &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and a vector of (continuous or not, whatever) explanatory variables &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;. Why is it called linear regression? Well, remember from algebra that a line can be described by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = mx + b&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the independent (explanatory) variable and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is the dependent (response) variable. We assume there is a number &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;, the slope, that describes the relationship here. Essentially, we are saying the same thing with linear regression. We want to find the best &lt;script type=&quot;math/tex&quot;&gt;\beta_ 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; such that the formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i = \beta_0 + \beta_1*X_i + \epsilon_i&lt;/script&gt;

&lt;p&gt;is the best fit given our data. It doesn’t matter if &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is linear, quadratic, exponential.. only that the relationship is linear in the &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; parameters. &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is an error term for every &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;  observation. So the problem is really trying to find &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is minimum overall, i.e. taking into account each observation. The math behind linear regression is all about how we best do that. For this post, we will only explore simple linear regression where we have one explanatory variable. This need not need be the case, and we will cover that another time - but it involves some linear algebra and is a little more involved. The rest of this post assumes some knowledge of statistics, just at a basic level like expected values and distributions.&lt;/p&gt;

&lt;p&gt;So, we have this model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i = \beta_0 + \beta_1*X_i + \epsilon_i&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; is the response variable, &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; is the intercept (fixed), &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; is the slope (fixed), &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is a covariate (fixed) and &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is the error term, which is random and unobservable. As always, we need some assumptions in order to make this workable.. We assume that the errors are normally distributed and unrelated… that is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon_i \sim N(0,\sigma^2)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E[\epsilon_i\epsilon_j] = 0&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These assumptions lead us to some key implicit assumptions of linear regression. &lt;em&gt;Linearity&lt;/em&gt;,  &lt;em&gt;constant variance&lt;/em&gt;, &lt;em&gt;independence&lt;/em&gt; and &lt;em&gt;normally distributed&lt;/em&gt;. That is,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E[Y_i \vert X_i ] = \beta_0 + \beta_1X_i&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_i^2 =  V[Y_i \vert X_i] = \sigma^2&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i \perp Y_j, i \neq j&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i \sim N(\beta_0 + \beta_1X_i,\sigma^2)&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We treat the covariates as fixed, but sometimes they aren’t.  For instance, there could be measurement error in the covariates. Really, the goal is to keep that randomness small.&lt;/p&gt;

&lt;p&gt;Let’s talk about &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;. How should we interpret them? Well, they are part of a linear relationship. We are  going to estimate them such that we fit a line to the data that should give us the expected value of the response given some input parameters.  That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[ Y_i | X_i ] = \hat{\beta_0} + \hat{\beta_1}X_i&lt;/script&gt;

&lt;p&gt;The little hat things mean they are estimated, not the true parameters… more on that later. So, &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_0}&lt;/script&gt; is the average value of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;  when &lt;script type=&quot;math/tex&quot;&gt;X = 0&lt;/script&gt;. Furthermore, &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_1}&lt;/script&gt; is the average change in &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; for a unit increase in &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. In general, it is best to not extrapolate beyond what your limits for &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; are. If your covariate is age, and you only have observations for people aged 15 - 30, don’t make the assumption that the model works for a 90-year-old… please.&lt;/p&gt;

&lt;p&gt;Okay! Now to the fun stuff… math. We wanna estimate the parameters here. How do we do that? Calculus. :)&lt;/p&gt;

&lt;p&gt;First, we need to define best. What is the &lt;em&gt;best&lt;/em&gt; fit? That’s arbitrary, as there are many ways to do this. We are going to do it via ordinary least squares, or OLS, which is the default way that any linear regression is fit. What this means is that we want to minimize the &lt;strong&gt;Sum of Squared Errors (SSE)&lt;/strong&gt;, or, in math,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{n} \hat{\epsilon_i}^2&lt;/script&gt;

&lt;p&gt;So, we need a definition of &lt;script type=&quot;math/tex&quot;&gt;\hat{\epsilon_i}&lt;/script&gt;, Well, it is just &lt;script type=&quot;math/tex&quot;&gt;Y_i - \hat{Y_i}&lt;/script&gt; or, even better, &lt;script type=&quot;math/tex&quot;&gt;Y_i - (\hat{\beta_0} + \hat{\beta_1}X_i)&lt;/script&gt; , since that is how we are guessing our reponse variable. This is good because now we have written &lt;script type=&quot;math/tex&quot;&gt;\hat{\epsilon_i}&lt;/script&gt; as a function of &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_0}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_1}&lt;/script&gt;, which are our tuneable parameters. Now we will need a little bit of calculus. This function will reach a critical value when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{\(*\)} \frac{\partial SSE}{\partial \beta_0} = 0&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{\(**\)} \frac{\partial SSE}{\partial \beta_1} = 0&lt;/script&gt;

&lt;p&gt;Remember? ;)&lt;/p&gt;

&lt;p&gt;Let’s solve (*) first.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial SSE}{\partial \beta_0} = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies -2 \sum_{i=1}^{n}(Y_i - \hat{\beta_0} -\hat{\beta_1}X_i) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \sum_{i=1}^{n}Y_i  - \hat{\beta_1}\sum_{i=1}^{n}X_i= n\hat{\beta_0}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X}&lt;/script&gt;

&lt;p&gt;So, if we know &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_1}&lt;/script&gt;, we can estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta_0}&lt;/script&gt; . Now, looking at (**) and subbing in this result,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial SSE}{\partial \beta_1} = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies -2 \sum_{i=1}^{n}X_i (Y_i - \hat{\beta_0} -\hat{\beta_1}X_i) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies -2 \sum_{i=1}^{n}X_i (Y_i -\bar{Y} +\hat{\beta_1}\bar{X} -\hat{\beta_1}X_i) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \sum_{i=1}^{n}X_i(Y_i - \bar{Y}) = \hat{\beta_1} \sum_{i=1}^{n}X_i(X_i - \bar{X})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies \hat{\beta_1} = \frac{SSXY}{SSX}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;SSXY = \sum_{i=1}^{n}X_i(Y_i - \bar{Y})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;SSX =  \sum_{i=1}^{n}X_i(X_i - \bar{X})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Awesome! Now, given a vector of covariates and a vector of responses, you can estimate both paramaters quite easily! We didn’t really need to do this though. It is good for understanding… but we didn’t need to do the calculus, we have &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; to do this!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>